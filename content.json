{"meta":{"title":"分享之","subtitle":"Share everything","description":"Share everything","author":"Hassan","url":"https://blog.fenxiangz.com","root":"/"},"pages":[{"title":"归档","date":"2020-12-24T03:21:03.231Z","updated":"2020-12-24T03:21:03.231Z","comments":true,"path":"archives/index.html","permalink":"https://blog.fenxiangz.com/archives/index.html","excerpt":"","text":""},{"title":"专题","date":"2020-12-24T03:21:03.231Z","updated":"2020-12-24T03:21:03.231Z","comments":true,"path":"series/index.html","permalink":"https://blog.fenxiangz.com/series/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-12-28T17:33:02.759Z","updated":"2020-12-28T17:33:02.759Z","comments":false,"path":"tags/index.html","permalink":"https://blog.fenxiangz.com/tags/index.html","excerpt":"","text":""},{"title":"类别","date":"2020-12-24T09:51:18.000Z","updated":"2020-12-28T17:33:02.759Z","comments":false,"path":"categories/index.html","permalink":"https://blog.fenxiangz.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Jetbrains系列产品重置试用方法","slug":"util/ide/2021-01-18_idea_reset","date":"2021-01-18T00:00:00.000Z","updated":"2021-01-18T06:14:03.465Z","comments":true,"path":"post/util/ide/2021-01-18_idea_reset.html","link":"","permalink":"https://blog.fenxiangz.com/post/util/ide/2021-01-18_idea_reset.html","excerpt":"","text":"原文：https://zhile.io/2020/11/18/jetbrains-eval-reset.html 0x0. 项目背景 Jetbrains家的产品有一个很良心的地方，他会允许你试用30天（这个数字写死在代码里了）以评估是否你真的需要为它而付费。 但很多时候会出现一种情况：IDE并不能按照我们实际的试用时间来计算。 我举个例子：如果我们开始了试用，然后媳妇生孩子要你回去陪产！陪产时我们并无空闲对IDE试用评估，它依旧算试用时间。（只是举个例子，或许你并没有女朋友） 发现了吗？你未能真的有30天来对它进行全面的试用评估，你甚至无法作出是否付费的决定。此时你会想要延长试用时间，然而Jetbrains并未提供相关功能，该怎么办？ 事实上有一款插件可以实现这个功能，你或许可以用它来重置一下试用时间。但切记不要无休止的一直试用，这并不是这个插件的本意！ 0x1. 如何安装 1). 插件市场安装： 在Settings/Preferences... -&gt; Plugins 内手动添加第三方插件仓库地址：https://plugins.zhile.io 搜索：IDE Eval Reset插件进行安装。如果搜索不到请注意是否做好了上一步？网络是否通畅？ 插件会提示安装成功。 2). 下载安装： 点击这个链接(v2.1.6)下载插件的zip包（macOS可能会自动解压，然后把zip包丢进回收站） 通常可以直接把zip包拖进IDE的窗口来进行插件的安装。如果无法拖动安装，你可以在Settings/Preferences... -&gt; Plugins 里手动安装插件（Install Plugin From Disk...） 插件会提示安装成功。 0x2. 如何使用 一般来说，在IDE窗口切出去或切回来时（窗口失去/得到焦点）会触发事件，检测是否长时间（25天）没有重置，给通知让你选择。（初次安装因为无法获取上次重置时间，会直接给予提示） 也可以手动唤出插件的主界面： 如果IDE没有打开项目，在Welcome界面点击菜单：Get Help -&gt; Eval Reset 如果IDE打开了项目，点击菜单：Help -&gt; Eval Reset 唤出的插件主界面中包含了一些显示信息，2个按钮，1个勾选项： 按钮：Reload 用来刷新界面上的显示信息。 按钮：Reset 点击会询问是否重置试用信息并重启IDE。选择Yes则执行重置操作并重启IDE生效，选择No则什么也不做。（此为手动重置方式） 勾选项：Auto reset before per restart 如果勾选了，则自勾选后每次重启/退出IDE时会自动重置试用信息，你无需做额外的事情。（此为自动重置方式） 0x3. 如何更新 1). 插件更新机制（推荐）： IDE会自行检测其自身和所安装插件的更新并给予提示。如果本插件有更新，你会收到提示看到更新日志，自行选择是否更新。 点击IDE的Check for Updates... 菜单手动检测IDE和所安装插件的更新。如果本插件有更新，你会收到提示看到更新日志，自行选择是否更新。 插件更新可能会需要重启IDE。 2). 手动更新： 从本页面下载最新的插件zip包安装更新。参考本文：下载安装小节。 插件更新需要重启IDE。 0x4. 一些说明 本插件默认不会显示其主界面，如果你需要，参考本文：如何使用小节。 市场付费插件的试用信息也会一并重置。 对于某些付费插件（如: Iedis 2, MinBatis）来说，你可能需要去取掉javaagent配置（如果有）后重启IDE： 如果IDE没有打开项目，在Welcome界面点击菜单：Configure -&gt; Edit Custom VM Options... -&gt; 移除 -javaagent: 开头的行。 如果IDE打开了项目，点击菜单：Help -&gt; Edit Custom VM Options... -&gt; 移除 -javaagent: 开头的行。 重置需要重启IDE生效！ 重置后并不弹出Licenses对话框让你选择输入License或试用，这和之前的重置脚本/插件不同（省去这烦人的一步）。 如果长达25天不曾有任何重置动作，IDE会有通知询问你是否进行重置。 如果勾选：Auto reset before per restart ，重置是静默无感知的。 简单来说：勾选了Auto reset before per restart则无需再管，一劳永逸。 0x5. 开源信息 插件是学习研究项目，源代码是开放的。源码仓库地址：Gitee。 如果你有更好的想法，欢迎给我提Pull Request来共同研究完善。 插件源码使用：GPL-2.0开源协议发布。 插件使用PHP编写，毕竟PHP是世界上最好的编程语言！ 0x6. 支持的产品 IntelliJ IDEA AppCode CLion DataGrip GoLand PhpStorm PyCharm Rider RubyMine WebStorm","categories":[{"name":"IDE","slug":"IDE","permalink":"https://blog.fenxiangz.com/categories/IDE/"}],"tags":[{"name":"Idea","slug":"Idea","permalink":"https://blog.fenxiangz.com/tags/Idea/"},{"name":"IDE","slug":"IDE","permalink":"https://blog.fenxiangz.com/tags/IDE/"}]},{"title":"武夷岩茶品类知识","slug":"tea/2021-01-07_武夷岩茶品类知识","date":"2021-01-07T00:00:00.000Z","updated":"2021-01-20T07:12:03.612Z","comments":true,"path":"post/tea/2021-01-07_武夷岩茶品类知识.html","link":"","permalink":"https://blog.fenxiangz.com/post/tea/2021-01-07_%E6%AD%A6%E5%A4%B7%E5%B2%A9%E8%8C%B6%E5%93%81%E7%B1%BB%E7%9F%A5%E8%AF%86.html","excerpt":"","text":"武夷岩茶的分类：大红袍、名枞、肉桂、水仙、奇种 不同的地理位置，造就了不同的特产。 茶叶更是典型的特产，需要特定地理位置才能种植相应的茶叶品种。 在我国，有六大名茶。其中福建省就有三大名茶 1、红茶：正山小种、金骏眉 2、乌龙茶：铁观音，武夷山岩茶（大红袍、肉桂），漳平水仙，漳州黄芽奇兰，永春佛手 3、白茶：福鼎白茶, 白毫银针、白牡丹、贡眉、寿眉等 所以，月是故乡明，茶是福建多！ 我国六大名茶是指：乌龙茶、红茶、绿茶、白茶、黑茶和黄茶。 **一、乌龙茶：**包括铁观音、黄金桂、武夷岩茶（包括大红袍、肉桂、武夷水仙）、漳平水仙、漳州黄芽奇兰、永春佛手、台湾冻顶乌龙、广东凤凰水仙、凤凰单枞等。 二、红茶：正山小种、金骏眉、银骏眉、坦洋工夫、祁门工夫、宁红等。 三、绿茶：龙井、碧螺春、黄山毛峰、南京雨花茶、信阳毛尖、庐山云雾茶、太平猴魁、六安瓜片等。 四、白茶：白毫银针、白牡丹、贡眉、寿眉等。 五、黑茶：普洱茶、茯砖茶、六堡茶等。 六、黄茶：君山银针、霍山黄芽、蒙山黄芽等。 今天，就说说武夷山岩茶。 岩茶属于乌龙茶系列之一，乌龙茶属于六大名茶之一。 大红袍与武夷岩茶的关系 大红袍，其实是一个茶树品种的名称，其3棵6株母树位于武夷山景区的九龙窠，已有超过350年的历史。 我们现在喝的大红袍，则是由母树大红袍的枝条扦插培育出来的，不是嫁接培育出来的，分为： 纯种大红袍、普通大红袍 及 拼配大红袍。 纯种大红袍: 母树的6株品种分别为：北斗1号、北斗2号和奇丹（每2株为1棵）。 普通大红袍: 从母树大红袍通过剪枝扦插，无性繁殖培育出来的。市场上经常说的一代大红袍、二代大红袍是不存在的，因为本身是无性繁殖。 拼配大红袍: 也称“商品大红袍”，是现在市场上最常见、销售量最多的大红袍。 大红袍属于武夷岩茶最具代表的产品。现在，大红袍又成为武夷岩茶对外的统一品牌名，武夷岩茶被统称为“大红袍”。 如果把大红袍当做品种名称，大红袍则是武夷岩茶的系列之一。 武夷岩茶的分类 武夷岩茶的名枞有几百种，甚至上千种，仅仅一个慧苑坑就有800多个种类。近几年武夷山还引进了外地的乌龙茶优良品种，有少量栽培、生产和上市销售，如黄旦、奇兰、黄奇、黄观音、金观音等。 不过，政府相关部门对岩茶种类进行了科学、规范的划分，根据**《武夷岩茶新国家标准（GB/T18745-2006）》**，武夷岩茶产品分为五大类： 1、大红袍 2、名枞 3、肉桂 4、水仙 5、奇种。 大红袍，是由母树大红袍的枝条扦插培育出来的品种。 肉桂和水仙，都是武夷岩茶中的当家品种，年总产量约占武夷岩茶的70%左右。这也难怪要将这两个品种单独列为两类了。 **名枞系列，**指的是从“菜茶”品种中经过长期选育而成，自然品质优异，具有典型的岩茶岩韵特征的有命名的茶树，典型的有十大名枞:大红袍、铁罗汉、白鸡冠、水金龟、半天妖、白牡丹、金桂、金锁匙、北斗、白瑞香。 大红袍是名枞系列的一种，但因为其知名度最高，故单独列出一个系列。 历史上将白鸡冠、铁罗汉、水金龟、半天妖、大红袍列为五大名枞，后来大红袍常被单独列为一大名枞，于是就有了我们现在常说的四大名枞。 奇种系列，是指武夷山野生茶叶树种，武夷山没有命名的野生茶叶树种或菜茶树种。“菜茶”，是武夷茶农对武夷山有性繁殖茶树群体品种的俗称。意思是这些茶就像门前门后所种的青菜一样普通，只供日常饮用。 对于五花八门的岩茶名称，我们实在没必要悉数记清，记住国家规定的五大类就差不多了。 就连当地一些茶农也反映，除了亲手栽培制作的，别的也很难分清，不易说对。我们只需辨识它们的质量好坏，感知它们的口感、岩韵就行了。 哪些地方才是正宗武夷岩茶？ 根据《武夷岩茶新国家标准（GB/T18745-2006）》，武夷岩茶是指，在福建省武夷山市所辖区域范围内，在独特的武夷山自然生态环境下选用适宜的茶树品种进行无性繁育和栽培，并用独特的传统加工工艺制作而成，具有岩韵（岩骨花香）品质特征的乌龙茶。 武夷岩茶地理标志产品保护范围限于国家质量监督检验检疫总局根据《地理标志产品保护规定》批准的范围，即：福建省武夷山市所辖区域范围，含岚谷乡、吴屯乡、洋庄乡、星村镇、兴田镇、五夫镇、上梅乡、新丰街道、崇安街道、武夷街道等地区。 武夷山核心区属于丹霞地貌，一座山垂直而下，光照、水流、土壤都有非常大的区别，一路高低错落、阴阳不定，可谓移步换景。 变量如此之多，以至于武夷山无形中被分割成了无数小山场，每个山场都是一个独立的小气候区。 不同的山场及气候，是武夷岩茶之所以复杂的第一个原因。 很早以前，武夷岩茶就被分为三个区域：正岩，半岩和洲茶。 正岩以三坑两涧（慧苑坑、牛栏坑、大坑口、留香涧和悟源涧）为代表； 九曲溪边的河滩，则被称为洲茶； 介于两者之间的就是半岩： 但如今，关于这三个区域的界定又进一步扩大： 三坑两涧被称作名岩；武夷山核心景区，包括三十六峰、九十九岩，统统算入正岩；核心景区外、武夷山境内，算半岩茶；而武夷山外的茶，才叫洲茶： 也就是说，从前最次的九曲溪边洲茶，按现在的标准，都是正岩茶了呢。 如果看了以上的内容，还是云里雾里，也没有关系，只要你喝的岩茶适合你的口味就是好茶。 最贵的茶，不一定是你喜欢的茶， 便宜的茶，有可能你很喜欢。","categories":[{"name":"武夷岩茶","slug":"武夷岩茶","permalink":"https://blog.fenxiangz.com/categories/%E6%AD%A6%E5%A4%B7%E5%B2%A9%E8%8C%B6/"}],"tags":[{"name":"武夷岩茶","slug":"武夷岩茶","permalink":"https://blog.fenxiangz.com/tags/%E6%AD%A6%E5%A4%B7%E5%B2%A9%E8%8C%B6/"},{"name":"茶知识","slug":"茶知识","permalink":"https://blog.fenxiangz.com/tags/%E8%8C%B6%E7%9F%A5%E8%AF%86/"}]},{"title":"HttpClient连接池原理及一次连接时序图","slug":"java/util/2020-12-23_HttpClient连接池原理及一次连接时序图","date":"2020-12-23T00:00:00.000Z","updated":"2020-12-23T05:47:04.368Z","comments":true,"path":"post/java/util/2020-12-23_HttpClient连接池原理及一次连接时序图.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/util/2020-12-23_HttpClient%E8%BF%9E%E6%8E%A5%E6%B1%A0%E5%8E%9F%E7%90%86%E5%8F%8A%E4%B8%80%E6%AC%A1%E8%BF%9E%E6%8E%A5%E6%97%B6%E5%BA%8F%E5%9B%BE.html","excerpt":"","text":"HttpClient介绍HttpClient是一个实现了http协议的开源Java客户端工具库，可以通过程序发送http请求。 1.1 HttpClient发送请求和接收响应1.1.1 代码示例以Get请求为例，以下代码获得google主页内容并将返回结果打印出来。 public final static void main(String[] args) throws Exception &#123; HttpClient httpclient = new DefaultHttpClient(); try &#123; HttpGet httpget = new HttpGet(&quot;http://www.google.com/&quot;); System.out.println(&quot;executing request &quot; + httpget.getURI()); // 创建response处理器 ResponseHandler&lt;String&gt; responseHandler = new BasicResponseHandler(); String responseBody = httpclient.execute(httpget, responseHandler); System.out.println(&quot;----------------------------------------&quot;); System.out.println(responseBody); System.out.println(&quot;----------------------------------------&quot;); &#125; finally &#123; //HttpClient不再使用时，关闭连接管理器以保证所有资源的释放 httpclient.getConnectionManager().shutdown(); &#125; &#125; 1.1.2 时序图httpClient执行一次请求，即运行一次httpClient.execute()方法，时序图如下： 1.1.3 时序图说明1.1.3.1 时序图编号说明 1.1、1.2、1.3等均为操作1的子操作，即：操作1 execute()中又分别调用了操作1.1 createClientConnectionManager()、操作1.2 createClientRequestDirector()以及操作1.3 requestDirector 对象的execute()方法等，以此类推。 按时间先后顺序分别编号为1,2,3等，以此类推。 1.1.3.2 主要类说明 对于图中各对象，httpClient jar包中均提供对应的接口及相应的实现类。 图中直接与服务器进行socket通信的是最右端接口OperatedClientConnection某一实现类的对象，图中从右到左进行了层层的封装，最终开发人员直接使用的是接口HttpClient某一实现类的对象进行请求的发送和响应的接收（如2.1.1代码示例）。 时序图中各对象所在类关系如下图类图所示（仅列出图中所出现的各个类及方法，参数多的方法省略部分参数，其他类属性和操作请参照源码）： 1.1.3.2.1 接口OperatedClientConnection 该接口对应一个http连接，与服务器端建立socket连接进行通信。 1.1.3.2.2 接口ManagedClientConnection 该接口对一个http连接OperatedClientConnection进行封装，ManagedClientConnection维持一个PoolEntry&lt;HttpRoute, OperatedClientConnection&gt;路由和连接的对应。提供方法获得对应连接管理器，对http连接的各类方法，如建立连接，获得相应，关闭连接等进行封装。 1.1.3.2.3 接口RequestDirector RequestDirector为消息的发送执行者，该接口负责消息路由的选择和可能的重定向，消息的鉴权，连接的分配回收（调用ClientConnectionManager相关方法），建立，关闭等并控制连接的保持。 连接是否保持以及保持时间默认原则如下： 连接是否保持：客户端如果希望保持长连接，应该在发起请求时告诉服务器希望服务器保持长连接（http 1.0设置connection字段为keep-alive，http 1.1字段默认保持）。根据服务器的响应来确定是否保持长连接，判断原则如下： 检查返回response报文头的Transfer-Encoding字段，若该字段值存在且不为chunked，则连接不保持，直接关闭。其他情况进入下一步。 检查返回的response报文头的Content-Length字段，若该字段值为空或者格式不正确（多个长度，值不是整数），则连接不保持，直接关闭。其他情况进入下一步 检查返回的response报文头的connection字段（若该字段不存在，则为Proxy-Connection字段）值 如果这俩字段都不存在，则http 1.1版本默认为保持，将连接标记为保持， 1.0版本默认为连接不保持，直接关闭。 如果字段存在，若字段值为close 则连接不保持，直接关闭；若字段值为keep-alive则连接标记为保持。 连接保持时间：连接交换至连接管理时，若连接标记为保持，则将由连接管理器保持一段时间；若连接没有标记为保持，则直接从连接池中删除并关闭entry。连接保持时，保持时间规则如下： 保持时间计时开始时间为连接交换至连接池的时间。 保持时长计算规则为：获取keep-alive字段中timeout属性的值， 若该字段存在，则保持时间为 timeout属性值*1000，单位毫秒。 若该字段不存在，则连接保持时间设置为-1，表示为无穷。 响应头日志示例： 17:59:42.051 [main] DEBUG org.apache.http.headers - &lt;&lt; Keep-Alive: timeout=5, max=100 17:59:42.051 [main] DEBUG org.apache.http.headers - &lt;&lt; Connection: Keep-Alive 17:59:42.051 [main] DEBUG org.apache.http.headers - &lt;&lt; Content-Type: text/html; charset=utf-8 17:59:42.062 [main] DEBUG c.ebupt.omp.sop.srmms.SopHttpClient - Connection can be kept alive for 5000 MILLISECONDS 若需要修改连接的保持及重用默认原则，则需编写子类继承自AbstractHttpClient，分别覆盖其 createConnectionReuseStrategy() 和createConnectionKeepAliveStrategy() 方法。 1.1.3.2.4 接口ClientConnectionManager ClientConnectionManager为连接池管理器，是线程安全的。Jar包中提供的具体实现类有BasicClientConnectionManager和PoolingClientConnectionManager。其中BasicClientConnectionManager只管理一个连接。PoolingClientConnectionManager管理连接池。 若有特殊需要，开发人员可自行编写连接管理器实现该接口。 连接管理器自动管理连接的分配以及回收工作，并支持连接保持以及重用。连接保持以及重用由RequestDirector进行控制。 1.1.3.2.5 接口HttpClient 接口HttpClient为开发人员直接使用的发送请求和接收响应的接口，是线程安全的。jar包中提供的实现类有：AbstractHttpClient, DefaultHttpClient, AutoRetryHttpClient, ContentEncodingHttpClient, DecompressingHttpClient, SystemDefaultHttpClient。其中其他所有类都继承自抽象类AbStractHttpClient，该类使用了门面模式，对http协议的处理进行了默认的封装,包括默认连接管理器，默认消息头，默认消息发送等，开发人员可以覆盖其中的方法更改其默认设置。 AbstractHttpClient默认设置连接管理器为BasicClientConnectionManager。若要修改连接管理器，则应该采用以下方式之一： 初始化时，传入连接池，例如： ClientConnectionManager connManager = new PoolingClientConnectionManager(); HttpClient httpclient = new DefaultHttpClient(connManager); 编写httpClient接口的实现类，继承自AbstractHttpClient并覆盖其createClientConnectionManager()方法，在方法中创建自己的连接管理器。 1.1.3.3 方法说明createClientConnectionManager()，创建连接池，该方法为protected。子类可覆盖修改默认连接池。 createClientRequestDirector()，创建请求执行者，该方法为protected。子类可覆盖但一般不需要。 httpClient中调用1.2方法所创建的请求执行者requestDirector的execute()方法。该方法中依次调用如下方法： 1.3.1调用连接管理器的requestConnection(route, userToken)方法，该方法调用连接池httpConnPool的lease方法，创建一个Future。Futrue用法参见Java标准API。返回clientConnectionRequest。 1.3.2调用clientConnectionRequest的getConnection(timeout, TimeUnit.MILLISECONDS)方法，该方法负责将连接池中可用连接分配给当前请求，具体如下： 创建clientConnectionOperator。 执行1.3.1中创建的Future的任务，该任务获得当前可用的poolEntry&lt;router，OperatedClientConnection&gt;并封装成managedClientConnectionImpl返回。 1.3.3调用 tryConnect(roureq, context)方法，该方法最终调用OperatedClientConnection的openning方法，与服务器建立socket连接。 1.3.4调用 tryExecute(roureq, context)方法，该方法最终调用OperatedClientConnection的receiveResponseHeader（）和receiveResponseEntity（）获得服务器响应。 1.3.5 判断连接是否保持用来重用，若保持，则设置保持时间，并将连接标记为可重用不保持则调用managedClientConnectionImpl的close方法关闭连接，该方法最终调用OperatedClientConnection的close()方法关闭连接。 最终respose返回至httpClient。 发送请求的线程需处理当前连接，若已被标记为重用，则交还至连接池管理器；否则，关闭当前连接。（使用响应处理器ResponseHanler）。本次请求结束。 1.2 HttpClient连接池若连接管理器配置为PoolingClientConnectionManager，则httpClient将使用连接池来管理连接的分配，回收等操作。 1.2.1 连接池结构连接池结构图如下，其中： PoolEntry&lt;HttpRoute, OperatedClientConnection&gt;为路由和连接的对应。 routeToPool可以多个（图中仅示例两个）；图中各队列大小动态变化，并不相等； maxTotal限制的是外层httpConnPool中leased集合和available队列的总和的大小，leased和available的大小没有单独限制； 同理：maxPerRoute限制的是routeToPool中leased集合和available队列的总和的大小； 1.2.2 连接池工作原理1.2.2.1 分配连接分配连接给当前请求包括两部分：1从连接池获取可用连接PoolEntry；2.将连接与当前请求绑定。其中第一部分从连接池获取可用连接的过程为： 1 获取route对应连接池routeToPool中可用的连接，有则返回该连接。若没有则转入下一步。 2 若routeToPool和外层HttpConnPool连接池均还有可用的空间，则新建连接，并将该连接作为可用连接返回；否则进行下一步 3 将当前请求放入pending队列，等待执行。 4 上述过程中包含各个队列和集合的删除，添加等操作以及各种判断条件，具体流程如下： 1.2.2.2 回收连接连接用完之后连接池需要进行回收，具体流程如下： 1 若当前连接标记为重用，则将该连接从routeToPool中的leased集合删除，并添加至available队列，同样的将该请求从外层httpConnPool的leased集合删除，并添加至其available队列。同时唤醒该routeToPool的pending队列的第一个PoolEntryFuture。将其从pending队列删除，并将其从外层httpConnPool的pending队列中删除。 2 若连接没有标记为重用，则分别从routeToPool和外层httpConnPool中删除该连接，并关闭该连接。 1.2.2.3 过期和空闲连接的关闭连接如果标记为保持时，将由连接管理器保持一段时间，此时连接可能出现的情况是： 连接处于空闲状态，时间已超过连接保持时间 连接处于空闲状态，时间没有超过连接保持时间 以上两种情况中，随时都会出现连接的服务端已关闭的情况，而此时连接的客户端并没有阻塞着去接受服务端的数据，所以客户端不知道连接已关闭，无法关闭自身的socket。 连接池提供的方法： 首先连接池在每个请求获取连接时，都会在RouteToPool的available队列获取Entry并检测此时Entry是否已关闭或者已过期，若是则关闭并移除该Entry。 closeExpiredConnections()该方法关闭超过连接保持时间的空闲连接。 closeIdleConnections(timeout,tunit)该方法关闭空闲时间超过timeout的连接，空闲时间从交还给连接管理器时开始，不管是否已过期超过空闲时间则关闭。所以Idle时间应该设置的尽量长一点。 以上两个方法连接关闭的过程均是： 关闭entry; RouteToPool中删除当前entry。先删available队列中的，如果没有，再删除leased集合中的。 httpConnPool中删除当前entry。删除过程同RouteToPool 唤醒阻塞在RouteToPool中的第一个future。 1.3 相关原理说明1.3.1 Tcp连接的关闭Http连接实际上在传输层建立的是tcp连接，最终利用的是socket进行通信。http连接的保持和关闭实际上都和TCP连接的关闭有关。TCP关闭过程如下图： 说明： TCP连接程序中使用socket编程进行实现。一条TCP是一条抽象的连接通道，由通信双方的IP+端口号唯一确定，两端分别通过socket实例进行操作，一个socket实例包括一个输入通道和输出通道，一端的输出通道为另一端的输入通道。 Tcp连接的关闭是连接的两端分别都需要进行关闭（调用close(socket)，该函数执行发送FIN，等待ACK等图示操作）。实际上没有客户端和服务端的区别，只有主动关闭和被动关闭的区别。对于上层的其http连接，实际上也就是http服务端主动关闭或者http客户端主动关闭，而不管谁主动，最终服务端和客户端都需要调用close(socket)关闭连接。 主动关闭的一端A调用了close函数之后，若另一端B并没有阻塞着等待着数据，就无法检测到连接的A端已关闭，就没法关闭自身的socket，造成资源的浪费。http连接都是一次请求和响应，之后便交回给连接管理池，因此在http连接池中应当能够移除已过期或者空闲太久的连接，因为他们可能已经被服务器端关闭或者客户端短期内不再使用。 TIME_WAIT状态： 可靠地实现TCP全双工连接的终止 在进行关闭连接四路握手协议时，最后的ACK是由主动关闭端发出的，如果这个最终的ACK丢失，被动关闭端将重发最终的FIN，因此主动关闭端必须维护状态信息允许它重发最终的ACK。如果不维持这个状态信息，那么主动关闭端将发送RST分节（复位），被动关闭端将此分节解释成一个错误（在java中会抛出connection reset的SocketException)。因而，要实现TCP全双工连接的正常终止，主动关闭的客户端必须维持状态信息进入TIME_WAIT状态。 允许老的重复分节在网络中消逝 TCP分节可能由于路由器异常而“迷途”，在迷途期间，TCP发送端可能因确认超时而重发这个分节，迷途的分节在路由器修复后也会被送到最终目的地，这个原来的迷途分节就称为lost duplicate。在关闭一个TCP连接后，马上又重新建立起一个相同的IP地址和端口之间的TCP连接，后一个连接被称为前一个连接的化身（incarnation)，那么有可能出现这种情况，前一个连接的迷途重复分组在前一个连接终止后出现，从而被误解成从属于新的化身。为了避免这个情况，TCP不允许处于TIME_WAIT状态的连接启动一个新的化身，因为TIME_WAIT状态持续2MSL，就可以保证当成功建立一个TCP连接的时候，来自连接先前化身的重复分组已经在网络中消逝。 HttpClient最佳实践2.1 总原则2.1.1 版本原Commons HttpClient：3.x不再升级维护，使用Apache HttpComponents的HttpClient代替。Pom文件修改如下： 1 原maven依赖： &lt;dependency&gt; &lt;groupId&gt;commons-httpclient&lt;/groupId&gt; &lt;artifactId&gt;commons-httpclient&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;/dependency&gt; 2 替换为： &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.2.1&lt;/version&gt; &lt;/dependency&gt; 2.1.2 使用http连接池管理器 编写类继承自DefaultHttpClient(以下假设为SopHttpClient)，覆盖其createClientConnectionManager()方法，方法中创建连接池管理器。 开启一个线程（假设为IdleConnectionMonitorThread）用来清除连接池中空闲和过期的连接。 2.1.3 保持HttpClient单例Spring配置中使用默认scope，即单例模式，其他类使用时由Spring配置进行依赖注入，不要使用new方法。SopHttpClient应该提供方法destroy()并配置在Spring销毁该bean前调用，destory()方法中关闭对应连接池管理器和监控线程IdleConnectionMonitorThread。 2.1.4 异常处理机制（请求和响应）：编写类实现接口HttpRequestRetryHandler（可参照默认实现DefaultHttpRequestRetryHandler），并覆盖AbstractHttpClient中的createHttpRequestRetryHandler()方法创建新的重试处理机制。 2.1.5 参数可配置各参数（连接池默认ip、端口和大小等，超时时间等）尽量都集中在SopHttpClient类中，设置为由Spring进行统一配置，且提供接口在程序中修改。 2.1.6 保证连接交回至连接池管理器2.1.6.1 方式HttpResponse response = httpclient.execute(httpMethod); HttpEntity entity = response.getEntity(); 这两段代码返回的entity是HttpEntity的实现类BasicManagedEntity。此时与本次请求关联的连接尚未归还至连接管理器。需要调用以下两条语句： InputStream instream = entity.getContent();//获得响应具体内容 //处理响应：代码省略 instream.close();//关闭输入流同时会将连接交回至连接处理器 2.1.6.2 使用默认的响应处理器BasicResponseHandler HttpClient Jar包中提供BasicResponseHandler。如果返回的类型能确定需要解码为String类型的话，推荐使用该响应处理器。 该处理器解码http连接响应字节流为String类型，对返回码&gt;=300的响应进行了异常封装，并能够保证连接交还给连接池管理器。 该处理器将字节解码为字符的过程依次如下： 1 如果响应http报文Head部分由指定的charset，则使用该charset进行解码，否则进行下一步。例如使用UTF-8解码以下响应： 17:59:42.051 [main] DEBUG org.apache.http.headers - &lt;&lt; Content-Type: text/html; charset=utf-8 2 如果响应报文未执行charset，则使用传入EntityUntils.toString()时指定的charset进行解码。否则进行下一步 3 使用ISO-8859-1进行解码。 2.1.6.3 BasicManagedEntity关闭连接池管理器原理2.1.6.3.1 BasicManagedEntity实现了三个接口:HttpEntity，ConnectionReleaseTrigger, EofSensorWatcher。 调用BasicManagedEntity的getContent方法时，实际上初始化了EofSensorInputStream的实例，并将BasicManagedEntity当前对象自身作为EofSensorWatcher传入。 //BasicManagedEntity类的继承体系，HttpEntityWrapper实现了接口HttpEntity public class BasicManagedEntity extends HttpEntityWrapper implements ConnectionReleaseTrigger, EofSensorWatcher BasicManagedEntity的getContent方法： @Override public InputStream getContent() throws IOException &#123; return new EofSensorInputStream(wrappedEntity.getContent(), this); &#125; // EofSensorInputStream构造函数声明 public EofSensorInputStream(final InputStream in,final EofSensorWatcher watcher); 2.1.6.3.2 调用EofSensorInputStream的close方法，该方法调用自身的checkClose()方法，checkClose()方法中调入了传入的EofSensorWatcher watcher的streamClosed()方法并关闭输入流，由于上一步骤中实际传入的watcher是BasicManagedEntity的实例，因此实际上调用的是BasicManagedEntity的streamClose()方法。 //close方法 @Override public void close() throws IOException &#123; // tolerate multiple calls to close() selfClosed = true; checkClose(); &#125; //checkClose方法 protected void checkClose() throws IOException &#123; if (wrappedStream != null) &#123; try &#123; boolean scws = true; // should close wrapped stream? if (eofWatcher != null) scws = eofWatcher.streamClosed(wrappedStream); if (scws) wrappedStream.close(); &#125; finally &#123; wrappedStream = null; &#125; &#125; &#125; 2.1.6.3.3 BasicManagedEntity的streamClose()方法中将连接交回至连接池管理器。 public boolean streamClosed(InputStream wrapped) throws IOException &#123; try &#123; if (attemptReuse &amp;&amp; (managedConn != null)) &#123; boolean valid = managedConn.isOpen(); // this assumes that closing the stream will // consume the remainder of the response body: try &#123; wrapped.close(); managedConn.markReusable(); &#125; catch (SocketException ex) &#123; if (valid) &#123; throw ex; &#125; &#125; &#125; &#125; finally &#123; releaseManagedConnection(); &#125; return false; } 2.1.7 其他HttpClient 提供了非常灵活的架构，同时提供了很多接口，需要修改时，找到对应接口和默认实现类，参照默认实现类进行修改即可（或继承默认实现类，覆盖其对应方法）。通常需要更改的类有AbstractHttpClient和各种handler以及Strategy 文章转载自：https://developer.aliyun.com/article/11893","categories":[{"name":"RPC","slug":"RPC","permalink":"https://blog.fenxiangz.com/categories/RPC/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"HttpClient","slug":"HttpClient","permalink":"https://blog.fenxiangz.com/tags/HttpClient/"}]},{"title":"Java Synchronized 原理图","slug":"java/basic/2012-12-22_Java_Synchronized原理图","date":"2020-12-22T00:00:00.000Z","updated":"2020-12-21T16:38:03.850Z","comments":true,"path":"post/java/basic/2012-12-22_Java_Synchronized原理图.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/basic/2012-12-22_Java_Synchronized%E5%8E%9F%E7%90%86%E5%9B%BE.html","excerpt":"","text":"","categories":[{"name":"Java 基础","slug":"Java-基础","permalink":"https://blog.fenxiangz.com/categories/Java-%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"同步","slug":"同步","permalink":"https://blog.fenxiangz.com/tags/%E5%90%8C%E6%AD%A5/"}]},{"title":"使用JS脚本批量去除文件名中部分关键字","slug":"util/2020-12-20_批量去除文件名部分关键字","date":"2020-12-20T00:00:00.000Z","updated":"2020-12-20T16:47:02.984Z","comments":true,"path":"post/util/2020-12-20_批量去除文件名部分关键字.html","link":"","permalink":"https://blog.fenxiangz.com/post/util/2020-12-20_%E6%89%B9%E9%87%8F%E5%8E%BB%E9%99%A4%E6%96%87%E4%BB%B6%E5%90%8D%E9%83%A8%E5%88%86%E5%85%B3%E9%94%AE%E5%AD%97.html","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940var fs = require(&#x27;fs&#x27;); var KEY_WORD = &#x27;要去掉的关键字&#x27;;var PATH = &#x27;/home/指定目录/&#x27;; //遍历目录得到文件信息function run(path, callback) &#123; var files = fs.readdirSync(path); files.forEach(function(file)&#123; f = path + &#x27;/&#x27; + file; if (fs.statSync(f).isFile()) &#123; callback(path, file); &#125; else &#123; //目录递归处理 run(f, callRename); &#125; &#125;);&#125; // 修改文件名称function rename(oldPath, newPath) &#123; fs.rename(oldPath, newPath, function(err) &#123; if (err) &#123; throw err; &#125; &#125;);&#125;function callRename(path, fileName) &#123; let oldPath = path + &#x27;/&#x27; + fileName; // 源文件路径 let newPath = path + &#x27;/&#x27; + fileName.replace(KEY_WORD, &#x27;&#x27;); // 新文件全路径 console.log(newPath); rename(oldPath, newPath);&#125;// 运行入口run(PATH, callRename);","categories":[{"name":"资源导航","slug":"资源导航","permalink":"https://blog.fenxiangz.com/categories/%E8%B5%84%E6%BA%90%E5%AF%BC%E8%88%AA/"}],"tags":[{"name":"文件名批量修改","slug":"文件名批量修改","permalink":"https://blog.fenxiangz.com/tags/%E6%96%87%E4%BB%B6%E5%90%8D%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9/"}]},{"title":"原来 8 张图，就可以搞懂「零拷贝」了","slug":"linux/2020-12-16_原来8张图，就可以搞懂「零拷贝」了","date":"2020-12-16T00:00:00.000Z","updated":"2020-12-20T16:47:02.979Z","comments":true,"path":"post/linux/2020-12-16_原来8张图，就可以搞懂「零拷贝」了.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2020-12-16_%E5%8E%9F%E6%9D%A58%E5%BC%A0%E5%9B%BE%EF%BC%8C%E5%B0%B1%E5%8F%AF%E4%BB%A5%E6%90%9E%E6%87%82%E3%80%8C%E9%9B%B6%E6%8B%B7%E8%B4%9D%E3%80%8D%E4%BA%86.html","excerpt":"","text":"原文：https://xie.infoq.cn/article/8d19a4c691918d313e60296d7 前言磁盘可以说是计算机系统最慢的硬件之一，读写速度相差内存 10 倍以上，所以针对优化磁盘的技术非常的多，比如零拷贝、直接 I/O、异步 I/O 等等，这些优化的目的就是为了提高系统的吞吐量，另外操作系统内核中的磁盘高速缓存区，可以有效的减少磁盘的访问次数。 这次，我们就以「文件传输」作为切入点，来分析 I/O 工作方式，以及如何优化传输文件的性能。 正文为什么要有 DMA 技术?在没有 DMA 技术前，I/O 的过程是这样的： CPU 发出对应的指令给磁盘控制器，然后返回； 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个中断； CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。 为了方便你理解，我画了一副图： 可以看到，整个数据的传输过程，都要需要 CPU 亲自参与搬运数据的过程，而且这个过程，CPU 是不能做其他事情的。 简单的搬运几个字符数据那没问题，但是如果我们用千兆网卡或者硬盘传输大量数据的时候，都用 CPU 来搬运的话，肯定忙不过来。 计算机科学家们发现了事情的严重性后，于是就发明了 DMA 技术，也就是直接内存访问（Direct Memory Access） 技术。 什么是 DMA 技术？简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。 那使用 DMA 控制器进行数据传输的过程究竟是什么样的呢？下面我们来具体看看。 具体过程： 用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态； 操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务； DMA 进一步将 I/O 请求发送给磁盘； 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满； DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务； 当 DMA 读取了足够多的数据，就会发送中断信号给 CPU； CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回； 可以看到， 整个数据传输的过程，CPU 不再参与数据搬运的工作，而是全程由 DMA 完成，但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪里传输到哪里，都需要 CPU 来告诉 DMA 控制器。 早期 DMA 只存在在主板上，如今由于 I/O 设备越来越多，数据传输的需求也不尽相同，所以每个 I/O 设备里面都有自己的 DMA 控制器。 传统的文件传输有多糟糕？如果服务端要提供文件传输的功能，我们能想到的最简单的方式是：将磁盘上的文件读取出来，然后通过网络协议发送给客户端。 传统 I/O 的工作方式是，数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的 I/O 接口从磁盘读取或写入。 代码通常如下，一般会需要两个系统调用： 12345read(file, tmp_buf, len);write(socket, tmp_buf, len); 代码很简单，虽然就两行代码，但是这里面发生了不少的事情。 首先，期间共发生了 4 次用户态与内核态的上下文切换，因为发生了两次系统调用，一次是 read() ，一次是 write()，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。 上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。 其次，还发生了 4 次数据拷贝，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程： 第一次拷贝，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。 第二次拷贝，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。 第三次拷贝，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。 第四次拷贝，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。 我们回过头看这个文件传输的过程，我们只是搬运一份数据，结果却搬运了 4 次，过多的数据拷贝无疑会消耗 CPU 资源，大大降低了系统性能。 这种简单又传统的文件传输方式，存在冗余的上文切换和数据拷贝，在高并发系统里是非常糟糕的，多了很多不必要的开销，会严重影响系统性能。 所以，要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数。 如何优化文件传输的性能？ 先来看看，如何减少「用户态与内核态的上下文切换」的次数呢？ 读取磁盘数据的时候，之所以要发生上下文切换，这是因为用户空间没有权限操作磁盘或网卡，内核的权限最高，这些操作设备的过程都需要交由操作系统内核来完成，所以一般要通过内核去完成某些任务的时候，就需要使用操作系统提供的系统调用函数。 而一次系统调用必然会发生 2 次上下文切换：首先从用户态切换到内核态，当内核执行完任务后，再切换回用户态交由进程代码执行。 所以，要想减少上下文切换到次数，就要减少系统调用的次数。 再来看看，如何减少「数据拷贝」的次数？ 在前面我们知道了，传统的文件传输方式会历经 4 次数据拷贝，而且这里面，「从内核的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到 socket 的缓冲区里」，这个过程是没有必要的。 因为文件传输的应用场景中，在用户空间我们并不会对数据「再加工」，所以数据实际上可以不用搬运到用户空间，因此用户的缓冲区是没有必要存在的。 如何实现零拷贝？零拷贝技术实现的方式通常有 2 种： mmap + write sendfile 下面就谈一谈，它们是如何减少「上下文切换」和「数据拷贝」的次数。 mmap + write在前面我们知道，read() 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用 mmap() 替换 read() 系统调用函数。 12345buf &#x3D; mmap(file, len);write(sockfd, buf, len); mmap() 系统调用函数会直接把内核缓冲区里的数据「映射」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。 具体过程如下： 应用进程调用了 mmap() 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区； 应用进程再调用 write()，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据； 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。 我们可以得知，通过使用 mmap() 来代替 read()， 可以减少一次数据拷贝的过程。 但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。 sendfile 在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 sendfile()，函数形式如下： 12345#include&lt;sys&#x2F;socket.h&gt;ssize_tsendfile(int out_fd, int in_fd, off_t *offset, size_t count); 它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。 首先，它可以替代前面的 read() 和 write() 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。 其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图： 但是这还不是真正的零拷贝技术，如果网卡支持 SG-DMA（The Scatter-Gather Direct Memory Access）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。 你可以在你的 Linux 系统通过下面这个命令，查看网卡是否支持 scatter-gather 特性： 12345$ ethtool -k eth0 | grep scatter-gatherscatter-gather: on 于是，从 Linux 内核 2.4 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， sendfile() 系统调用的过程发生了点变化，具体过程如下： 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里； 第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝； 所以，这个过程之中，只进行了 2 次数据拷贝，如下图： 这就是所谓的零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。。 零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。 所以，总体来看，零拷贝技术可以把文件传输的性能提高至少一倍以上。 使用零拷贝技术的项目事实上，Kafka 这个开源项目，就利用了「零拷贝」技术，从而大幅提升了 I/O 的吞吐率，这也是 Kafka 在处理海量数据为什么这么快的原因之一。 如果你追溯 Kafka 文件传输的代码，你会发现，最终它调用了 Java NIO 库里的 transferTo 方法： 123456789@OverridepubliclongtransferFrom(FileChannel fileChannel, long position, long count)throws IOException &#123; return fileChannel.transferTo(position, count, socketChannel);&#125; 如果 Linux 系统支持 sendfile() 系统调用，那么 transferTo() 实际上最后就会使用到 sendfile() 系统调用函数。 曾经有大佬专门写过程序测试过，在同样的硬件条件下，传统文件传输和零拷拷贝文件传输的性能差异，你可以看到下面这张测试数据图，使用了零拷贝能够缩短 65% 的时间，大幅度提升了机器传输数据的吞吐量。 另外，Nginx 也支持零拷贝技术，一般默认是开启零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置如下： 1234567891011http &#123;... sendfile on...&#125; sendfile 配置的具体意思: 设置为 on 表示，使用零拷贝技术来传输文件：sendfile ，这样只需要 2 次上下文切换，和 2 次数据拷贝。 设置为 off 表示，使用传统的文件传输技术：read + write，这时就需要 4 次上下文切换，和 4 次数据拷贝。 当然，要使用 sendfile，Linux 内核版本必须要 2.1 以上的版本。 PageCache 有什么作用？回顾前面说道文件传输过程，其中第一步都是先需要先把磁盘文件数据拷贝「内核缓冲区」里，这个「内核缓冲区」实际上是磁盘高速缓存（PageCache）。 由于零拷贝使用了 PageCache 技术，可以使得零拷贝进一步提升了性能，我们接下来看看 PageCache 是如何做到这一点的。 读写磁盘相比读写内存的速度慢太多了，所以我们应该想办法把「读写磁盘」替换成「读写内存」。于是，我们会通过 DMA 把磁盘里的数据搬运到内存里，这样就可以用读内存替换读磁盘。 但是，内存空间远比磁盘要小，内存注定只能拷贝磁盘里的一小部分数据。 那问题来了，选择哪些磁盘数据拷贝到内存呢？ 我们都知道程序运行的时候，具有「局部性」，所以通常，刚被访问的数据在短时间内再次被访问的概率很高，于是我们可以用 PageCache 来缓存最近被访问的数据，当空间不足时淘汰最久未被访问的缓存。 所以，读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存 PageCache 中。 还有一点，读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是非常耗时的，为了降低它的影响，PageCache 使用了「预读功能」。 比如，假设 read 方法每次只会读 32 KB 的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会把其后面的 32～64 KB 也读取到 PageCache，这样后面读取 32～64 KB 的成本就很低，如果在 32～64 KB 淘汰出 PageCache 前，进程读取到它了，收益就非常大。 所以，PageCache 的优点主要是两个： 缓存最近被访问的数据； 预读功能； 这两个做法，将大大提高读写磁盘的性能。 但是，在传输大文件（GB 级别的文件）的时候，PageCache 会不起作用，那就白白浪费 DMA 多做的一次数据拷贝，造成性能的降低，即使使用了 PageCache 的零拷贝也会损失性能 这是因为如果你有很多 GB 级别文件需要传输，每当用户访问这些大文件的时候，内核就会把它们载入 PageCache 中，于是 PageCache 空间很快被这些大文件占满。 另外，由于文件太大，可能某些部分的文件数据被再次访问的概率比较低，这样就会带来 2 个问题： PageCache 由于长时间被大文件占据，其他「热点」的小文件可能就无法充分使用到 PageCache，于是这样磁盘读写的性能就会下降了； PageCache 中的大文件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷贝到 PageCache 一次； 所以，针对大文件的传输，不应该使用 PageCache，也就是说不应该使用零拷贝技术，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，这样在高并发的环境下，会带来严重的性能问题。 大文件传输用什么方式实现？那针对大文件的传输，我们应该使用什么方式呢？ 我们先来看看最初的例子，当调用 read 方法读取文件时，进程实际上会阻塞在 read 方法调用，因为要等待磁盘数据的返回，如下图： 具体过程： 当调用 read 方法时，会阻塞着，此时内核会向磁盘发起 I/O 请求，磁盘收到请求后，便会寻址，当磁盘数据准备好后，就会向内核发起 I/O 中断，告知内核磁盘数据已经准备好； 内核收到 I/O 中断后，就将数据从磁盘控制器缓冲区拷贝到 PageCache 里； 最后，内核再把 PageCache 中的数据拷贝到用户缓冲区，于是 read 调用就正常返回了。 对于阻塞的问题，可以用异步 I/O 来解决，它工作方式如下图： 它把读操作分为两部分： 前半部分，内核向磁盘发起读请求，但是可以不等待数据就位就可以返回，于是进程此时可以处理其他任务； 后半部分，当内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的通知，再去处理数据； 而且，我们可以发现，异步 I/O 并没有涉及到 PageCache，所以使用异步 I/O 就意味着要绕开 PageCache。 绕开 PageCache 的 I/O 叫直接 I/O，使用 PageCache 的 I/O 则叫缓存 I/O。通常，对于磁盘，异步 I/O 只支持直接 I/O。 前面也提到，大文件的传输不应该使用 PageCache，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache。 于是，在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术。 直接 I/O 应用场景常见的两种： 应用程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启； 传输大文件的时候，由于大文件难以命中 PageCache 缓存，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用直接 I/O。 另外，由于直接 I/O 绕过了 PageCache，就无法享受内核的这两点的优化： 内核的 I/O 调度算法会缓存尽可能多的 I/O 请求在 PageCache 中，最后「合并」成一个更大的 I/O 请求再发给磁盘，这样做是为了减少磁盘的寻址操作； 内核也会「预读」后续的 I/O 请求放在 PageCache 中，一样是为了减少对磁盘的操作； 于是，传输大文件的时候，使用「异步 I/O + 直接 I/O」了，就可以无阻塞地读取文件了。 所以，传输文件的时候，我们要根据文件的大小来使用不同的方式： 传输大文件的时候，使用「异步 I/O + 直接 I/O」； 传输小文件的时候，则使用「零拷贝技术」； 在 nginx 中，我们可以用如下配置，来根据文件的大小来使用不同的方式： 1234567891011location &#x2F;video&#x2F; &#123; sendfile on; aio on; directio 1024m; &#125; 当文件大小大于 directio 值后，使用「异步 I/O + 直接 I/O」，否则使用「零拷贝技术」。 总结早期 I/O 操作，内存与磁盘的数据传输的工作都是由 CPU 完成的，而此时 CPU 不能执行其他任务，会特别浪费 CPU 资源。 于是，为了解决这一问题，DMA 技术就出现了，每个 I/O 设备都有自己的 DMA 控制器，通过这个 DMA 控制器，CPU 只需要告诉 DMA 控制器，我们要传输什么数据，从哪里来，到哪里去，就可以放心离开了。后续的实际数据传输工作，都会由 DMA 控制器来完成，CPU 不需要参与数据传输的工作。 传统 IO 的工作方式，从硬盘读取数据，然后再通过网卡向外发送，我们需要进行 4 上下文切换，和 4 次数据拷贝，其中 2 次数据拷贝发生在内存里的缓冲区和对应的硬件设备之间，这个是由 DMA 完成，另外 2 次则发生在内核态和用户态之间，这个数据搬移工作是由 CPU 完成的。 为了提高文件传输的性能，于是就出现了零拷贝技术，它通过一次系统调用（sendfile 方法）合并了磁盘读取与网络发送两个操作，降低了上下文切换次数。另外，拷贝数据都是发生在内核中的，天然就降低了数据拷贝的次数。 Kafka 和 Nginx 都有实现零拷贝技术，这将大大提高文件传输的性能。 零拷贝技术是基于 PageCache 的，PageCache 会缓存最近访问的数据，提升了访问缓存数据的性能，同时，为了解决机械硬盘寻址慢的问题，它还协助 I/O 调度算法实现了 IO 合并与预读，这也是顺序读比随机读性能好的原因。这些优势，进一步提升了零拷贝的性能。 需要注意的是，零拷贝技术是不允许进程对文件内容作进一步的加工的，比如压缩数据再发送。 另外，当传输大文件时，不能使用零拷贝，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，并且大文件的缓存命中率不高，这时就需要使用「异步 IO + 直接 IO 」的方式。 在 Nginx 里，可以通过配置，设定一个文件大小阈值，针对大文件使用异步 IO 和直接 IO，而对小文件使用零拷贝。","categories":[{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/categories/IO/"}],"tags":[{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"},{"name":"零拷贝","slug":"零拷贝","permalink":"https://blog.fenxiangz.com/tags/%E9%9B%B6%E6%8B%B7%E8%B4%9D/"},{"name":"异步IO","slug":"异步IO","permalink":"https://blog.fenxiangz.com/tags/%E5%BC%82%E6%AD%A5IO/"},{"name":"直接IO","slug":"直接IO","permalink":"https://blog.fenxiangz.com/tags/%E7%9B%B4%E6%8E%A5IO/"}]},{"title":"Redis存储优化","slug":"dateabase/Redis/2020_12_08_Redis清理方法","date":"2020-12-08T00:00:00.000Z","updated":"2020-12-20T16:47:02.945Z","comments":true,"path":"post/dateabase/Redis/2020_12_08_Redis清理方法.html","link":"","permalink":"https://blog.fenxiangz.com/post/dateabase/Redis/2020_12_08_Redis%E6%B8%85%E7%90%86%E6%96%B9%E6%B3%95.html","excerpt":"","text":"bigkeys 扫描/redis-cli -h localhost -p 6379 --bigkeys 全量扫描通过以下脚本扫描 Redis中的全量Keys， Keys存量较大的情况下，可以分多次优化处理。比如：keys.txt文件收集到100MB时，停止，先优化一部分；优化完，再扫描，再优化。脚本如下，注意确保脚本在 ./redis-cli 统计目录，或修改为绝对 redis-cli 绝对路径执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/bin/bash##redis主机IPhost=$1##redis端口port=$2##key模式pattern=$3db=$4##游标cursor=0##退出信号signal=0echo &quot;&quot;&gt;~/keys.txt##循环获取key并删除while [ $signal -ne 1 ] do ##将redis scan得到的结果赋值到变量 re=$(./redis-cli -h $host -p $port -c -n $db scan $cursor count 1000 match $pattern) ##以换行作为分隔符 IFS=$&#x27;\\n&#x27; ##转成数组 arr=($re) ##第一个元素是游标值 cursor=$&#123;arr[0]&#125; ##打印数组 len=$&#123;#arr[*]&#125; for ((i=1;i&lt;len;i++)) do a=$&#123;arr[i]&#125; echo $a &gt;&gt; ~/keys.txt done ##游标为0表示没有key了 if [ $cursor -eq 0 ];then signal=1 fidoneecho &#x27;done&#x27; 得到 keys.txt 文件后，使用 awk, uniq, sort 等命令进行归并排序，脚本如下： 12sort keys.txt &gt;&gt; keys-sort.txtcat keys-sort.txt | awk -F &#x27;:&#x27; &#x27;&#123;print $1&quot;:&quot;$2&quot;:&quot;$3&#125;&#x27; | uniq -c | sort -rn 分隔符和打印内容可以按实际情况调节输出，便于阅读。输出结果后，便可定位具体的Key进行优化了。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://blog.fenxiangz.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://blog.fenxiangz.com/tags/Redis/"},{"name":"优化","slug":"优化","permalink":"https://blog.fenxiangz.com/tags/%E4%BC%98%E5%8C%96/"}]},{"title":"英语语法笔记","slug":"English/2020_11_29_语法学习笔记","date":"2020-11-29T00:00:00.000Z","updated":"2021-01-07T07:36:03.946Z","comments":true,"path":"post/English/2020_11_29_语法学习笔记.html","link":"","permalink":"https://blog.fenxiangz.com/post/English/2020_11_29_%E8%AF%AD%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html","excerpt":"","text":"英语法治思维，注重结构美 汉语人治思维，注重意境美 词法 10种词类 名词（n.）： 形容词（adj.）：修饰名词 代词（adv.）：修饰动词、形容词 数词（）： 冠词（）：a / an / the 动词（）： 副词（）： 介词（）：介词后面跟名词； 连词（）： 感叹句（）： 句法 三种句型 简单句 主谓：It rains. 主谓宾：I love you. 主系表： ​ be动词：You are my sunshine. ​ 感官动词：fell, sound，…… 主谓宾宾：主谓+人+物 ​ She offers me a great hand. 她给我提供了我一个很大的帮助。 主谓宾+宾补： ​ She makes me sad 并列句 复合句：三种从句，定语从句、名词性从句 定语从句 概念：在复合句中，修饰名词或代词的从句叫做定语从句。性质和形容词很像，也称形容词性从句。 成分：先行词， 关系代词：that，which，who（宾格为 whom，所有格为 whose），或者关系副词 where，when，why等。 名词性从句","categories":[{"name":"其他/英语","slug":"其他-英语","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96-%E8%8B%B1%E8%AF%AD/"}],"tags":[{"name":"英语","slug":"英语","permalink":"https://blog.fenxiangz.com/tags/%E8%8B%B1%E8%AF%AD/"},{"name":"语法","slug":"语法","permalink":"https://blog.fenxiangz.com/tags/%E8%AF%AD%E6%B3%95/"}]},{"title":"猴子管理法则","slug":"其他/2020-11-28_猴子管理法则","date":"2020-11-28T00:00:00.000Z","updated":"2020-12-20T16:47:02.989Z","comments":true,"path":"post/其他/2020-11-28_猴子管理法则.html","link":"","permalink":"https://blog.fenxiangz.com/post/%E5%85%B6%E4%BB%96/2020-11-28_%E7%8C%B4%E5%AD%90%E7%AE%A1%E7%90%86%E6%B3%95%E5%88%99.html","excerpt":"","text":"很多管理者往往遇到这样的情况，当你听完下属员工的工作汇报后发现事情并没有得到彻底解决，而原本计划好今天要做的工作也因此耽误了不少时间。 猴子管理法则的目的在于帮助经理人确定由适当人选在适当的时间，用正确的方法做正确的事。 身为经理人要能够让员工去抚养自己的“猴子”，你也有足够的时间去做规划、协调、创新等重要工作。 责任是一只猴子这其中的关键在于，本来该下属员工自行完成的工作，因为逃避责任的缘故， 交由上司处理。 每个下属都有自己的猴子，如果都交由上司管理，显然，管理者自己的时间将变得十分不够用。 当你一旦接收部属所该看养的猴子，他们就会以为是你自己要这些猴子的， 因此，你收的愈多，他们给的就愈多。 于是你饱受堆积如山、永远处理不完的问题所困扰，甚至没有时间照顾自己的猴子，努力将一些不该摆在第一位的事情做得更有效率，平白让自己的成效打了折扣。 经理人应该将时间投资在最重要的管理层面上，而不是养一大堆别人的猴子。 当然，这个法则只能运用在有生存价值的猴子身上，不该存活的猴子，就狠心把他杀了吧！ “猴子”=问题你是问题处理高手吗？假如你的下属崇拜你，你或许会相当高兴。但那以后， 他几乎每件事都向你请示，你会觉得如何呢？你是否会感觉自己的时间不够用了，并因此开始检查自己的管理是不是出了什么问题呢？ 有一天，你的一位下属在办公室的走廊与你不期而遇，下属停下脚步问：“老板，有一个问题，我一直想向你请示该怎么办。” 此时，下属的身上有一只需要照顾的 “猴子”，接下来他如此这般将问题汇报了一番。尽管你有要事在身，但还是不太好意思让急切地想把事情办好的下属失望。 你非常认真地听着……慢慢地，“猴子”的一只脚已悄悄搭在你的肩膀上。你一直在认真倾听，并不时点头，几分钟后，你对他说这是一个非常不错的问题，很想先听听他的意见，并问：“你觉得该怎么办？” “老板，我就是因为想不出办法，才不得不向你求援的呀。” “不会吧，你一定能找到更好的方法。”你看了看手表，“这样吧，我现在正好有急事，明天下午四点后我有空，到时你拿几个解决方案来我们一起讨论。” 告别前，你没有忘记补充一句：“你不是刚刚受过‘头脑风暴’训练吗？实在想不出，找几个搭档来一次‘头脑风暴’，明天我等你们的答案。” “猴子”悄悄收回了搭在你身上的那只脚，继续留在此下属的肩膀上。 第二天，下属如约前来。从脸上表情看得出，他似乎胸有成竹：“老板，按照你的指点，我们已有了 5个觉得还可以的方案，只是不知道哪一个更好，现在就是请你拍板了。” 即使你一眼就已看出哪一个更好，也不要急着帮他作出决定。不然，他以后对你会有依赖，或者万一事情没办好，他一定会说：“老板，这不能怪我，我都是按照你的意见去办的。” 关于作决定，记住以下准则1、该下属做决定的事，一定要让他们自己学着做决定； 2、做决定意味着为自己的决定负责任。不想做决定，常常是潜意识里他不想承担责任； 3、下属不思考问题、不习惯做决定的根源一般有两个：其一是有“托付思想”，依赖上司或别人，这样的下属不堪大用；其二是上司习惯代替下属做决定或喜欢享受别人听命于自己的成就感，这样的上司以及他所带领的团队难以胜任复杂的任务； 4、让下属自己想办法，做决定，就是训练下属独立思考问题的能力和勇于承担责任的行事风格。 对话还在继续。你兴奋地说：“太棒了，这么多好方案。你认为，相比较而言哪一个方案更好？”“我觉得 A 方案更好一些。”“这的确是一个不错的方案，不过你有没有考虑过万一出现这种情况，该怎么办？”“噢，有道理，看来用 E 方案更好。”“这方案真的也很好，可是，你有没有想过……” “我明白，应该选择 B 方案。”“非常好，我的想法跟你一样，我看就按你的意见去办吧。” 凭你的经验，其实你早就知道应该选择 B 方案，你不直接告诉他的目的是想借此又多赢得一次训练部属的机会。 训练是一个虽慢反快的过程，训练的“慢”是为了将来更快。你这样做的好处不言而喻： 1、打断下属负面的“依赖”神经链。 2、训练了下属分析问题、全面思考问题的能力。 3、让下属产生信心与成就感。他会觉得自己居然也有解决复杂问题的能力。越来越有能力的下属能越来越胜任更重要的任务。 4、激发下属的行动力。 5、你将因此不必照看下属的“猴子”而腾出更多的精力去照看自己的“猴子”。 管理艺术的 5个严格规则规则一：“猴子”要么被喂养，要么被杀死。否则，他们会饿死，而经理则要将大量宝贵时间浪费在尸体解剖或试图使他们复活上。 规则二：“猴子”的数量必须被控制在经理有时间喂养的最大数额以下。下属会尽量找时间喂养猴子，但不应比这更多。饲养一只正常状况的猴子时间不应超过 5～15 分钟。 规则三：“猴子”只能在约定的时间喂养。 经理无须四处寻找饥饿的“猴子”，抓到一只喂一只。 规则四：“猴子”应面对面或通过电话进行喂养，而不要通过邮件。文档处理可能会增加喂养程序，但不能取代喂养。 规则五：应确定每只“猴子”下次的喂养时间。这可以在任何时间由双方修改并达成一致，但不要模糊不清。否则，“猴子”或者会饿死或者最终回到经理的背上。 “猴子管理”理论的启示1、每一个人都应该照看自己的“猴子”； 2、不要麻烦别人照看自己的“猴子”； 3、组织中，每一个人都应该明白自己应该照看哪些“猴子”以及如何照看好它们； 4、不要试图把自己的“猴子”托付给别人照顾。这里的别人可能是上司、下属、别的部门的同事，也可能是公司、社会乃至上天、命运等； 5、不要出现没有人照看的“猴子”，也不要出现有两个以上“主人”的“猴子”； 6、作为上司不仅应明确让下属知道他应该照看好哪些“猴子”，更需要训练下属如何照看好他们的“猴子”。 7、“猴子管理”并不适用于所有管理。比如说我们在创业初期时，就需要创业者或创业团队背负所有的“猴子”。并主动的帮助其他人背负“猴子”，否则创业不会成功。 8、“猴子管理”会使人变得自私，类似于各人自扫门前雪，会使团队失去活力。正常的团队应该是抢着“猴子”背负，而不是千方百计的防止别人的“猴子”窜上自己的肩膀。","categories":[{"name":"其他/管理","slug":"其他-管理","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96-%E7%AE%A1%E7%90%86/"}],"tags":[{"name":"管理","slug":"管理","permalink":"https://blog.fenxiangz.com/tags/%E7%AE%A1%E7%90%86/"}]},{"title":"『认知升级』是比其他一切都更加重要的思维模型转变","slug":"其他/2020-11-27_『认知升级』是比其他一切都更加重要的思维模型转变","date":"2020-11-27T00:00:00.000Z","updated":"2020-12-20T16:47:02.989Z","comments":true,"path":"post/其他/2020-11-27_『认知升级』是比其他一切都更加重要的思维模型转变.html","link":"","permalink":"https://blog.fenxiangz.com/post/%E5%85%B6%E4%BB%96/2020-11-27_%E3%80%8E%E8%AE%A4%E7%9F%A5%E5%8D%87%E7%BA%A7%E3%80%8F%E6%98%AF%E6%AF%94%E5%85%B6%E4%BB%96%E4%B8%80%E5%88%87%E9%83%BD%E6%9B%B4%E5%8A%A0%E9%87%8D%E8%A6%81%E7%9A%84%E6%80%9D%E7%BB%B4%E6%A8%A1%E5%9E%8B%E8%BD%AC%E5%8F%98.html","excerpt":"","text":"我们在过去的时光中，谈了很多关于技术、关于生活、关于工作、关于学习的话题，这次我们来谈谈关于『认知升级』这个话题。 实际上，『认知升级』相对来说是个比较抽象的概念，它与掌握一门技术相比不是那么容易看到。因为技术的进步和成长是任何人都可以看到成效的。比如说，你之前不会Netty或是对Netty不是那么了解，通过学习和实践掌握了Netty的方方面面，那这其中的进步就是非常明显的，也是很容易看到成效的。 与技术上的成长相比，『认知升级』则属于更加抽象，且更具方法论的一个话题了。那么，到底什么是认知呢？ 我个人认为，所谓认知指的是你对这个世界，对身边环境，对于生活、学习与工作的认识以及所采取的行事方法。 如果说切实掌握了一项技术属于『硬技能』的话，那么『认知升级』则属于软技能这个领域。不过，这里千万不要认为只有『硬技能』才是我们需要掌握的；相反，『软技能』的重要性有时还会超越『硬技能』，为什么这么说呢？ 根据我的经历，诸如『认知』这样的软技能是我们能够切实掌握『硬技能』的一个重要前提和方法论保证。具体来说，我们在学习任何一项新技术时，每个人所采取的方式与方法都是不尽相同的，你有你的方法，他有他的方法，而我显然也有我的方法。不过，无论中间采取了何种方法，我们的目标是不是都是一样，或是几乎是一样的呢？那我们的目标是什么呢？显然，我们的目标是扎实地掌握所学习的这项技术或是这个框架，并且对于一些重要技术与框架来说，掌握的越深入、越扎实越好。 既然大家的目标是一样的，即我们所要追求的最终结果几乎是无差别的，那么这中间的过程就会对结果起到很大的影响，有的影响是正向的，有的影响则是负向的。 从我们准备学习一门技术开始，一直到最终彻底掌握它，这中间会经历很多的过程，也会出现很多反复。因此，如何更好地确保中间过程的效率与效果就会对最终的结果产生极大的影响。 可以将掌握一门技术的过程分为如下几个步骤： 觉得这门技术挺有用，准备学习。 搜集学习资料，看官方文档、购买相关图书、看相关视频。 不停地遇到各种各样的问题，在网上不停地搜索解决方案。 继续看官方文档、继续看书、继续看视频。 依然会遇到各种问题，心情比较烦躁地搜索关于问题的解决方案。 初步掌握了这项技术。 一段时间没有使用或是没有再看这项技术，开始产生遗忘。 又经过一段时间，发现之前学习的这项技术很多都已经记不清楚了，甚至当时非常清晰的一些细节已经完全回忆不起来了。 重新开始学习这门技术。 历经千辛万苦，终于算是比较深入地掌握了这项技术。 又有一段时间没有再碰这项技术。 当有一天翻看这项技术时，发现又有太多、太多的细节已经完全想不起来了。 感到非常的沮丧。 感到更加的沮丧。 重复上述的步骤9。 是不是上面的这15个步骤感到似曾相识呢？ 原因在于什么？ 根本原因在于，你将太多的精力放在了非核心上面，而对真正的核心之处却从来没有深入思考过。 经常有人咨询我，为什么我学起一些技术会比较快，而且还比较深入，并且还能将自己的积累很系统地讲出来。但是换作自己，哪怕将一门技术扎实掌握都很难做到呢？其实，这个问题并非个案，而是一个普遍存在的问题。这个普遍存在的问题严重到会成为制约你更好前进的一个巨大障碍。 正所谓『不识庐山真面目，只缘身在此山中』。我们每个人都上了十多年学，但是很多人甚至连最为重要的学习方法都没有掌握。这里面一方面有学校教育的缺失，另一方面则是作为个体从来没有认真思考过这个问题。 对于我来说，在上大学时收获的最重要两个方面并非掌握了什么专业知识，而是我在大三时明白了下面两点： 我知道自己热爱的专业是什么：我不喜欢自己当时所在的专业，我更加喜欢计算机专业，因此确定了跨校跨专业考研的目标。 我掌握了适合自己的较为高效的自学方式：这一点在后来的时光中对我产生了巨大的帮助，让我能够走得更加从容不迫。 回到上面的话题，为什么我们在学习一项技术时总是容易遗忘，哪怕当时印象极其深刻的内容，以为自己永远也不会忘记的内容过一阵还是会遗忘呢？ 答案就是『无输出』。 是的，道理就是如此简单。 无论你的学习手段是什么，是看官方文档，看书，看文章，还是看视频，这些都是『输入』。是别人的东西灌输到你的脑海中，但它不是你的。 如何将别人的东西最终变成自己的呢？答案只有一个：输出。即，通过自己的不断输入，在脑海中经过一系列的加工，最终变成自己的输出。即下面这3个过程： 输入 加工 输出。 很多很多人在学习时，第1个步骤做得都还可以；第2步则因人而异了，有些人会思考，有些人则全盘接受，更可悲的是将网上看来的东西就当作真理一般对待。至于第3步，只有很少很少人才会做。因为，这个步骤是最耗费时间与精力的一个步骤。而且，第3步在你学习的当下你会认为是一个毫无存在必要的步骤，因为你当时自我感觉已经将待学习的这项技术理解的很透彻了。然而，成败就在一念之间。 对于没有输出的学习，其最终的效果就如同我上面所列出的15个步骤那般。 为什么总有人说，一项技术只有在项目中实际用过了才能真正掌握，其实这里面暗暗隐含着『输出』这个环节。在项目中实际用过显然就是一种输出方式。但在项目中使用过仅仅是『输出』的一种方式而已，它并非全部，请勿一叶障目，不见树林。 在项目中使用本质上就是一种『输出』方式，它会令你产生一种错觉：一项技术只有在项目中使用过了才能算真正掌握。 当下的技术领域如此之多，一个项目充其量只会使用其中很少的一些技术集合。按照上面的理论，难道项目中用不上的技术就不用学了么？答案不言自明。 其实，在项目中使用会令你加深对一项技术的理解与认识这个观点只不过是对于一种方法论的具体解读而已。 『输出』的形态其实有很多种： 在项目中使用 形成记录（记录到印象笔记或是有道云笔记上），发表到博客、微信公众号等媒体上 给别人讲 这里面我只列出了自己所钟爱的3种方式，其他方式也有很多。 因此，你觉得在项目中使用才算掌握一门技术，在我眼里看来，可谓是『认知』尚未升级，因为你并未透过现象看到本质。在项目中使用可以让我们比较好地学会到应用，但是对于技术的深层次掌握是需要额外下功夫的，这通常都是对自己有着较高要求的人才会做的事情。 给别人讲是一种我特别推崇的学习方式。通过这个过程，你会发现自己在技术理解上的诸多问题，同时会不断加深对技术细节的把控；可以这么说，将上面3种『输出』方式有机结合起来，会令你真正掌握学习的方法论，也会令你在学习之路上越走越好，越走越踏实。 可以举一个例子，目前圣思园正在发布『深入理解JVM』课程。实际上，除了本职工作就是与JVM打交道之外，绝大多数人的日常工作并非天天都会接触到JVM，那为何还要学习呢？因为它重要啊！ 既然无法做到天天与JVM打交道，那该如何学习JVM呢？显然，既然无法做到在项目中直接使用，那我们就完全可以用其余2种方法：形成技术+给别人讲。 参加圣思园课程学习的不少小伙伴已经在践行我上面所提出的观点，并且均取得了不错的效果，这里我也期望你能将自己的学习观点与认知方式分享出来，欢迎大家的评论。 原文：https://blog.csdn.net/ricohzhanglong/article/details/95947294","categories":[{"name":"其他/管理","slug":"其他-管理","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96-%E7%AE%A1%E7%90%86/"}],"tags":[{"name":"管理","slug":"管理","permalink":"https://blog.fenxiangz.com/tags/%E7%AE%A1%E7%90%86/"},{"name":"认知","slug":"认知","permalink":"https://blog.fenxiangz.com/tags/%E8%AE%A4%E7%9F%A5/"}]},{"title":"Jira 修改 Path 路径后的用户管理问题","slug":"其他/2020-11-26_Jira修改Path路径后的用户管理问题","date":"2020-11-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.988Z","comments":true,"path":"post/其他/2020-11-26_Jira修改Path路径后的用户管理问题.html","link":"","permalink":"https://blog.fenxiangz.com/post/%E5%85%B6%E4%BB%96/2020-11-26_Jira%E4%BF%AE%E6%94%B9Path%E8%B7%AF%E5%BE%84%E5%90%8E%E7%9A%84%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%E9%97%AE%E9%A2%98.html","excerpt":"","text":"Confluence 同步 Jira 的几个相关配置 1. 仅使用外部用户管理 2. Confluence 的 Links 配置 3. Jira 修改 Context 的 Path 路径后，Confluence账号无法登陆，用户管理问题。 如果本身是可编辑的就直接编辑，非可编辑状态，需要先下移改成非首选，然后进入编辑页面更新外部用户目录的实际路径。","categories":[{"name":"其他/Jira","slug":"其他-Jira","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96-Jira/"}],"tags":[{"name":"Jira","slug":"Jira","permalink":"https://blog.fenxiangz.com/tags/Jira/"}]},{"title":"开发资源导航","slug":"util/2020-11-26_开发资源整理","date":"2020-11-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.984Z","comments":true,"path":"post/util/2020-11-26_开发资源整理.html","link":"","permalink":"https://blog.fenxiangz.com/post/util/2020-11-26_%E5%BC%80%E5%8F%91%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86.html","excerpt":"","text":"Javascript jQuery Fundamentals - jQuery 入门教程。 JavaScript库 代码解构 - 将JavaScript流行框架源代码条分缕析展现出来 深入理解Javascript系列 Script的defer和async的区别 Javascript面向对象基础 Backbone.js基础 JavaScript Madness: Keyboard Events Let’s Make Frameworks 国内公司JS框架：Kissy - Taobao | Arale - Alipay | Tangram - Baidu JS1K, 1k Javascript contest JSON Home Page NB JS Wiki(CSS、PHP、jQuery、Linux) 理解并解决IE的内存泄漏方式 2 3 4 Understanding and Solving Internet Explorer Leak Patterns Javscript Bind函数 Javscript设计模式 JSON工具： JSONLint - The JSON Validator SimpleJSON - Python Stuff JSON Formatter (&amp; Validator!) Tidy JSON - JSON Pretty Printer/Colorer - C#(.NET) Cerny.js - JSON Pretty Printing Demo jsonpretty(ruby) Vim Json： tidify a json in Vim JSON.vim - syntax VIM - How to format and syntax highlight JSON file Add JSON syntax highlighting in Vim on OS X JSONP： 使用 JSONP 实现跨域通信，第 1 部分: 结合 JSONP 和 jQuery 快速构建强大的 mashup 第 2 部分: 使用 JSONP、jQuery 和 Yahoo! 查询语言构建 mashup JSONP的起源 Mashups：Web 应用程序新成员 Javascript闭包： 闭包 (计算机科学) Javascript Closures 什么是闭包 Javascript Closures 中文 2 学习Javascript闭包（Closure） 作用域链 词法作用域 与 闭包（一） （二） Javascript工具： JSLint JavaScript Lint Fork of hallettj/jslint.vim Google Closure Compiler压缩优化规则初探 使用Google 的Closure Compiler来压缩javascript 在项目中使用Google Closure Compiler Mac下用Closure compiler JS 库浅析之 Google Closure Closure Compiler 高级模式及更多思考 Closure Compiler vs YUICompressor Minify JS QUnit @github JsBeautify：Online Javascript jsbeautifier github , vimscript js beautifier - plugin for Chrome NodeJS： nodeJS - 服务器端 JavaScript 编程 Node.js is genuinely exciting 在cygwin环境下编译node.js node char - 用 nodeJS 写的聊天室 Joyent Node | mattn.no.de HTML &amp; CSS CSS Hacks &amp; Expression XHTML Character Entity Reference HTML实体字符引用 精选15个国外CSS框架 Pure Css Speech Bubbles CSS栅格系统(Grid System)： The 1Kb CSS Grid - 拖放各个阈值并直接下载自动生成的CSS。 Variable Grid System - 可直接修改各个阈值并预览效果。 Grid Designer YAML Builder - A tool for visual development of YAML based CSS layouts. 960 Grid System 960-Grid-System@github 网页的栅格系统设计 - 青云 我的栅格系统 - 明城 Five simple steps to designing grid systems 2 3 4 5 CSS疑难杂症： HasLayout和BFC(Block Formatting Contexts)的区别完整对比 Block Formatting Contexts的特性 hasLayout.net On having layout 中文版 Internet Explorer 6 and the Expanding Box Problem 万能清除浮动样式 CSS Border使用小分享 表格樣式集錦 复选框单选框与文字对齐问题的研究与解决 连续字符自动换行的解决方案 三谈Iframe自适应高度 | 再谈iframe自适应高度 跨浏览器的inline-block en display:inline-block的深入理解 HTML5 &amp; CSS3 Dive Into HTML5 HTML5训练营 HTML5 WebSockets IE支持HTML5 感受HTML5&amp;CSS3 用JavaScript玩转计算机图形学：(一)光线追踪入门 (二)基本光源 Canvas： Canvas (HTML元素) Canvas Tutorial Canvas 教程 en The canvas element HTML5 Canvas Canvas使用教程： 开题 基本语法 图形绘制 图片应用 Canvas Games： Unreal Soccer Canvascape Plasma demo using the HTML Canvas element Lodes Computer Graphics Tutorial CubeOut - 3D 俄罗斯方块 自行车越野 Box2DJS Canvas Cycles Agent 8 Ball - 台球 工具： CSS3 Generator CSS3 Gradient Generator CSS3 Button Maker CSS3 Please! - The Cross-Browser CSS3 Rule Generator Framework： mxGraph - the AJAX diagramming soluting Scalable Vector Graphics Web Browser using Flash HTML5 Canvas for Internet Explorer Library that provides support for SVG and VML with an SVG style interface DHTML: Draw Line, Ellipse, Oval, Circle, Polyline, Plygon, Triangle, with JavaScript 翻译Browser Drawing一篇:Canvas/SVG/VML Drawing Roundup Three.js Rapha?l - 非常棒的跨平台 JavaScript 图形库 | raphael@github | blog uupaa.js spin-off projects Demo： Alex Buga Livingroom burn-canvas-test - 画图 10 HTML5 Demos to Make You Forget About Flash cn deviantART Muro Biolab Disaster - Game 乒乓球游戏：左边用Flash，右边用HTML5 20 Things I Learned About Browsers and the Web Pure CSS Twitter ‘Fail Whale’ CSS3-Man A啦多梦告诉你浏览器对 CSS3 的支持程度 前端相关 World Wide Web Consortium The Web Standards Project W3Schools Online Web Tutorials HTTP 状态代码 网页错误代码详解 w3school - 在线教程 REST介绍 浏览器兼容解决方案 (AliceUI) | W3C 标准文档 (AliceUI) Microformats：Microformats | 什么是微格式及经典实例演示 | 微格式 - Wikipedia en | Microformats Cheat Sheat | 微格式全功略Hcard、 hCalendar、hReview、XFN 轻松掌握 | 微格式 Microformats ? hCard | 使用微格式来丰富网站语义：简介 | Getting Semantic With Microformats, Introduction Python 简明Python教程 Dive Into Python中文版 Dive Into Python3中文版 啄木鸟社区Wiki Python绝对简明手册 正则表达式 正则表达式30分钟入门教程 2 正则表达式工作室 | 揭开正则表达式的神秘面纱 | 正则表达式话题 | DEELX 正则引擎性能与特点 各种工具之正则表达式语法比较 2 Regular_Expression 入门 Lesson: Regular Expressions(Java) Regular expression operations(Python) 正则表达式 - 维基百科 RegExp - Mozilla 学习 Linux，101: 使用正则表达式搜索文本文件 构建用于正则表达式的抽象 Java API Regexp Syntax Summary JavaScript, Regex, and Unicode 《理解正则表达式（程序员第3期文章）》 利用有限自动机分析正则表达式 《Linux系统最佳实践工具：命令行技术》 我爱正则表达式 正则表达式论坛 正则表达式工具：Regexpal (online) 2 | RegExr | RegexBuddy | Expresso (free, open-source) | Match Tracer | HiFi Regex Tester | RegEx Builder 开发相关 MarkDown语法 HTML转义 OAuth OAuth OAuth核心 OAtuh for Web Application Vim-oauth webapi-vim&amp; OAuth在线测试：服务端 | 客户端 国内开源镜像站：Sohu.com | 163.com 在线IDE：CodeRun | jsFiddle | JS Bin | 小可 优良的文本处理工具：SED &amp; AWK sed.sf.net | AWK @wikipedia 中文 Gawk for Windows | Sed for Windows sed-非交互式文本编辑器(L.E.McMahon 著,中文翻译) En | awk-模式扫描与处理语言(Aho,Kernighan,Weinberger著,中文翻译)(第二版) En 详解注明的AWK oneliner：一：空行、行号和计算 | 二：文本替换 | 三：选择性输出特定行 | 四：定义字符串和数组 详解AWK oneliner原文：Famous Awk One-Liners Explained | Update on Famous Awk One-Liners Explained: String and Array Creation sed和awk的简单使用 - 潘魏增 参考书籍：The AWK Programming Language | Effective awk Programming, Third Edition | sed &amp; awk, Second Edition | sed and awk Pocket Reference, Second Edition 函数式编程： @wikipedia 中文 对象-函数式编程简史 2 用函数式编程技术编写优美的 JavaScript 函数式编程 - 老赵 函数式编程 - czk wiki 函数式编程の道 哪种语言将统治多核时代 再看函数式语言特性 The Tao Of Programming ,2 ,《编程之道》 文言文版 by Livecn，2 白话文版 by 小赵 代码高亮：DlHightLight代码高亮组件 | Google Code Prettify 版本控制 版本控制系统（RCS）的选择与比较 拥抱Mercurial—选择分布式版本控制工具 几个分布式vcs比较 Comparison of revision control software 代码片段管理： gist@github notepad.cc Snipt.org Ubuntu Paste Pastebin Lodge It! SVN相关： Subversion 与版本控制 TortoiseSVN 中文帮助手册(v1.4.1) v1.6.8 Tigris.org - for Windows. Subversive中文站 Subclipse Subversive RabbitVCS - for Linux. Ubuntu下最好用的SVN客户端 SVN 命令行 Mercurial相关： Mercurial | Mercurial 使用教程 Hg Init: a Mercurial tutorial Mercurial: The Definitive Guide read 在Google Code上用 Mercurial 取代 Subversion 管理你的项目 乔尔谈软件终结篇：分布式版本控制系统的时代到来了 - 讲到了分布式版本控制的精髓：管理变更，而不是管理版本。 A Git User’s Guide to Mercurial Queues Mercurial托管服务：Mercurial hosting - bitbucket.org | KilnHg Git相关： Git - the Fast Version Control System. Stanford出品的Git Magic教程 最详细Git介绍：Pro Git (book , 中文版 ) Git官方帮助文档 简明教程：git 之五分钟教程 | 进一步学习 Git | 使用 Git 管理源代码 | 分布式版本控制工具Git简明笔记 | 译文:GIT日常命令20来条 Git开发管理之道 Git讨论：Why Git is Better than X | Git 改变了分布式 Web 开发规则 | 为什么说 Git 将取代 SVN 做软件版本控制？ SVN+GIT=鱼与熊掌兼得 面向 Subversion 用户的 Git：一: 入门指南 | 面向 Subversion 用户的 Git，第 2 部分: 实施控制 Git的推广心得 | 你为神马不用git-flow呢? | 开始实践git-flow github | jekyll | codes | demos | GitHub Pages | Publishing a Blog with GitHub Pages and Jekyll 系统相关 用win+r启动程序和文档 Linux彻底定制指南(Linux From Scratch) 服务器和架构方面的一些文章 Linux Shell常用技巧(目录) Linux Shell高级技巧(目录) Ubuntu新手入门指引 | Ubuntu桌面入门指南 终端(Terminal)：zsh | zsh.sf.net | 终极Shell——Zsh | 把 Mac 上的 bash 换成 zsh | zsh – 给你的Mac不同体验的Terminal！ | oh-my-zsh@github 网络监控：Fiddler 2 | HttpWatch | Charles 远程控制： SSH技巧详解：SSH: More than secure shell SSH：SSH Filesystem | 在 Ubuntu 上使用 sshfs 映射远程 ssh 文件系统为本地磁盘 | MacFUSE | sshfs for Mac OS X SecureCRT：SecureCRT | SecureCRT 常用命令 PuTTY：PuTTY | PuTTY 中文版 | PuTTY: A Free Telnet/SSH Client | PuTTY 中文教程 | @google docs | 转 | @wikipedia cURL：cURL and libcurl | docs | PHP: cURL - Manual | libcurl的使用总结（一） | libcurl的使用总结（二） rsync：rsync | zh@wikipedia | en@wikipedia | A Tutorial on Using | cwrsync - Rsync for Windows | 如何用 Rsync 来备份 Linux 文件 | AIX 上配置 rsync 简记 | 用 Rsync(cwRsync)备份 Dreamhost 到 Windows 上 | Rsync 与 OpenSSH 结合运用进行文件同步 Email相关： HTML Email Boilerplate - Email模板 如何發送內嵌圖片的 E-mail ( Inline Attachment ) 发送内嵌图片邮件的正确方法 使用 Commons-Email 在邮件内容中直接嵌入图片 内嵌图片或附档 HTML email inline styler CSS to inline styles converter 设计相关图片、图标(Icons)： 2011年50个最佳图标设计集合 FindIcons图标搜索引擎 Icon Archive PNG icons &amp; Icons Picks Download DryIcons - Free Icons and Vector Graphics Icon Easy Tutorial9 | Photoshop Tutorials, Photography Tuts, and Resources 16 Sketchy Hand Drawn Icon Sets Gnome Icon Theme dutch icon? 开源图标库 PhotoShop：PhotoShop通道白解 | PhotoShop CS5序列号 字体： 什么是衬线体 文泉驿 - 开源字体。开彼源兮，斯流永继。 让代码更美:10大编程字体 Type is Beautiful - 字体排版 Google Font Directory ASCII Generator 假文填充：Lorem ipsum | 中文假文MoreText.js MoreText的Vim插件 | 亂數假文產生器 Chinese Lorem Ipsum 表格： 15个优秀的表格设计技巧 Creating a table with dynamically highlighted columns like Crazy Eggs pricing table 越减越妙：简单表格的再设计 15款提高表格操作的jQuery插件 资源： Dribble：著名设计师聚合网站 站酷：交流设计、分享快乐 全景：创意图片库 Designlol：全球设计精享 Designboom 配色方案：Color Schemer | kuler | Piknik Color Picker 80多个绝对漂亮的双屏壁纸 VIM 配置挑选Vim颜色(Color Scheme) VIM折叠简介 FuzzyFinder快速查找文件 转自：https://www.luxiaolei.com/wiki","categories":[{"name":"资源导航","slug":"资源导航","permalink":"https://blog.fenxiangz.com/categories/%E8%B5%84%E6%BA%90%E5%AF%BC%E8%88%AA/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.fenxiangz.com/tags/%E5%B7%A5%E5%85%B7/"},{"name":"Wiki","slug":"Wiki","permalink":"https://blog.fenxiangz.com/tags/Wiki/"},{"name":"整理","slug":"整理","permalink":"https://blog.fenxiangz.com/tags/%E6%95%B4%E7%90%86/"}]},{"title":"V2ray 配置","slug":"其他/2020-10-25_V2ray配置","date":"2020-10-25T00:00:00.000Z","updated":"2020-12-20T16:47:02.988Z","comments":true,"path":"post/其他/2020-10-25_V2ray配置.html","link":"","permalink":"https://blog.fenxiangz.com/post/%E5%85%B6%E4%BB%96/2020-10-25_V2ray%E9%85%8D%E7%BD%AE.html","excerpt":"","text":"Mac下载软件： https://github.com/yanue/V2rayU/releases/download/2.1.0/V2rayU.dmg 配置教程：https://github.com/yanue/V2rayU/wiki/V2rayU%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E Window下载软件：https://github.com/2dust/v2rayN/releases/download/3.13/v2rayN-Core.zip Android下载软件：https://github.com/2dust/v2rayNG/releases/download/1.2.4/v2rayNG_1.2.4.apk iOSAppStore： Shadowrocket教程：https://www.hijk.pw/shadowrocket-config-v2ray-tutorial/","categories":[{"name":"其他/V2ray","slug":"其他-V2ray","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96-V2ray/"}],"tags":[{"name":"V2ray","slug":"V2ray","permalink":"https://blog.fenxiangz.com/tags/V2ray/"}]},{"title":"MySQL什么时候会使用内部临时表?","slug":"dateabase/mysql/2020-09-13_MySQL什么时候会使用内部临时表","date":"2020-09-13T00:00:00.000Z","updated":"2020-12-20T16:47:02.948Z","comments":true,"path":"post/dateabase/mysql/2020-09-13_MySQL什么时候会使用内部临时表.html","link":"","permalink":"https://blog.fenxiangz.com/post/dateabase/mysql/2020-09-13_MySQL%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E4%BC%9A%E4%BD%BF%E7%94%A8%E5%86%85%E9%83%A8%E4%B8%B4%E6%97%B6%E8%A1%A8.html","excerpt":"","text":"1.union执行过程 首先我们创建一个表t1 create table t1(id int primary key, a int, b int, index(a)); delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&lt;=1000)do insert into t1 values(i, i, i); set i=i+1; end while; end;; delimiter ; call idata(); 然后我们执行一下这条语句 explain select 1000 as f union (select id from t1 order by id desc limit 2) 首先说下union的语义，union的语义是取两个结果的并集，重复的保留一行,然后我们来看下explain的结果，第二行的key=PRIMARY，说明用到了主键索引。 第三行的Extra的Using temporary说明用到了临时表 下面我们看下这条语句的执行流程： 1.创建一个临时表，只有f一个字段，且为主键 2.将1000这个数据插入临时表 3.子查询中步骤: 1.插入1000进入临时表,因为主键冲突，插入失败 2.插入第二行900，插入成功 4.将临时表数据作为结果返回，并删除临时表 这个过程的流程图如下： 如果我们把union改成union all，就不需要使用临时表了，因为union all是重复的也保留， 大家可以看到extra这一列已经没有了Using temporary explain select 1000 as f union all (select id from t1 order by id desc limit 2) 2.group by执行过程 我们来看下面这条语句: explain select id%10 as m, count(*) as c from t1 group by m; 可以看到explain结果 Using index(使用到了覆盖索引a，不需要回表); Using temporary(用到了临时表); Using filesort(对数据进行了排序) 这条语句的意思是将id%10进行分组统计，并按照m进行排序 执行流程如下: 1.创建临时表，增加m,c字段，m是主键 2.计算id%10的结果记为x 3.如果临时表里面没有主键为x的行，则插入(x,1)，如果有的话，就将该行的c值加1 4.遍历完成后，按照m字段排序返回结果给客户端 流程图如下 接下来我们看下这条语句的执行结果 explain select id%10 as m, count(*) as c from t1 group by m 其实，如果我们不需要对查询结果进行排序，我们可以加一个order by null 我们执行一下这条语句 explain select id%10 as m, count(*) as c from t1 group by m order by null 可以看到这里没有进行排序，由于扫描是从表t的id是从1开始的，所以第一行是1 如果我们执行下列语句，会发生什么呢？ 我们上面说的临时表，其实是内存临时表，如果我们把内存临时表的容量改的比我们要查询的数据的容量小，那么就会使用到磁盘临时表，磁盘临时表的默认引擎是innodb set tmp_table_size=1024; select id%100 as m, count(*) as c from t1 group by m order by null limit 10 group by 优化方法–直接排序 其实在上面的关于从内存临时表转化成磁盘临时表是很浪费时间的，也就是说mysql，在执行过程中发现空间不够了，在转成磁盘临时表，但是如果我们直接告诉mysql，我要查询的数据很大，那么mysql优化器就会想到，既然你告诉我数据很大，那么我就直接用sort_buffer进行排序，如果sort_buffer内存不够大，会用到磁盘临时表辅助排序。 select SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m; 小结一下: 1.如果我们不需要对统计结果进行排序，可以加上order by null省去排序流程。 2.尽量让排序过程用上内存临时表，可以通过适当调大tmp_table_size的值来避免用到磁盘临时表。 3.如果数据量实在太大，使用SQL_BIG_RESULT告诉优化器，直接使用排序算法。 原文：https://zhuanlan.zhihu.com/p/66847189","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/tags/MySQL/"},{"name":"优化","slug":"优化","permalink":"https://blog.fenxiangz.com/tags/%E4%BC%98%E5%8C%96/"},{"name":"临时表","slug":"临时表","permalink":"https://blog.fenxiangz.com/tags/%E4%B8%B4%E6%97%B6%E8%A1%A8/"}]},{"title":"MySQL 中的 Character Set 与 Collation","slug":"dateabase/mysql/2020-09-12_MySQL中的Character_Set与Collation","date":"2020-09-12T00:00:00.000Z","updated":"2020-12-20T16:47:02.948Z","comments":true,"path":"post/dateabase/mysql/2020-09-12_MySQL中的Character_Set与Collation.html","link":"","permalink":"https://blog.fenxiangz.com/post/dateabase/mysql/2020-09-12_MySQL%E4%B8%AD%E7%9A%84Character_Set%E4%B8%8ECollation.html","excerpt":"","text":"背景MySQL 应该算是目前最流行的数据库之一，经常建库建表的同学应该对 Character Set 和 Collation 这两个词不陌生。 虽然一直有接触，但我还是挺云里雾里的。直到前些天特地做了功课，才敢说有个比较清晰的了解，所以就有了这篇文章。 Character Set 与 Collation简单地说，Character Set 是字符集，而 Collation 是比对方法，是两个不同维度的概念。 我们经常看到的 utf8、gbk、ascii 都是相互独立的字符集，即对 Unicode 的一套编码。看到一个比较有趣的解释，摘抄过来。 打个比方，你眼前有一个苹果，苹果在英文里称之为「Apple」，在中文里称之为「苹果」。苹果这个实体的概念就是 Unicode，而 utf8 之类的可以认为是不同语言对苹果的不同称谓，本质上都是描述苹果这个实体。 每套字符集有一系列与之对应的比对方法，比如 utf8 字符集对应 utf8_general_ci、utf8_unicode_ci 等比对方法，不同的比对方法下得到的搜索结果、排序结果不尽相同。 utf8 与 utf8mb4抛开数据库，标准的 UTF-8 字符集编码是可以用 1 ~ 4 个字节去编码 21 位字符，这几乎包含了世界上所有能看见的语言。 然而 MySQL 中实现的 utf8 最长只使用了 3 个字节，也就是只支持到了 Unicode 中的 基本多文本平面。任何不在基本多文本平面的 Unicode 字符，都无法使用 MySQL 的 utf8 字符集存储。包括 Emoji 表情、一些不常用的汉字，以及任何新增的 Unicode 字符等等。 为了解决这个问题，MySQL 在 5.5.3 之后增加了 utf8mb4 字符编码，mb4 即 most bytes 4。简单说 utf8mb4 是 utf8 的超集并完全兼容 utf8，能够用四个字节存储更多的字符。官方手册 中也有提到 utf8mb4 的解释，我摘抄部分过来。 The utfmb4 character set has these characteristics: Supports BMP and supplementary characters. Requires a maximum of four bytes per multibyte character. utf8mb4 contrasts with the utf8mb3 character set, which supports only BMP characters and uses a maximum of three bytes per character: For a BMP character, utf8mb4 and utf8mb3 have identical storage characteristics: same code values, same encoding, same length. For a supplementary character, utf8mb4 requires four bytes to store it, whereas utf8mb3 cannot store the character at all. When converting utf8mb3 columns to utf8mb4, you need not worry about converting supplementary characters because there will be none. utf8mb4_general_ci 与 utf8mb4_unicode_ciutf8mb4 对应的比对方法中，常用的有 utf8mb4_general_ci 和 utf8mb4_unicode_ci。关于这两个的区别，可以看下 StackOverflow 上有一个相关的热门讨论：What’s the difference between utf8_general_ci and utf8_unicode_ci，这边引用一下 Sean’s Notes 的翻译： 主要从排序准确性和性能两方面看： 准确性 utf8mb4_unicode_ci 基于标准的 Unicode 来排序和比较，能够在各种语言之间精确排序。 utf8mb4_general_ci 没有实现 Unicode 排序规则，在遇到某些特殊语言或字符时，排序结果可能不是所期望的。 但是在绝大多数情况下，这种特殊字符的顺序可能不需要那么精确。比如 *_unicode_ci 把 ß、Œ 当成 ss 和 OE 来看，而 *_general_ci 会把它们当成 s、e，再如 ÀÁÅåāă 各自都与 A 相等。 性能 utf8mb4_general_ci 在比较和排序的时候更快。 utf8mb4_unicode_ci 在特殊情况下，Unicode 排序规则为了能够处理特殊字符的情况，实现了略微复杂的排序算法。 但是在绝大多数情况下，不会发生此类复杂比较。*_general_ci 理论上比 *_unicode_ci 可能快些，但相比现在的 CPU 来说，它远远不足以成为考虑性能的因素，索引涉及、SQL 设计才是。 我个人推荐是 utf8mb4_unicode_ci，将来 8.0 里也极有可能使用变为默认的规则。相比选择哪一种 collation，使用者应该更关心字符集与排序规则在数据库里的统一性。 小结出于兼容性的考虑，对存储空间和性能没有特殊要求的场合下，建议使用 utf8mb4 字符集和 utf8mb4_unicode_ci 对比方法。 原文：https://zhuanlan.zhihu.com/p/64570524","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/tags/MySQL/"},{"name":"字符集","slug":"字符集","permalink":"https://blog.fenxiangz.com/tags/%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"name":"Collation","slug":"Collation","permalink":"https://blog.fenxiangz.com/tags/Collation/"}]},{"title":"高性能 MySQL 阅读笔记 (2)","slug":"dateabase/mysql/2020-09-11_高性能MySQL阅读笔记(2)","date":"2020-09-11T00:00:00.000Z","updated":"2020-12-20T16:47:02.947Z","comments":true,"path":"post/dateabase/mysql/2020-09-11_高性能MySQL阅读笔记(2).html","link":"","permalink":"https://blog.fenxiangz.com/post/dateabase/mysql/2020-09-11_%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0(2).html","excerpt":"","text":"Scheme与数据类型优化选择合适的类型要点 更小的通常更好：尽可能把列的大小指定在合理的范围； 简单，例如：能用整型就用整型；避免字符串表示日期，应该用内建的日期和时间类型；ip应转成long存储等； 尽量避免默认值为 NULL，尤其是当需要在该列上创建索引的时候； 例外情况：如果是稀疏数据，比如很多行都为NULL，只有少部分数据为NULL，使用NULL可以很好的提高空间利用率； 整数类型 有符号和无符号类型使用相同的存储空间，并具有相同的性能； 整数计算一般使用64位 BIGINT整数（一些聚合函数除外，它们使用 DECIMAL 或 DOUBLE 进行计算）； 整型可以指定宽度，例如：INT(11) ，对大多数应用没有意义：它不会影响合法范围，只是给一些交互功工具（例如：MySQL命令行终端）用来提示显示字符宽度。不影响存储和计算； 实数类型 高精度计算：DECIMAL ，非CPU直接计算，通过MySQL服务器自身实现高精度计算，性能不如：FLOAT、DOUBLE； 大整数：BIGINT，可以把小数乘以小数位数（按精度需要，提高相应的倍数）来存储，可以提高计算性能； 浮点相对精度较低：FLOAT、DOUBLE，CPU直接支持浮点计算，精度不一定很高，但运算更快； 字符串类型VARCHAR ： 变长，省空间，更新时需要做更多额外的工作，容易碎片化； CHAR：定长，占用固定的空间，不容易产生碎片； 存储和存储引擎具体的实现有关； VARCHAR 需要额外1或2个字节记录字符串长度； 虽然VARCHAR 是变长存储，但也要控制好长度，例如：VARCHAR(5)和VARCHAR(200)对于存储 hello 来讲，空间开销是一样的，但在内存中保存时，通常会分配固定的大小来保持。在使用“内存临时表”进行排序或操作时会影响存储性能，在使用“磁盘临时表”进行排序时也一样，所以应该正确的分配空间； BLOB 和 TEXT不同点BLOB： 二进制存储，没有排序规则或字符集TEXT：有字符集和排序规则 （参考：MySQL 中的 Character Set 与 Collation） 查询和排序问题 涉及变量：max_sort_length BLOB 和 TEXT 列的查询会使用磁盘临时表，严重影响性能，应该避免查询列（排序列）使用 BLOB 和 TEXT ；如果无法避免时，通过使用 SUBSTRING(column, length) 将列值转换为字符串（ORDER BY 也同样适用），这样可以适用内存临时表进行计算，但要确保子串足够短，避免超过 max_heap_table_size 或 tmp_table_size ，超过以后又会使用磁盘临时表。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/tags/MySQL/"},{"name":"性能优化","slug":"性能优化","permalink":"https://blog.fenxiangz.com/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"阅读笔记","slug":"阅读笔记","permalink":"https://blog.fenxiangz.com/tags/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}]},{"title":"高性能 MySQL - MySQL慢查询日志总结","slug":"dateabase/mysql/2020-09-10_高性能MySQL_MySQL慢查询日志总结","date":"2020-09-10T00:00:00.000Z","updated":"2020-12-20T16:47:02.947Z","comments":true,"path":"post/dateabase/mysql/2020-09-10_高性能MySQL_MySQL慢查询日志总结.html","link":"","permalink":"https://blog.fenxiangz.com/post/dateabase/mysql/2020-09-10_%E9%AB%98%E6%80%A7%E8%83%BDMySQL_MySQL%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97%E6%80%BB%E7%BB%93.html","excerpt":"","text":"原文：https://www.cnblogs.com/kerrycode/p/5593204.html 慢查询日志概念 MySQL的慢查询日志是MySQL提供的一种日志记录，它用来记录在MySQL中响应时间超过阀值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。long_query_time的默认值为10，意思是运行10S以上的语句。默认情况下，Mysql数据库并不启动慢查询日志，需要我们手动来设置这个参数，当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。慢查询日志支持将日志记录写入文件，也支持将日志记录写入数据库表。 官方文档，关于慢查询的日志介绍如下（部分资料，具体参考官方相关链接）： The slow query log consists of SQL statements that took more than long_query_time seconds to execute and required at least min_examined_row_limit rows to be examined. The minimum and default values of long_query_time are 0 and 10, respectively. The value can be specified to a resolution of microseconds. For logging to a file, times are written including the microseconds part. For logging to tables, only integer times are written; the microseconds part is ignored. By default, administrative statements are not logged, nor are queries that do not use indexes for lookups. This behavior can be changed usinglog_slow_admin_statements and log_queries_not_using_indexes, as described later. 慢查询日志相关参数MySQL 慢查询的相关参数解释： slow_query_log ：是否开启慢查询日志，1表示开启，0表示关闭。 log-slow-queries ：旧版（5.6以下版本）MySQL数据库慢查询日志存储路径。可以不设置该参数，系统则会默认给一个缺省的文件host_name-slow.log slow-query-log-file：新版（5.6及以上版本）MySQL数据库慢查询日志存储路径。可以不设置该参数，系统则会默认给一个缺省的文件host_name-slow.log long_query_time ：慢查询阈值，当查询时间多于设定的阈值时，记录日志。 log_queries_not_using_indexes：未使用索引的查询也被记录到慢查询日志中（可选项）。 log_output：日志存储方式。log_output=’FILE’表示将日志存入文件，默认值是’FILE’。log_output=’TABLE’表示将日志存入数据库，这样日志信息就会被写入到mysql.slow_log表中。MySQL数据库支持同时两种日志存储方式，配置的时候以逗号隔开即可，如：log_output=’FILE,TABLE’。日志记录到系统的专用日志表中，要比记录到文件耗费更多的系统资源，因此对于需要启用慢查询日志，又需要能够获得更高的系统性能，那么建议优先记录到文件。 慢查询日志配置默认情况下slow_query_log的值为OFF，表示慢查询日志是禁用的，可以通过设置slow_query_log的值来开启，如下所示： mysql&gt; show variables like &#39;%slow_query_log%&#39;; +---------------------+-----------------------------------------------+ | Variable_name | Value | +---------------------+-----------------------------------------------+ | slow_query_log | OFF | | slow_query_log_file | /home/WDPM/MysqlData/mysql/DB-Server-slow.log | +---------------------+-----------------------------------------------+ 2 rows in set (0.00 sec) mysql&gt; set global slow_query_log=1; Query OK, 0 rows affected (0.09 sec) mysql&gt; show variables like &#39;%slow_query_log%&#39;; +---------------------+-----------------------------------------------+ | Variable_name | Value | +---------------------+-----------------------------------------------+ | slow_query_log | ON | | slow_query_log_file | /home/WDPM/MysqlData/mysql/DB-Server-slow.log | +---------------------+-----------------------------------------------+ 2 rows in set (0.00 sec) 使用set global slow_query_log=1开启了慢查询日志只对当前数据库生效，如果MySQL重启后则会失效。如果要永久生效，就必须修改配置文件my.cnf（其它系统变量也是如此）。例如如下所示： mysql&gt; show variables like &#39;slow_query%&#39;; +---------------------+-----------------------------------------------+ | Variable_name | Value | +---------------------+-----------------------------------------------+ | slow_query_log | OFF | | slow_query_log_file | /home/WDPM/MysqlData/mysql/DB-Server-slow.log | +---------------------+-----------------------------------------------+ 2 rows in set (0.01 sec) 修改my.cnf文件，增加或修改参数slow_query_log 和slow_query_log_file后，然后重启MySQL服务器，如下所示 slow_query_log =1 slow_query_log_file=/tmp/mysql_slow.log mysql&gt; show variables like &#39;slow_query%&#39;; +---------------------+---------------------+ | Variable_name | Value | +---------------------+---------------------+ | slow_query_log | ON | | slow_query_log_file | /tmp/mysql_slow.log | +---------------------+---------------------+ 2 rows in set (0.00 sec) 关于慢查询的参数slow_query_log_file ，它指定慢查询日志文件的存放路径，系统默认会给一个缺省的文件host_name-slow.log（如果没有指定参数slow_query_log_file的话） mysql&gt; show variables like &#39;slow_query_log_file&#39;; +---------------------+-----------------------------------------------+ | Variable_name | Value | +---------------------+-----------------------------------------------+ | slow_query_log_file | /home/WDPM/MysqlData/mysql/DB-Server-slow.log | +---------------------+-----------------------------------------------+ 1 row in set (0.00 sec) 那么开启了慢查询日志后，什么样的SQL才会记录到慢查询日志里面呢？ 这个是由参数long_query_time控制，默认情况下long_query_time的值为10秒，可以使用命令修改，也可以在my.cnf参数里面修改。关于运行时间正好等于long_query_time的情况，并不会被记录下来。也就是说，在mysql源码里是判断大于long_query_time，而非大于等于。从MySQL 5.1开始，long_query_time开始以微秒记录SQL语句运行时间，之前仅用秒为单位记录。如果记录到表里面，只会记录整数部分，不会记录微秒部分。 mysql&gt; show variables like &#39;long_query_time%&#39;; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.000000 | +-----------------+-----------+ 1 row in set (0.00 sec) mysql&gt; set global long_query_time=4; Query OK, 0 rows affected (0.00 sec) mysql&gt; show variables like &#39;long_query_time&#39;; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.000000 | +-----------------+-----------+ 1 row in set (0.00 sec) 如上所示，我修改了变量long_query_time，但是查询变量long_query_time的值还是10，难道没有修改到呢？注意：使用命令 set global long_query_time=4修改后，需要重新连接或新开一个会话才能看到修改值。你用show variables like ‘long_query_time’查看是当前会话的变量值，你也可以不用重新连接会话，而是用show global variables like ‘long_query_time’; 如下所示： 在MySQL里面执行下面SQL语句，然后我们去检查对应的慢查询日志，就会发现类似下面这样的信息。 mysql&gt; select sleep(3); +----------+ | sleep(3) | +----------+ | 0 | +----------+ 1 row in set (3.00 sec) [root@DB-Server ~]# more /tmp/mysql_slow.log /usr/sbin/mysqld, Version: 5.6.20-enterprise-commercial-advanced-log (MySQL Enterprise Server - Advanced Edition (Commercial)). started with: Tcp port: 0 Unix socket: (null) Time Id Command Argument /usr/sbin/mysqld, Version: 5.6.20-enterprise-commercial-advanced-log (MySQL Enterprise Server - Advanced Edition (Commercial)). started with: Tcp port: 0 Unix socket: (null) Time Id Command Argument # Time: 160616 17:24:35 # User@Host: root[root] @ localhost [] Id: 5 # Query_time: 3.002615 Lock_time: 0.000000 Rows_sent: 1 Rows_examined: 0 SET timestamp=1466069075; select sleep(3); log_output 参数是指定日志的存储方式。log_output=’FILE’表示将日志存入文件，默认值是’FILE’。log_output=’TABLE’表示将日志存入数据库，这样日志信息就会被写入到mysql.slow_log表中。MySQL数据库支持同时两种日志存储方式，配置的时候以逗号隔开即可，如：log_output=’FILE,TABLE’。日志记录到系统的专用日志表中，要比记录到文件耗费更多的系统资源，因此对于需要启用慢查询日志，又需要能够获得更高的系统性能，那么建议优先记录到文件。 mysql&gt; show variables like &#39;%log_output%&#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | log_output | FILE | +---------------+-------+ 1 row in set (0.00 sec) mysql&gt; set global log_output=&#39;TABLE&#39;; Query OK, 0 rows affected (0.00 sec) mysql&gt; show variables like &#39;%log_output%&#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | log_output | TABLE | +---------------+-------+ 1 row in set (0.00 sec) mysql&gt; select sleep(5) ; +----------+ | sleep(5) | +----------+ | 0 | +----------+ 1 row in set (5.00 sec) mysql&gt; mysql&gt; select * from mysql.slow_log; +---------------------+---------------------------+------------+-----------+-----------+---------------+----+----------------+-----------+-----------+-----------------+-----------+ | start_time | user_host | query_time | lock_time | rows_sent | rows_examined | db | last_insert_id | insert_id | server_id | sql_text | thread_id | +---------------------+---------------------------+------------+-----------+-----------+---------------+----+----------------+-----------+-----------+-----------------+-----------+ | 2016-06-16 17:37:53 | root[root] @ localhost [] | 00:00:03 | 00:00:00 | 1 | 0 | | 0 | 0 | 1 | select sleep(3) | 5 | | 2016-06-16 21:45:23 | root[root] @ localhost [] | 00:00:05 | 00:00:00 | 1 | 0 | | 0 | 0 | 1 | select sleep(5) | 2 | +---------------------+---------------------------+------------+-----------+-----------+---------------+----+----------------+-----------+-----------+-----------------+-----------+ 2 rows in set (0.00 sec) mysql&gt; 系统变量log-queries-not-using-indexes：未使用索引的查询也被记录到慢查询日志中（可选项）。如果调优的话，建议开启这个选项。另外，开启了这个参数，其实使用full index scan的sql也会被记录到慢查询日志。 This option does not necessarily mean that no index is used. For example, a query that uses a full index scan uses an index but would be logged because the index would not limit the number of rows. mysql&gt; show variables like &#39;log_queries_not_using_indexes&#39;; +-------------------------------+-------+ | Variable_name | Value | +-------------------------------+-------+ | log_queries_not_using_indexes | OFF | +-------------------------------+-------+ 1 row in set (0.00 sec) mysql&gt; set global log_queries_not_using_indexes=1; Query OK, 0 rows affected (0.00 sec) mysql&gt; show variables like &#39;log_queries_not_using_indexes&#39;; +-------------------------------+-------+ | Variable_name | Value | +-------------------------------+-------+ | log_queries_not_using_indexes | ON | +-------------------------------+-------+ 1 row in set (0.00 sec) 系统变量log_slow_admin_statements表示是否将慢管理语句例如ANALYZE TABLE和ALTER TABLE等记入慢查询日志 mysql&gt; show variables like &#39;log_slow_admin_statements&#39;; +---------------------------+-------+ | Variable_name | Value | +---------------------------+-------+ | log_slow_admin_statements | OFF | +---------------------------+-------+ 1 row in set (0.00 sec) mysql&gt; 系统变量log_slow_slave_statements 表示 By default, a replication slave does not write replicated queries to the slow query log. To change this, use thelog_slow_slave_statements system variable. When the slow query log is enabled, this variable enables logging for queries that have taken more than long_query_time seconds to execute on the slave. This variable was added in MySQL 5.7.1. Setting this variable has no immediate effect. The state of the variable applies on all subsequent START SLAVE statements. 参数–log-short-format The server writes less information to the slow query log if you use the –log-short-format option.H2M_LI_HEADERCommand-Line Format--log-short-formatPermitted Values**Type**booleanDefaultFALSE 另外，如果你想查询有多少条慢查询记录，可以使用系统变量。 mysql&gt; show global status like &#39;%Slow_queries%&#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | Slow_queries | 2104 | +---------------+-------+ 1 row in set (0.00 sec) mysql&gt; 日志分析工具mysqldumpslow 在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活，MySQL提供了日志分析工具mysqldumpslow 查看mysqldumpslow的帮助信息： [root@DB-Server ~]# mysqldumpslow --help Usage: mysqldumpslow [ OPTS... ] [ LOGS... ] Parse and summarize the MySQL slow query log. Options are --verbose verbose --debug debug --help write this text to standard output -v verbose -d debug -s ORDER what to sort by (al, at, ar, c, l, r, t), &#39;at&#39; is default al: average lock time ar: average rows sent at: average query time c: count l: lock time r: rows sent t: query time -r reverse the sort order (largest last instead of first) -t NUM just show the top n queries -a don&#39;t abstract all numbers to N and strings to &#39;S&#39; -n NUM abstract numbers with at least n digits within names -g PATTERN grep: only consider stmts that include this string -h HOSTNAME hostname of db server for *-slow.log filename (can be wildcard), default is &#39;*&#39;, i.e. match all -i NAME name of server instance (if using mysql.server startup script) -l don&#39;t subtract lock time from total time -s, 是表示按照何种方式排序， c : 访问计数 l : 锁定时间 r : 返回记录 t : 查询时间 al : 平均锁定时间 ar : 平均返回记录数 at : 平均查询时间 -t, 是top n的意思，即为返回前面多少条的数据； -g, 后边可以写一个正则匹配模式，大小写不敏感的； 比如 得到返回记录集最多的10个SQL。 mysqldumpslow -s r -t 10 /database/mysql/mysql06_slow.log 得到访问次数最多的10个SQL mysqldumpslow -s c -t 10 /database/mysql/mysql06_slow.log 得到按照时间排序的前10条里面含有左连接的查询语句。 mysqldumpslow -s t -t 10 -g “left join” /database/mysql/mysql06_slow.log 另外建议在使用这些命令时结合 | 和more 使用 ，否则有可能出现刷屏的情况。 mysqldumpslow -s r -t 20 /mysqldata/mysql/mysql06-slow.log | more 参考资料： https://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_long_query_time 作者：潇湘隐者 出处：http://www.cnblogs.com/kerrycode/","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/tags/MySQL/"},{"name":"性能优化","slug":"性能优化","permalink":"https://blog.fenxiangz.com/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"慢查","slug":"慢查","permalink":"https://blog.fenxiangz.com/tags/%E6%85%A2%E6%9F%A5/"}]},{"title":"高性能 MySQL 阅读笔记 (1)","slug":"dateabase/mysql/2020-09-09_高性能MySQL阅读笔记(1)","date":"2020-09-09T00:00:00.000Z","updated":"2020-12-20T16:47:02.947Z","comments":true,"path":"post/dateabase/mysql/2020-09-09_高性能MySQL阅读笔记(1).html","link":"","permalink":"https://blog.fenxiangz.com/post/dateabase/mysql/2020-09-09_%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0(1).html","excerpt":"","text":"概述什么是性能？完成某件任务所需要的时间度量，也就是响应时间。 什么是优化？在一定的工作负载下，尽可能地降低响应时间。 核心：找到时间花在哪里，这很重要。 如何找？通过测量，所以测量是性能优化的关键方法。 如何测量？找到系统的可测量点，但首先需要系统可测量化的支持，然而实际情况是系统很少可以做到可测量化。 因为很难进行系统内部测量，所以我们只能尽可能通过外部去测量系统。 另外需要注意是的，无论是内部测量还是外部测量，数据都不一定是百分之百准确的。 举例：如果SQL慢查了，花费了10s，如果9.6s都在等待磁盘IO，那么追究其他的0.4s就没什么意义。 理解性能优剖析什么是值得优化的查询1. 占系统总体性能影响比重大的； 2. 投入成本低于优化后的收入； 异常情况系统内部的异常 未知的未知性能优化过程发现了“丢失的时间”，比如：程序内部测量点发现耗时10s，但MySQL内部测量发现耗时是8s，那么丢失的2s可能就是没有测量到的，需要注意。 注意平均值问题平均值往往掩盖一些频率小但有性能问题的点 对应用程序剖析增加测量点本身一定程度上会增加部分开销，但这部分开销如果远小于性能优化的贡献。轻量级监控：为了尽可能降低性能监控开销，可以增加灰度测量点，只针对n%的概率进行监控。 剖析MySQL× 慢查日志 × 剖析单条查询","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/tags/MySQL/"},{"name":"性能优化","slug":"性能优化","permalink":"https://blog.fenxiangz.com/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"阅读笔记","slug":"阅读笔记","permalink":"https://blog.fenxiangz.com/tags/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}]},{"title":"MSSQL · 最佳实践 ·  SQL Server备份策略","slug":"dateabase/MSSQL/2020-09-08_MSSQL最佳实践_SQL_Server备份策略","date":"2020-09-08T00:00:00.000Z","updated":"2020-12-20T16:47:02.945Z","comments":true,"path":"post/dateabase/MSSQL/2020-09-08_MSSQL最佳实践_SQL_Server备份策略.html","link":"","permalink":"https://blog.fenxiangz.com/post/dateabase/MSSQL/2020-09-08_MSSQL%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5_SQL_Server%E5%A4%87%E4%BB%BD%E7%AD%96%E7%95%A5.html","excerpt":"","text":"摘要在上一期月报中我们分享了SQL Server三种常见的备份技术及工作方式，本期月报将分享如何充分利用三者的优点来制定SQL Server数据库的备份和还原策略以达到数据库快速灾难恢复能力。 上期月报：MSSQL · 最佳实践 · SQL Server三种常见备份 三个术语在详细介绍SQL Server的灾备策略之前，我们先简要介绍三个重要的术语： RTO (Recovery Time Objective)恢复时间目标，是指出现灾难后多长时间能成功恢复数据库，即企业可容许服务中断的最大时间长度。比如说灾难发生后一天内恢复成功，则RTO值就是二十四小时； RPO (Recovery Point Objective)恢复点目标，是指服务恢复后，恢复回来的数据所对应的最新时间点。比如企业每天凌晨零晨进行完全备份一次，那么这个全备恢复回来的系统数据只会是最近灾难发生当天那个凌晨零时的资料； ERT(Estimated Recovery Time)预估恢复时间，是指根据备份链路的长度和备份文件的大小以及设备的还原效率来估算的服务恢复时间。从以上的三个术语解释来看，最优的灾备方案是RTO极小，即出现故障能够立马恢复数据；RPO无线接近故障时间点，即最少的数据丢失；ERT最小，即可快速恢复服务。但是，现实场景中的灾备方案往往很难达到如此优化的方案。 制定灾备策略以上三个术语是衡量灾备方案和还原策略优劣的重要指标，我们的灾备策略的目标也是无限的靠近RTO、RPO和ERT的最优值。以下我们列举一个典型的灾备场景来分析和解答：假设某个企业对SQL Server数据库DBA提出的灾难恢复要求是数据丢失不超过一小时（RPO不超过一小时），在尽可能短的时间内（RTO尽可能短）恢复应用数据库服务，且灾备策略必须具备任意时间点还原的能力。综合上一期月报分享，我们先抛开灾备策略的优劣来看，我们看看三种典型的灾备策略方案是否可以实现RPO？ 每个小时一次完全备份：备份文件过大，备份还原效率低下，这种方案无法实现任意时间点的还原； 每天一个完全备份 + 每小时一个日志备份：解决了备份文件过大和效率问题，也可以实现任意时间点还原，但是拉长了日志还原链条； 每天一个完全备份 + 每六个小时一个差异备份 + 每小时一个日志备份：具备任意时间点还原的能力，综合了备份文件大小、效率和备份链条长度。从这个分析来看，也恰好应证了上一期的月报中的结论，即：完全备份集是所有备份的基础，但数据量大且备份耗时；事务日志备份集相对较小且快速，但会拉长备份文件还原链条，增大还原时间开销；差异备份解决了事务日志备份链条过长的问题。 时间点恢复我们假设备份数据增量为每小时1GB，初始完全备份大小为100GB，按照时间维度计算每小时产生的备份集大小，统计如下： 典型场景假设我们非常重要的订单数据库，在13:30被人为的错误删除掉了，灾备系统在14:00进行了一个事务日志备份。那么，这个事务日志备份对我们业务的灾难恢复就非常关键和重要了，它使得我们有能力将数据库还原到13:29:59这个时间点。如此，我们只会丢失13:30 - 14:00之间的这半个小时的数据（实际上我们也有能力找回13:30 - 14:00）。但是，如果没有14:00这个事务日志备份文件，但存在13:00的事务日志备份文件的话，我们的系统数据会丢失13:00 - 14:00之间这一个小时的数据，一个小时的数据丢失是公司不被允许的。场景如下图展示： 模拟备份策略我们可以使用以下方法模拟灾备方案和灾难恢复的步骤： 第一步：创建测试数据库并修改为FULL模式 第二步：创建一个完全备份 第三步：每一个小时做一次事务日志备份 第四步：每六个小时做一个差异备份 详细的模拟方法和语句如下所示： -- Create testing DB IF DB_ID(&#39;TestDR&#39;) IS NULL CREATEDATABASE TestDR; GO -- Change Database to FULL Recovery Mode -- for time point recovery supportingALTERDATABASE [TestDR] SETRECOVERYFULLWITH NO_WAIT GOUSE TestDR GO-- Create Testing TableIF OBJECT_ID(&#39;dbo.tb_DR&#39;, &#39;U&#39;) ISNOTNULLDROPTABLE dbo.tb_DR GOCREATETABLE dbo.tb_DR ( IDINTIDENTITY(1,1) NOTNULL PRIMARY KEY, CommentVARCHAR(100) NULL, Indate DATETIME NOTNULLDEFAULT(GETDATE()) ); GO USE TestDR GO-- Init dataINSERTINTO dbo.tb_DR(Comment) SELECT&#39;Full Backup @ 00:00&#39;; -- Take Full BackupBACKUPDATABASE [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@00:00_FULL.bak&#39;WITH COMPRESSION,INIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 01:00&#39;; -- Take TRN BackupBACKUPLOG [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@01:00_LOG.trn&#39;WITH COMPRESSION,NOINIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 02:00&#39;; -- Take TRN BackupBACKUPLOG [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@02:00_LOG.trn&#39;WITH COMPRESSION,NOINIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 03:00&#39;; -- Take TRN BackupBACKUPLOG [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@03:00_LOG.trn&#39;WITH COMPRESSION,NOINIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 04:00&#39;; -- Take TRN BackupBACKUPLOG [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@04:00_LOG.trn&#39;WITH COMPRESSION,NOINIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 05:00&#39;; -- Take TRN BackupBACKUPLOG [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@05:00_LOG.trn&#39;WITH COMPRESSION,NOINIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;DIFF Backup @ 06:00&#39;; -- Take DIFF BackupBACKUPDATABASE [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@06:00_DIFF.bak&#39;WITH DIFFERENTIAL,COMPRESSION,NOINIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 07:00&#39;; -- Take TRN BackupBACKUPLOG [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@07:00_LOG.trn&#39;WITH COMPRESSION,NOINIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 08:00&#39;; -- Take TRN BackupBACKUPLOG [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@08:00_LOG.trn&#39;WITH COMPRESSION,NOINIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 09:00&#39;; -- Take TRN BackupBACKUPLOG [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@09:00_LOG.trn&#39;WITH COMPRESSION,NOINIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 10:00&#39;; -- Take TRN BackupBACKUPLOG [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@10:00_LOG.trn&#39;WITH COMPRESSION,NOINIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 11:00&#39;; -- Take TRN BackupBACKUPLOG [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@11:00_LOG.trn&#39;WITH COMPRESSION,NOINIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;DIFF Backup @ 12:00&#39;; -- Take DIFF BackupBACKUPDATABASE [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@12:00_DIFF.bak&#39;WITH DIFFERENTIAL,COMPRESSION,NOINIT,STATS=5; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 13:00&#39;; -- Take TRN BackupBACKUPLOG [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@13:00_LOG.trn&#39;WITH COMPRESSION,NOINIT,STATS=5; -- This record is similate for point time recoveryINSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 13:29:59&#39;; WAITFOR DELAY &#39;00:00:02&#39; INSERTINTO dbo.tb_DR(Comment) SELECT&#39;Transaction Log Backup @ 14:00&#39;; -- Take TRN BackupBACKUPLOG [TestDR] TO DISK =N&#39;C:\\Temp\\TestDR_20171217@14:00_LOG.trn&#39;WITH COMPRESSION,NOINIT,STATS=5; -- Query DataSELECT * FROM dbo.tb_DR; 我们看看测试表的数据情况，方框选中的这条数据是需要我们恢复出来的： 我们也可以再次检查数据库备份历史记录，来确保灾备信息准确性： SELECT bs.database_name AS&#39;Database Name&#39;, bs.backup_start_date AS&#39;Backup Start&#39;, bs.backup_finish_date AS&#39;Backup Finished&#39;, DATEDIFF(MINUTE, bs.backup_start_date, bs.backup_finish_date) AS&#39;Duration (min)&#39;, bmf.physical_device_name AS&#39;Backup File&#39;, CASEWHEN bs.[type] = &#39;D&#39;THEN&#39;Full Backup&#39;WHEN bs.[type] = &#39;I&#39;THEN&#39;Differential Database&#39;WHEN bs.[type] = &#39;L&#39;THEN&#39;Log&#39;WHEN bs.[type] = &#39;F&#39;THEN&#39;File/Filegroup&#39;WHEN bs.[type] = &#39;G&#39;THEN&#39;Differential File&#39;WHEN bs.[type] = &#39;P&#39;THEN&#39;Partial&#39;WHEN bs.[type] = &#39;Q&#39;THEN&#39;Differential partial&#39;ENDAS&#39;Backup Type&#39;FROM msdb.dbo.backupmediafamily bmf WITH(NOLOCK) INNERJOIN msdb..backupset bs WITH(NOLOCK) ON bmf.media_set_id = bs.media_set_id WHERE bs.database_name = &#39;TestDR&#39;ORDERBY bs.backup_start_date ASC 查询的灾备历史记录展示如下： 从这个备份历史记录来看，和我们的测试表中的数据是吻合且对应起来的。 灾难恢复步骤接下来，我们需要根据TestDR数据库的备份文件，将数据库恢复到模拟时间点2017-12-17 23:04:45.130（即真实场景中的发生人为操作失误的时间点13:30），为了包含ID为15的这条数据，我们就恢复到2017-12-17 23:04:46.130时间点即可，然后检查看看ID等于15的这条记录是否存在，如果这条记录存在，说明我们备份和还原策略工作正常，否则无法实现公司的要求。为了试验的目的，我们先把TestDR数据库删除掉（真实环境，请不要随意删除数据库，这很危险）： -- for testing, drop db first.USE [master] GOALTERDATABASE [TestDR] SET SINGLE_USER WITHROLLBACKIMMEDIATEGODROPDATABASE [TestDR] GO 恢复方案一：全备 + 日志备份为了实现灾难恢复，我们需要先把完全备份文件恢复，然后一个接一个的事务日志备份按时间升序恢复，在最后一个事务日志恢复的时候，使用STOPAT关键字恢复到时间点并把数据库Recovery回来带上线，详细的代码如下： USE [master] GO-- restore from full backupRESTOREDATABASE TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@00:00_FULL.bak&#39;WITH NORECOVERY, REPLACE-- restore from log backupRESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@01:00_LOG.trn&#39;WITH NORECOVERY RESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@02:00_LOG.trn&#39;WITH NORECOVERY RESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@03:00_LOG.trn&#39;WITH NORECOVERY RESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@04:00_LOG.trn&#39;WITH NORECOVERY RESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@05:00_LOG.trn&#39;WITH NORECOVERY -- skip diff backup at 06:00RESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@07:00_LOG.trn&#39;WITH NORECOVERY RESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@08:00_LOG.trn&#39;WITH NORECOVERY RESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@09:00_LOG.trn&#39;WITH NORECOVERY RESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@10:00_LOG.trn&#39;WITH NORECOVERY RESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@11:00_LOG.trn&#39;WITH NORECOVERY -- skip diff backup at 12:00RESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@13:00_LOG.trn&#39;WITH NORECOVERY -- restore from log and stop at 2017-12-17 23:04:46.130RESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@14:00_LOG.trn&#39;WITH STOPAT = &#39;2017-12-17 23:04:46.130&#39;, RECOVERY-- Double check test dataUSE TestDR GOSELECT * FROM dbo.tb_DR 从测试表中的数据展示来看，我们已经成功的将ID为15的这条数据还原回来，即发生人为失误导致的数据丢失（灾难）已经恢复回来了。 细心的你一定发现了这个恢复方案，使用的是完全备份 + 很多个事务日志备份来恢复数据的，这种方案的恢复链条十分冗长，在这里，恢复到第13个备份文件才找回了我们想要的数据。有没有更为简单，恢复更为简洁的灾难恢复方案呢？请看恢复方案二。 恢复方案二：全备 + 差备 + 日志备份为了解决完全备份 + 日志备份恢复链条冗长的问题，我们接下来采取一种更为简洁的恢复方案，即采用完全备份 + 差异备份 + 事务日志备份的方法来实现灾难恢复，方法如下： --=========FULL + DIFF + TRN LOGUSE [master] GO-- restore from full backupRESTOREDATABASE TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@00:00_FULL.bak&#39;WITH NORECOVERY, REPLACE-- restore from diff backupRESTOREDATABASE TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@12:00_DIFF.bak&#39;WITH NORECOVERY -- restore from trn logRESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@13:00_LOG.trn&#39;WITH NORECOVERY -- restore from log and stop at 2017-12-17 23:04:46.130RESTORELOG TestDR FROM DISK = &#39;C:\\Temp\\TestDR_20171217@14:00_LOG.trn&#39;WITH STOPAT = &#39;2017-12-17 23:04:46.130&#39;, RECOVERY-- Double check test dataUSE TestDR GOSELECT * FROM dbo.tb_DR 从这个灾难恢复链路来看，将灾难恢复的步骤从13个备份文件减少到4个备份文件，链路缩短，方法变得更为简洁快速。当然同样可以实现相同的灾难恢复效果，满足公司的对数据RPO的要求。 恢复方案三：使用SSMS当然灾难恢复的方法除了使用脚本以外，微软的SSMS工具通过IDE UI操作也是可以达到相同的效果，可以实现相同的功能，方法如下：右键点击你需要还原的数据库 =&gt; Tasks =&gt; Restore =&gt; Database，如下如所示： 选择Timeline =&gt; Specific date and time =&gt; 设置你需要还原到的时间点（这里选择2017-12-17 23:04:46） =&gt; 确定。时间点恢复还原时间消耗取决于你数据库备份文件的大小，在我的例子中，一会功夫，就已经还原好你想要的数据库了。 最后总结本期月报是继前一个月分享SQL Server三种常见的备份技术后的深入，详细讲解了如何制定灾备策略来满足企业对灾难恢复能力的要求，并以一个具体的例子来详细阐述了SQL Server灾备的策略和灾难恢复的方法，使企业在数据库灾难发生时，数据损失最小化。但是，这里还是有一个疑问暂时留给读者：为什么我们可以使用多种灾难恢复（我们这里只谈到了两种，实际上还有其他方法）的方法呢？到底底层的原理是什么的？预知后事如何，我们下期月报分享。 参考典型场景中的场景图 Point-in-time recovery 原文：https://developer.aliyun.com/article/379022","categories":[{"name":"MSSQL","slug":"MSSQL","permalink":"https://blog.fenxiangz.com/categories/MSSQL/"}],"tags":[{"name":"备份","slug":"备份","permalink":"https://blog.fenxiangz.com/tags/%E5%A4%87%E4%BB%BD/"},{"name":"MSSQL","slug":"MSSQL","permalink":"https://blog.fenxiangz.com/tags/MSSQL/"}]},{"title":"How to learn Spring Cloud – the practical way","slug":"java/spring/2020-09-02_How-to-learn-Spring-Cloud","date":"2020-09-02T00:00:00.000Z","updated":"2020-12-20T16:47:02.971Z","comments":true,"path":"post/java/spring/2020-09-02_How-to-learn-Spring-Cloud.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/spring/2020-09-02_How-to-learn-Spring-Cloud.html","excerpt":"","text":"I have recently spoken at a meetup about Practical Choreography with Spring Cloud Stream. It was a great event where I was asked many questions after the talk. One question got me thinking: “What book about Spring Cloud do you recommend?” *which as it turns out boils down to *“How do you learn Spring Cloud?”. I heard that question posed a few times before in different ways. Here, I will give you my answer on what I think is the best way of learning Spring Cloud. With Spring Cloud being probably the hottest framework on JVM for integrating microservices, the interest in it is growing. Most people interested in the microservices are already familiar with Spring Boot. If you haven’t heard of it before, check out my Spring Boot introduction blog post, and definitely see the official site– it has some very good Getting Started Guides. With that out of the way, let’s look at learning Spring Cloud! Understand the ScopeThe first thing to do when trying to learn something so big and diverse is understanding the scope. Learning Spring Cloud can mean many things. First of all, the Spring Cloud currently contains: Spring Cloud Config Spring Cloud Netflix Spring Cloud Bus Spring Cloud for Cloud Foundry Spring Cloud Cloud Foundry Service Broker Spring Cloud Cluster Spring Cloud Consul Spring Cloud Security Spring Cloud Sleuth Spring Cloud Data Flow Spring Cloud Stream Spring Cloud Stream App Starters Spring Cloud Task Spring Cloud Task App Starters Spring Cloud Zookeeper Spring Cloud for Amazon Web Services Spring Cloud Connectors Spring Cloud Starters Spring Cloud CLI Spring Cloud Contract Spring Cloud Gateway Wow! This is a lot to take in! Clearly, the number of different projects here means that you can’t learn it by simply going through them one by one with a hope of understanding or mastering Spring Cloud by the end of it. So, what is the best strategy for learning such an extensive framework (or a microservice blueprint, as I describe it in another article)? I think the most sensible ways of learning is understanding what you would like to use Spring Cloud for. Setting yourself a learning goal. Goal Oriented LearningWhat kind of learning goals are we talking about here? Let me give you a few ideas: Set up communication between microservices based on Spring Cloud Stream Build microservices that use configuration provided by Spring Cloud Config Build a small microservices system based on Orchestration- what is needed and how to use it Test microservices with Spring Cloud Contract Use Spring Cloud Data Flow to take data from one place, modify it and store it in Elastic Search If you are interested in learning some parts of Spring Cloud, think of an absolutely tiny project and build it! Once you have done it, you know that you understood at least the basics and you validated it by having something working. I will quote Stephen R. Covey here (author of “The 7 Habits of Highly Effective People”): “to learn and not to do is really not to learn. To know and not to do is really not to know.” With topics as complex and broad as Spring Cloud, this quote rings very true! StudyYou picked your goal and you want to get started. What resources can help you? I will give you a few ideas here, but remember- the goal is to learn only as much as necessary in order to achieve your goal. Don’t learn much more just yet, as you may end up overwhelmed and move further away from completing your goal. There will be time to learn more in depth. Let’s assume that your goal is Using Spring Cloud Config correctly in your personal project. Here are the resources I recommend: Official Spring Cloud Config Quickstart to get a basic idea If you enjoy books and want to learn more Spring Cloud in the future – Spring Microservices in Action is a great reference. Don’t read it all yet! Check out the chapters on Spring Cloud Configuration and read as much as necessary to know what to do. If you use Pluralsight, then check out Java Microservices with Spring Cloud: Developing Services – a very good introduction! Again, start with the chapters on Spring Cloud Config. You can google the topic and find articles like Quick Intro to Spring Cloud Configuration You can even find YouTube videos about Spring Cloud Config I really want to make a point here. There is a huge amount of resources out there, free or paid of very high quality. You can spend weeks just reviewing them, but this is a mistake. Chose what works for you and get moving towards your goal! Do something – achieve your goalOnce you identified the resources you need, get on with your goal! If your goal was to learn about Spring Cloud Config- set up the server, get the clients connecting and experiment with it. You should have enough information to complete your simple task. If you find that something is not working- great! That shows that you need to revisit the resources and correct your understanding. If you completed your goal, but you want to experiment more with the tech- go for it! You have something working and playing with it is much more fun than reading dry tech documentation. By playing with the technology you start to notice nuances and develop a deeper understanding. Understanding that will not be easily acquired by reading countless articles, as most things would just fly over your head. Study AgainOnce you completed your goal and played a little with the tech you should have a much better idea what you are dealing with. Now is the time to go deep! Read all you can around the area that you explored. See what you could have done differently, how it is used and what are the best practices. Now, all the reading you will do will make much more sense and will be more memorable. Suddenly dry documentation turns into fascinating discoveries of what you could have done better. And the best of all- if something sounds really great- you have your test-bed to try it. TeachTeaching others really helps with memorizing and understanding the subject. This is one of the reasons why I am writing this blog. You not only get a chance of sharing your knowledge but also learn yourself by teaching. If blogging is not your thing, you can talk to your colleagues or friends about what you have been tinkering with. You may be confronted with questions or perspectives that you did not consider before- great! Another chance to make the learning more complete. One thing to remember is- don’t be afraid to teach. Even if what you have just learned seems basic to you- it was not so basic before you started learning it! If you were in this position, then so must be countless others! There is a value to the unique way you can explain the subject in your own way. Especially given your practical experience gained from the goal that you achieved. Staying up to DateSpring Cloud is constantly changing and growing. If your ultimate goal is becoming an expert in this ecosystem, then you need to think about ways of staying up to date. One thing that is pretty much a must is working with it. If you are not lucky enough to use it on your day job- make sure that you use it in your spare time. You could be building a personal project making use of the tech or simply tinker with it and try different things. What matters is that you actually get that hands-on experience. The second part of staying fresh is knowing whats coming and reading other people experiences. Some of the sources I really enjoy following are: The Spring.io blog with a very good newsletter Baeldung – an amazing source of Spring related articles and a weekly newsletter InfoQ Microservices – huge and very active website maintained by multiple authors Using Twitter to stay up to date and see what people are reading. I share plenty of articles on that topic with my @bartoszjd account. These are just some of the sources that I follow. There are countless others. The point is to choose some that you enjoy reading and keep an eye for exciting stuff. ConclusionSpring Cloud is a huge and fascinating set of tools for building microservices. It can’t be learned as a “single thing”. Using different goals is the best way of approaching this learning. The idea presented here can be used for learning any technical concept. I found it extremely beneficial for myself and used it with success. I really recommend checking out SimpleProgrammer’s Learning to learn article which describes very similar idea for learning new technologies or frameworks. Happy learning! 原文：https://www.e4developer.com/2018/03/06/how-to-learn-spring-cloud-the-practical-way/","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://blog.fenxiangz.com/categories/Spring-Cloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://blog.fenxiangz.com/tags/Spring/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://blog.fenxiangz.com/tags/Spring-Cloud/"}]},{"title":"字符集知识点","slug":"java/advance/2020-06-29_字符集知识点","date":"2020-06-28T00:00:00.000Z","updated":"2020-12-20T16:47:02.963Z","comments":true,"path":"post/java/advance/2020-06-29_字符集知识点.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2020-06-29_%E5%AD%97%E7%AC%A6%E9%9B%86%E7%9F%A5%E8%AF%86%E7%82%B9.html","excerpt":"","text":"获取系统支持的字符集Charset.availableCharsets(); // Java 编解码编码：字符 -&gt; 字节 ；解码：字节 -&gt; 字符； 字符集ASCII : American Standard Code for Information Interchange7 bit来表示一个字符，共计可以表示 128 种字符。 IOS-8859-18 bit来表示一个字符，一个字节表示一个字符，基于 ASCII 向后扩展，完全兼容 ASCII，共计可以表示 256 种字符。 GB23122个字节（16位）表示一个汉字。 GBK扩展了 GB2312， 增加生僻字。 GB18030继续扩展了GBK。 Big5台湾，繁体字。 Unicode 编码方式全球统一字符，两个字节表示一个字符，256^2 个字符。 UTF: Unicode Translation Format由于 Unicode 存储问题（2个字节），诞生了 UTF，本质上 UTF 是一种存储方式，而不是编码方式。UTF 存在： UTF-8，UTF-16 （UTF-16LE，UTF-16BE），UTF-32 。UTF-16 ：ZERO WIDTH NO-BREAK SPACE : 0xFEFF (BE) , 0xFFFE (LE)。 UTF-8变长字节表示形式，兼容： IOS-8859-1；通过3个字节表示一个中文； BOMByte Order Mark ， Windows遗留问题，BOM是用来判断文本文件是哪一种Unicode编码的标记，其本身是一个Unicode字符（”\\uFEFF”），位于文本文件头部。 在不同的Unicode编码中，对应的bom的二进制字节如下： FE FF -- UTF16BE FF FE -- UTF16LE EF BB BF -- UTF8","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"字符集","slug":"字符集","permalink":"https://blog.fenxiangz.com/tags/%E5%AD%97%E7%AC%A6%E9%9B%86/"}]},{"title":"深入理解JVM-内存模型（jmm）和GC","slug":"java/advance/2020-06-28_java_jvm","date":"2020-06-28T00:00:00.000Z","updated":"2020-12-20T16:47:02.963Z","comments":true,"path":"post/java/advance/2020-06-28_java_jvm.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2020-06-28_java_jvm.html","excerpt":"","text":"转载：https://www.jianshu.com/p/76959115d486 1 CPU和内存的交互了解jvm内存模型前，了解下cpu和计算机内存的交互情况。【因为Java虚拟机内存模型定义的访问操作与计算机十分相似】 有篇很棒的文章，从cpu讲到内存模型:什么是java内存模型 在计算机中，cpu和内存的交互最为频繁，相比内存，磁盘读写太慢，内存相当于高速的缓冲区。 但是随着cpu的发展，内存的读写速度也远远赶不上cpu。因此cpu厂商在每颗cpu上加上高速缓存，用于缓解这种情况。现在cpu和内存的交互大致如下。 cpu上加入了高速缓存这样做解决了处理器和内存的矛盾(一快一慢)，但是引来的新的问题 缓存一致性 在多核cpu中，每个处理器都有各自的高速缓存(L1,L2,L3)，而主内存确只有一个 。 以我的pc为例,因为cpu成本高，缓存区一般也很小。 CPU要读取一个数据时，首先从一级缓存中查找，如果没有找到再从二级缓存中查找，如果还是没有就从三级缓存或内存中查找，每个cpu有且只有一套自己的缓存。 如何保证多个处理器运算涉及到同一个内存区域时，多线程场景下会存在缓存一致性问题，那么运行时保证数据一致性？ 为了解决这个问题，各个处理器需遵循一些协议保证一致性。【如MSI，MESI啥啥的协议。。】 大概如下 |-cpu与内存-| 在CPU层面，内存屏障提供了个充分必要条件 1.1.1 内存屏障(Memory Barrier)CPU中，每个CPU又有多级缓存【上图统一定义为高速缓存】，一般分为L1,L2,L3，因为这些缓存的出现，提高了数据访问性能，避免每次都向内存索取，但是弊端也很明显，不能实时的和内存发生信息交换，分在不同CPU执行的不同线程对同一个变量的缓存值不同。 硬件层的内存屏障分为两种：Load Barrier 和 Store Barrier即读屏障和写屏障。【内存屏障是硬件层的】 为什么需要内存屏障由于现代操作系统都是多处理器操作系统，每个处理器都会有自己的缓存，可能存再不同处理器缓存不一致的问题，而且由于操作系统可能存在重排序，导致读取到错误的数据，因此，操作系统提供了一些内存屏障以解决这种问题. 简单来说:1.在不同CPU执行的不同线程对同一个变量的缓存值不同，为了解决这个问题。 2.用volatile可以解决上面的问题，不同硬件对内存屏障的实现方式不一样。java屏蔽掉这些差异，通过jvm生成内存屏障的指令。 对于读屏障:在指令前插入读屏障，可以让高速缓存中的数据失效，强制从主内存取。 内存屏障的作用cpu执行指令可能是无序的，它有两个比较重要的作用 1.阻止屏障两侧指令重排序 2.强制把写缓冲区/高速缓存中的脏数据等写回主内存，让缓存中相应的数据失效。 volatile型变量当我们声明某个变量为volatile修饰时，这个变量就有了线程可见性，volatile通过在读写操作前后添加内存屏障。 用代码可以这么理解 //相当于读写时加锁，保证及时可见性，并发时不被随意修改。publicclassSynchronizedInteger&#123;privatelong value;publicsynchronizedintget()&#123;return value;&#125;publicsynchronizedvoidset(long value)&#123;this.value = value;&#125;&#125; volatile型变量拥有如下特性 可见性，对于一个该变量的读，一定能看到读之前最后的写入。 防止指令重排序，执行代码时,为了提高执行效率,会在不影响最后结果的前提下对指令进行重新排序,使用volatile可以防止，比如单例模式双重校验锁的创建中有使用到，如(https://www.jianshu.com/p/b30a4d568be4) 注意的是volatile不具有原子性，如volatile++这样的复合操作,这里感谢大家的指正。 至于volatile底层是怎么实现保证不同线程可见性的，这里涉及到的就是硬件上的，被volatile修饰的变量在进行写操作时，会生成一个特殊的汇编指令，该指令会触发mesi协议，会存在一个总线嗅探机制的东西，简单来说就是这个cpu会不停检测总线中该变量的变化，如果该变量一旦变化了，由于这个嗅探机制，其它cpu会立马将该变量的cpu缓存数据清空掉，重新的去从主内存拿到这个数据。简单画了个图。 2. Java内存区域 前提:本文讲的基本都是以Sun HotSpot虚拟机为基础的，Oracle收购了Sun后目前得到了两个【Sun的HotSpot和JRockit(以后可能合并这两个),还有一个是IBM的IBMJVM】 之所以扯了那么多计算机内存模型，是因为java内存模型的设定符合了计算机的规范。 Java程序内存的分配是在JVM虚拟机内存分配机制下完成。 Java内存模型（Java Memory Model ,JMM）就是一种符合内存模型规范的，屏蔽了各种硬件和操作系统的访问差异的，保证了Java程序在各种平台下对内存的访问都能保证效果一致的机制及规范。 简要言之，jmm是jvm的一种规范，定义了jvm的内存模型。它屏蔽了各种硬件和操作系统的访问差异，不像c那样直接访问硬件内存，相对安全很多，它的主要目的是解决由于多线程通过共享内存进行通信时，存在的本地内存数据不一致、编译器会对代码指令重排序、处理器会对代码乱序执行等带来的问题。可以保证并发编程场景中的原子性、可见性和有序性。 从下面这张图可以看出来，Java数据区域分为五大数据区域。这些区域各有各的用途，创建及销毁时间。 其中方法区和堆是所有线程共享的，栈，本地方法栈和程序虚拟机则为线程私有的。 根据java虚拟机规范，java虚拟机管理的内存将分为下面五大区域。 |-jmm-| 2.1 五大内存区域2.1.1 程序计数器程序计数器是一块很小的内存空间，它是线程私有的，可以认作为当前线程的行号指示器。 为什么需要程序计数器 我们知道对于一个处理器(如果是多核cpu那就是一核)，在一个确定的时刻都只会执行一条线程中的指令，一条线程中有多个指令，为了线程切换可以恢复到正确执行位置，每个线程都需有独立的一个程序计数器，不同线程之间的程序计数器互不影响，独立存储。 注意：如果线程执行的是个java方法，那么计数器记录虚拟机字节码指令的地址。如果为native【底层方法】，那么计数器为空。这块内存区域是虚拟机规范中唯一没有OutOfMemoryError的区域。 2.1.2 Java栈（虚拟机栈）同计数器也为线程私有，生命周期与相同，就是我们平时说的栈，栈描述的是Java方法执行的内存模型。 每个方法被执行的时候都会创建一个栈帧用于存储局部变量表，操作栈，动态链接，方法出口等信息。每一个方法被调用的过程就对应一个栈帧在虚拟机栈中从入栈到出栈的过程。【栈先进后出，下图栈1先进最后出来】 对于栈帧的解释参考 Java虚拟机运行时栈帧结构 栈帧: 是用来存储数据和部分过程结果的数据结构。 栈帧的位置: 内存 -&gt; 运行时数据区 -&gt; 某个线程对应的虚拟机栈 -&gt; here[在这里] 栈帧大小确定时间: 编译期确定，不受运行期数据影响。 通常有人将java内存区分为栈和堆，实际上java内存比这复杂，这么区分可能是因为我们最关注，与对象内存分配关系最密切的是这两个。 平时说的栈一般指局部变量表部分。 局部变量表:一片连续的内存空间，用来存放方法参数，以及方法内定义的局部变量，存放着编译期间已知的数据类型(八大基本类型和对象引用(reference类型),returnAddress类型。它的最小的局部变量表空间单位为Slot，虚拟机没有指明Slot的大小，但在jvm中，long和double类型数据明确规定为64位，这两个类型占2个Slot，其它基本类型固定占用1个Slot。 reference类型:与基本类型不同的是它不等同本身，即使是String，内部也是char数组组成，它可能是指向一个对象起始位置指针，也可能指向一个代表对象的句柄或其他与该对象有关的位置。 returnAddress类型:指向一条字节码指令的地址【深入理解Java虚拟机】怎么理解returnAddress 需要注意的是，局部变量表所需要的内存空间在编译期完成分配，当进入一个方法时，这个方法在栈中需要分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表大小。 Java虚拟机栈可能出现两种类型的异常： 线程请求的栈深度大于虚拟机允许的栈深度，将抛出StackOverflowError。 虚拟机栈空间可以动态扩展，当动态扩展是无法申请到足够的空间时，抛出OutOfMemory异常。 2.1.3 本地方法栈本地方法栈是与虚拟机栈发挥的作用十分相似,区别是虚拟机栈执行的是Java方法(也就是字节码)服务，而本地方法栈则为虚拟机使用到的native方法服务，可能底层调用的c或者c++,我们打开jdk安装目录可以看到也有很多用c编写的文件，可能就是native方法所调用的c代码。 2.1.4 堆对于大多数应用来说，堆是java虚拟机管理内存最大的一块内存区域，因为堆存放的对象是线程共享的，所以多线程的时候也需要同步机制。因此需要重点了解下。 java虚拟机规范对这块的描述是:所有对象实例及数组都要在堆上分配内存，但随着JIT编译器的发展和逃逸分析技术的成熟，这个说法也不是那么绝对，但是大多数情况都是这样的。 即时编译器:可以把把Java的字节码，包括需要被解释的指令的程序）转换成可以直接发送给处理器的指令的程序) 逃逸分析:通过逃逸分析来决定某些实例或者变量是否要在堆中进行分配，如果开启了逃逸分析，即可将这些变量直接在栈上进行分配，而非堆上进行分配。这些变量的指针可以被全局所引用，或者其其它线程所引用。 参考逃逸分析 注意:它是所有线程共享的，它的目的是存放对象实例。同时它也是GC所管理的主要区域，因此常被称为GC堆，又由于现在收集器常使用分代算法，Java堆中还可以细分为新生代和老年代，再细致点还有Eden(伊甸园)空间之类的不做深究。 根据虚拟机规范，Java堆可以存在物理上不连续的内存空间，就像磁盘空间只要逻辑是连续的即可。它的内存大小可以设为固定大小，也可以扩展。 当前主流的虚拟机如HotPot都能按扩展实现(通过设置 -Xmx和-Xms)，如果堆中没有内存内存完成实例分配，而且堆无法扩展将报OOM错误(OutOfMemoryError) 2.1.5 方法区方法区同堆一样，是所有线程共享的内存区域，为了区分堆，又被称为非堆。 用于存储已被虚拟机加载的类信息、常量、静态变量，如static修饰的变量加载类的时候就被加载到方法区中。 运行时常量池 是方法区的一部分，class文件除了有类的字段、接口、方法等描述信息之外，还有常量池用于存放编译期间生成的各种字面量和符号引用。 在老版jdk，方法区也被称为永久代【因为没有强制要求方法区必须实现垃圾回收，HotSpot虚拟机以永久代来实现方法区，从而JVM的垃圾收集器可以像管理堆区一样管理这部分区域，从而不需要专门为这部分设计垃圾回收机制。不过自从JDK7之后，Hotspot虚拟机便将运行时常量池从永久代移除了。】 jdk1.7开始逐步去永久代。从String.interns()方法可以看出来 String.interns()native方法:作用是如果字符串常量池已经包含一个等于这个String对象的字符串，则返回代表池中的这个字符串的String对象，在jdk1.6及以前常量池分配在永久代中。可通过 -XX:PermSize和-XX:MaxPermSize限制方法区大小。 publicclassStringIntern&#123;//运行如下代码探究运行时常量池的位置publicstaticvoidmain(String[] args)throwsThrowable&#123;//用list保持着引用 防止full gc回收常量池List&lt;String&gt; list =newArrayList&lt;String&gt;();int i =0;while(true)&#123; list.add(String.valueOf(i++).intern());&#125;&#125;&#125;//如果在jdk1.6环境下运行 同时限制方法区大小 将报OOM后面跟着PermGen space说明方法区OOM，即常量池在永久代//如果是jdk1.7或1.8环境下运行 同时限制堆的大小 将报heap space 即常量池在堆中 idea设置相关内存大小设置 这边不用全局的方式，设置main方法的vm参数。 做相关设置，比如说这边设定堆大小。（-Xmx5m -Xms5m -XX:-UseGCOverheadLimit） 这边如果不设置UseGCOverheadLimit将报java.lang.OutOfMemoryError: GC overhead limit exceeded， 这个错是因为GC占用了多余98%（默认值）的CPU时间却只回收了少于2%（默认值）的堆空间。目的是为了让应用终止，给开发者机会去诊断问题。一般是应用程序在有限的内存上创建了大量的临时对象或者弱引用对象，从而导致该异常。虽然加大内存可以暂时解决这个问题，但是还是强烈建议去优化代码，后者更加有效，也可通过UseGCOverheadLimit避免[不推荐，这里是因为测试用，并不能解决根本问题] jdk8真正开始废弃永久代，而使用元空间(Metaspace) java虚拟机对方法区比较宽松，除了跟堆一样可以不存在连续的内存空间，定义空间和可扩展空间，还可以选择不实现垃圾收集。 2.2 对象的内存布局在HotSpot虚拟机中。对象在内存中存储的布局分为 1.对象头 2.实例数据 3.对齐填充 2.2.1 对象头【markword】在32位系统下，对象头8字节，64位则是16个字节【未开启压缩指针，开启后12字节】。 markword很像网络协议报文头，划分为多个区间，并且会根据对象的状态复用自己的存储空间。 为什么这么做:省空间，对象需要存储的数据很多，32bit/64bit是不够的，它被设计成非固定的数据结构以便在极小的空间存储更多的信息， 假设当前为32bit，在对象未被锁定情况下。25bit为存储对象的哈希码、4bit用于存储分代年龄，2bit用于存储锁标志位，1bit固定为0。 不同状态下存放数据 这其中锁标识位需要特别关注下。锁标志位与是否为偏向锁对应到唯一的锁状态。 锁的状态分为四种无锁状态、偏向锁、轻量级锁和重量级锁 不同状态时对象头的区间含义，如图所示。 |-对象头-| HotSpot底层通过markOop实现Mark Word，具体实现位于markOop.hpp文件。 markOop中提供了大量方法用于查看当前对象头的状态，以及更新对象头的数据，为synchronized锁的实现提供了基础。[比如说我们知道synchronized锁的是对象而不是代码，而锁的状态保存在对象头中，进而实现锁住对象]。 关于对象头和锁之间的转换，网上大神总结 |-偏向锁轻量级锁重量级锁-| 2.2.2 实例数据存放对象程序中各种类型的字段类型，不管是从父类中继承下来的还是在子类中定义的。 分配策略:相同宽度的字段总是放在一起，比如double和long 2.2.3 对齐填充这部分没有特殊的含义，仅仅起到占位符的作用满足JVM要求。 由于HotSpot规定对象的大小必须是8的整数倍，对象头刚好是整数倍，如果实例数据不是的话，就需要占位符对齐填充。 2.3 对象的访问定位java程序需要通过引用(ref)数据来操作堆上面的对象，那么如何通过引用定位、访问到对象的具体位置。 对象的访问方式由虚拟机决定，java虚拟机提供两种主流的方式 1.句柄访问对象 2.直接指针访问对象。(Sun HotSpot使用这种方式) 参考Java对象访问定位 2.3.1 句柄访问 简单来说就是java堆划出一块内存作为句柄池,引用中存储对象的句柄地址,句柄中包含对象实例数据、类型数据的地址信息。 优点:引用中存储的是稳定的句柄地址,在对象被移动【垃圾收集时移动对象是常态】只需改变句柄中实例数据的指针，不需要改动引用【ref】本身。 |-访问方式2-| 2.3.2 直接指针 与句柄访问不同的是，ref中直接存储的就是对象的实例数据,但是类型数据跟句柄访问方式一样。 优点:优势很明显，就是速度快，相比于句柄访问少了一次指针定位的开销时间。【可能是出于Java中对象的访问时十分频繁的,平时我们常用的JVM HotSpot采用此种方式】 |-访问方式1-| 3.内存溢出两种内存溢出异常[注意内存溢出是error级别的] 1.StackOverFlowError:当请求的栈深度大于虚拟机所允许的最大深度 2.OutOfMemoryError:虚拟机在扩展栈时无法申请到足够的内存空间[一般都能设置扩大] java -verbose:class -version 可以查看刚开始加载的类，可以发现这两个类并不是异常出现的时候才去加载，而是jvm启动的时候就已经加载。这么做的原因是在vm启动过程中我们把类加载起来，并创建几个没有堆栈的对象缓存起来，只需要设置下不同的提示信息即可，当需要抛出特定类型的OutOfMemoryError异常的时候，就直接拿出缓存里的这几个对象就可以了。 比如说OutOfMemoryError对象，jvm预留出4个对象【固定常量】，这就为什么最多出现4次有堆栈的OutOfMemoryError异常及大部分情况下都将看到没有堆栈的OutOfMemoryError对象的原因。 参考OutOfMemoryError解读 Snip20180904_8.png 两个基本的例子 publicclassMemErrorTest&#123;publicstaticvoidmain(String[] args)&#123;try&#123;List&lt;Object&gt; list =newArrayList&lt;Object&gt;();for(;;)&#123; list.add(newObject());//创建对象速度可能高于jvm回收速度&#125;&#125;catch(OutOfMemoryError e)&#123; e.printStackTrace();&#125;try&#123;hi();//递归造成StackOverflowError 这边因为每运行一个方法将创建一个栈帧，栈帧创建太多无法继续申请到内存扩展&#125;catch(StackOverflowError e)&#123; e.printStackTrace();&#125;&#125;publicstaticvoidhi()&#123;hi();&#125;&#125; 4.GC简介 GC(Garbage Collection)：即垃圾回收器，诞生于1960年MIT的Lisp语言，主要是用来回收，释放垃圾占用的空间。 java GC泛指java的垃圾回收机制，该机制是java与C/C++的主要区别之一，我们在日常写java代码的时候，一般都不需要编写内存回收或者垃圾清理的代码，也不需要像C/C++那样做类似delete/free的操作。 4.1.为什么需要学习GC 对象的内存分配在java虚拟机的自动内存分配机制下，一般不容易出现内存泄漏问题。但是写代码难免会遇到一些特殊情况，比如OOM神马的。。尽管虚拟机内存的动态分配与内存回收技术很成熟，可万一出现了这样那样的内存溢出问题，那么将难以定位错误的原因所在。 对于本人来说，由于水平有限，而且作为小开发，并没必要深入到GC的底层实现，但至少想要说学会看懂gc及定位一些内存泄漏问题。 从三个角度切入来学习GC 1.哪些内存要回收 2.什么时候回收 3.怎么回收 哪些内存要回收 java内存模型中分为五大区域已经有所了解。我们知道程序计数器、虚拟机栈、本地方法栈，由线程而生，随线程而灭，其中栈中的栈帧随着方法的进入顺序的执行的入栈和出栈的操作，一个栈帧需要分配多少内存取决于具体的虚拟机实现并且在编译期间即确定下来【忽略JIT编译器做的优化，基本当成编译期间可知】，当方法或线程执行完毕后，内存就随着回收，因此无需关心。 而Java堆、方法区则不一样。方法区存放着类加载信息，但是一个接口中多个实现类需要的内存可能不太一样，一个方法中多个分支需要的内存也可能不一样【只有在运行期间才可知道这个方法创建了哪些对象没需要多少内存】，这部分内存的分配和回收都是动态的，gc关注的也正是这部分的内存。 Java堆是GC回收的“重点区域”。堆中基本存放着所有对象实例，gc进行回收前，第一件事就是确认哪些对象存活，哪些死去[即不可能再被引用] 4.2 堆的回收区域为了高效的回收，jvm将堆分为三个区域 1.新生代（Young Generation）NewSize和MaxNewSize分别可以控制年轻代的初始大小和最大的大小 2.老年代（Old Generation） 3.永久代（Permanent Generation）【1.8以后采用元空间，就不在堆中了】 GC为什么要分代-R大的回答 关于元空间 5 判断对象是否存活算法1.引用计数算法 早期判断对象是否存活大多都是以这种算法，这种算法判断很简单，简单来说就是给对象添加一个引用计数器，每当对象被引用一次就加1，引用失效时就减1。当为0的时候就判断对象不会再被引用。 优点:实现简单效率高，被广泛使用与如python何游戏脚本语言上。 缺点:难以解决循环引用的问题，就是假如两个对象互相引用已经不会再被其它其它引用，导致一直不会为0就无法进行回收。 2.可达性分析算法 目前主流的商用语言[如java、c#]采用的是可达性分析算法判断对象是否存活。这个算法有效解决了循环利用的弊端。 它的基本思路是通过一个称为“GC Roots”的对象为起始点，搜索所经过的路径称为引用链，当一个对象到GC Roots没有任何引用跟它连接则证明对象是不可用的。 |-gc-| 可作为GC Roots的对象有四种 ①虚拟机栈(栈桢中的本地变量表)中的引用的对象，就是平时所指的java对象，存放在堆中。 ②方法区中的类静态属性引用的对象，一般指被static修饰引用的对象，加载类的时候就加载到内存中。 ③方法区中的常量引用的对象, ④本地方法栈中JNI（native方法)引用的对象 即使可达性算法中不可达的对象，也不是一定要马上被回收，还有可能被抢救一下。网上例子很多，基本上和深入理解JVM一书讲的一样对象的生存还是死亡 要真正宣告对象死亡需经过两个过程。 1.可达性分析后没有发现引用链 2.查看对象是否有finalize方法，如果有重写且在方法内完成自救[比如再建立引用]，还是可以抢救一下，注意这边一个类的finalize只执行一次，这就会出现一样的代码第一次自救成功第二次失败的情况。[如果类重写finalize且还没调用过，会将这个对象放到一个叫做F-Queue的序列里，这边finalize不承诺一定会执行，这么做是因为如果里面死循环的话可能会时F-Queue队列处于等待，严重会导致内存崩溃，这是我们不希望看到的。] HotSpot虚拟机如何实现可达性算法 5 垃圾收集算法 jvm中，可达性分析算法帮我们解决了哪些对象可以回收的问题，垃圾收集算法则关心怎么回收。 5.1 三大垃圾收集算法1.标记/清除算法【最基础】 2.复制算法 3.标记/整理算法 jvm采用`分代收集算法`对不同区域采用不同的回收算法。 参考GC算法深度解析 新生代采用复制算法 新生代中因为对象都是”朝生夕死的”，【深入理解JVM虚拟机上说98%的对象,不知道是不是这么多，总之就是存活率很低】，适用于复制算法【复制算法比较适合用于存活率低的内存区域】。它优化了标记/清除算法的效率和内存碎片问题，且JVM不以5:5分配内存【由于存活率低，不需要复制保留那么大的区域造成空间上的浪费，因此不需要按1:1【原有区域:保留空间】划分内存区域，而是将内存分为一块Eden空间和From Survivor、To Survivor【保留空间】，三者默认比例为8:1:1，优先使用Eden区，若Eden区满，则将对象复制到第二块内存区上。但是不能保证每次回收都只有不多于10%的对象存货，所以Survivor区不够的话，则会依赖老年代年存进行分配】。 GC开始时，对象只会存于Eden和From Survivor区域，To Survivor【保留空间】为空。 GC进行时，Eden区所有存活的对象都被复制到To Survivor区，而From Survivor区中，仍存活的对象会根据它们的年龄值决定去向，年龄值达到年龄阈值(默认15是因为对象头中年龄战4bit，新生代每熬过一次垃圾回收，年龄+1)，则移到老年代，没有达到则复制到To Survivor。 老年代采用标记/清除算法或标记/整理算法 由于老年代存活率高，没有额外空间给他做担保，必须使用这两种算法。 5.2 枚举根节点算法GC Roots 被虚拟机用来判断对象是否存活 可作为GC Roos的节点主要是在一些全局引用【如常量或静态属性】、执行上下文【如栈帧中本地变量表】中。那么如何在这么多全局变量和本地变量表找到【枚举】根节点将是个问题。 可达性分析算法需考虑 1.如果方法区几百兆，一个个检查里面的引用，将耗费大量资源。 2.在分析时，需保证这个对象引用关系不再变化，否则结果将不准确。【因此GC进行时需停掉其它所有java执行线程(Sun把这种行为称为‘Stop the World’)，即使是号称几乎不会停顿的CMS收集器，枚举根节点时也需停掉线程】 解决办法:实际上当系统停下来后JVM不需要一个个检查引用，而是通过OopMap数据结构【HotSpot的叫法】来标记对象引用。 虚拟机先得知哪些地方存放对象的引用，在类加载完时。HotSpot把对象内什么偏移量什么类型的数据算出来，在jit编译过程中，也会在特定位置记录下栈和寄存器哪些位置是引用，这样GC在扫描时就可以知道这些信息。【目前主流JVM使用准确式GC】 OopMap可以帮助HotSpot快速且准确完成GC Roots枚举以及确定相关信息。但是也存在一个问题，可能导致引用关系变化。 这个时候有个safepoint(安全点)的概念。 HotSpot中GC不是在任意位置都可以进入，而只能在safepoint处进入。 GC时对一个Java线程来说，它要么处在safepoint,要么不在safepoint。 safepoint不能太少，否则GC等待的时间会很久 safepoint不能太多，否则将增加运行GC的负担 安全点主要存放的位置 1:循环的末尾 2:方法临返回前/调用方法的call指令后 3:可能抛异常的位置 参考:关于安全点safepoint 6.垃圾收集器如果说垃圾回收算法是内存回收的方法论，那么垃圾收集器就是具体实现。jvm会结合针对不同的场景及用户的配置使用不同的收集器。 年轻代收集器 Serial、ParNew、Parallel Scavenge 老年代收集器 Serial Old、Parallel Old、CMS收集器 特殊收集器 G1收集器[新型，不在年轻、老年代范畴内] |-收集器，连线代表可结合使用-| 新生代收集器6.1 Serial最基本、发展最久的收集器，在jdk3以前是gc收集器的唯一选择，Serial是单线程收集器，Serial收集器只能使用一条线程进行收集工作，在收集的时候必须得停掉其它线程，等待收集工作完成其它线程才可以继续工作。 虽然Serial看起来很坑，需停掉别的线程以完成自己的gc工作，但是也不是完全没用的，比如说Serial在运行在Client模式下优于其它收集器[简单高效,不过一般都是用Server模式，64bit的jvm甚至没Client模式] JVM的Client模式与Server模式 优点:对于Client模式下的jvm来说是个好的选择。适用于单核CPU【现在基本都是多核了】 缺点:收集时要暂停其它线程，有点浪费资源，多核下显得。 6.2 ParNew收集器可以认为是Serial的升级版，因为它支持多线程[GC线程]，而且收集算法、Stop The World、回收策略和Serial一样，就是可以有多个GC线程并发运行，它是HotSpot第一个真正意义实现并发的收集器。默认开启线程数和当前cpu数量相同【几核就是几个，超线程cpu的话就不清楚了 - -】，如果cpu核数很多不想用那么多，可以通过*-XX:ParallelGCThreads*来控制垃圾收集线程的数量。 优点:1.支持多线程，多核CPU下可以充分的利用CPU资源 2.运行在Server模式下新生代首选的收集器【重点是因为新生代的这几个收集器只有它和Serial可以配合CMS收集器一起使用】 缺点: 在单核下表现不会比Serial好，由于在单核能利用多核的优势，在线程收集过程中可能会出现频繁上下文切换，导致额外的开销。 6.3 Parallel Scavenge采用复制算法的收集器，和ParNew一样支持多线程。 但是该收集器重点关心的是吞吐量【吞吐量 = 代码运行时间 / (代码运行时间 + 垃圾收集时间) 如果代码运行100min垃圾收集1min，则为99%】 对于用户界面，适合使用GC停顿时间短,不然因为卡顿导致交互界面卡顿将很影响用户体验。 对于后台 高吞吐量可以高效率的利用cpu尽快完成程序运算任务，适合后台运算 Parallel Scavenge注重吞吐量，所以也成为”吞吐量优先”收集器。 老年代收集器6.4 Serial Old和新生代的Serial一样为单线程，Serial的老年代版本，不过它采用”标记-整理算法”，这个模式主要是给Client模式下的JVM使用。 如果是Server模式有两大用途 1.jdk5前和Parallel Scavenge搭配使用，jdk5前也只有这个老年代收集器可以和它搭配。 2.作为CMS收集器的后备。 6.5 Parallel Old支持多线程，Parallel Scavenge的老年版本，jdk6开始出现， 采用”标记-整理算法”【老年代的收集器大都采用此算法】 在jdk6以前，新生代的Parallel Scavenge只能和Serial Old配合使用【根据图，没有这个的话只剩Serial Old，而Parallel Scavenge又不能和CMS配合使用】，而且Serial Old为单线程Server模式下会拖后腿【多核cpu下无法充分利用】，这种结合并不能让应用的吞吐量最大化。 Parallel Old的出现结合Parallel Scavenge，真正的形成“吞吐量优先”的收集器组合。 6.6 CMSCMS收集器(Concurrent Mark Sweep)是以一种获取最短回收停顿时间为目标的收集器。【重视响应，可以带来好的用户体验，被sun称为并发低停顿收集器】 启用CMS：-XX:+UseConcMarkSweepGC 正如其名，CMS采用的是”标记-清除”(Mark Sweep)算法，而且是支持并发(Concurrent)的 它的运作分为4个阶段 1.初始标记:标记一下GC Roots能直接关联到的对象，速度很快 2.并发标记:GC Roots Tarcing过程，即可达性分析 3.重新标记:为了修正因并发标记期间用户程序运作而产生变动的那一部分对象的标记记录，会有些许停顿，时间上一般 初始标记 &lt; 重新标记 &lt; 并发标记 4.并发清除 以上初始标记和重新标记需要stw(停掉其它运行java线程) 之所以说CMS的用户体验好，是因为CMS收集器的内存回收工作是可以和用户线程一起并发执行。 总体上CMS是款优秀的收集器，但是它也有些缺点。 1.cms堆cpu特别敏感，cms运行线程和应用程序并发执行需要多核cpu，如果cpu核数多的话可以发挥它并发执行的优势，但是cms默认配置启动的时候垃圾线程数为 (cpu数量+3)/4，它的性能很容易受cpu核数影响，当cpu的数目少的时候比如说为为2核，如果这个时候cpu运算压力比较大，还要分一半给cms运作，这可能会很大程度的影响到计算机性能。 2.cms无法处理浮动垃圾，可能导致Concurrent Mode Failure（并发模式故障）而触发full GC 3.由于cms是采用”标记-清除“算法,因此就会存在垃圾碎片的问题，为了解决这个问题cms提供了 -XX:+UseCMSCompactAtFullCollection选项，这个选项相当于一个开关【默认开启】，用于CMS顶不住要进行full GC时开启内存碎片合并，内存整理的过程是无法并发的，且开启这个选项会影响性能(比如停顿时间变长) 浮动垃圾:由于cms支持运行的时候用户线程也在运行，程序运行的时候会产生新的垃圾，这里产生的垃圾就是浮动垃圾，cms无法当次处理，得等下次才可以。 6.7 G1收集器G1(garbage first:尽可能多收垃圾，避免full gc)收集器是当前最为前沿的收集器之一(1.7以后才开始有)，同cms一样也是关注降低延迟，是用于替代cms功能更为强大的新型收集器，因为它解决了cms产生空间碎片等一系列缺陷。 摘自甲骨文:适用于 Java HotSpot VM 的低暂停、服务器风格的分代式垃圾回收器。G1 GC 使用并发和并行阶段实现其目标暂停时间，并保持良好的吞吐量。当 G1 GC 确定有必要进行垃圾回收时，它会先收集存活数据最少的区域（垃圾优先) g1的特别之处在于它强化了分区，弱化了分代的概念，是区域化、增量式的收集器，它不属于新生代也不属于老年代收集器。 用到的算法为标记-清理、复制算法 jdk1.7,1.8的都是默认关闭的，更高版本的还不知道 开启选项 -XX:+UseG1GC 比如在tomcat的catania.sh启动参数加上 g1是区域化的，它将java堆内存划分为若干个大小相同的区域【region】，jvm可以设置每个region的大小(1-32m,大小得看堆内存大小，必须是2的幂),它会根据当前的堆内存分配合理的region大小。 jdk7中计算region的源码,这边博主看了下也看不怎么懂，也翻了下openjdk8的看了下关于region的处理似乎不太一样。。 g1通过并发(并行)标记阶段查找老年代存活对象，通过并行复制压缩存活对象【这样可以省出连续空间供大对象使用】。 g1将一组或多组区域中存活对象以增量并行的方式复制到不同区域进行压缩，从而减少堆碎片，目标是尽可能多回收堆空间【垃圾优先】，且尽可能不超出暂停目标以达到低延迟的目的。 g1提供三种垃圾回收模式 young gc、mixed gc 和 full gc,不像其它的收集器，根据区域而不是分代，新生代老年代的对象它都能回收。 几个重要的默认值，更多的查看官方文档oracle官方g1中文文档 g1是自适应的回收器，提供了若干个默认值，无需修改就可高效运作 -XX:G1HeapRegionSize=n 设置g1 region大小，不设置的话自己会根据堆大小算，目标是根据最小堆内存划分2048个区域 -XX:MaxGCPauseMillis=200 最大停顿时间 默认200毫秒 7 Minor GC、Major GC、FULL GC、mixed gc7.1 Minor GC 在年轻代Young space(包括Eden区和Survivor区)中的垃圾回收称之为 Minor GC,Minor GC只会清理年轻代. 7.2 Major GC Major GC清理老年代(old GC)，但是通常也可以指和Full GC是等价，因为收集老年代的时候往往也会伴随着升级年轻代，收集整个Java堆。所以有人问的时候需问清楚它指的是full GC还是old GC。 7.3 Full GC full gc是对新生代、老年代、永久代【jdk1.8后没有这个概念了】统一的回收。 【知乎R大的回答:收集整个堆，包括young gen、old gen、perm gen（如果存在的话)、元空间(1.8及以上)等所有部分的模式】 7.4 mixed GC【g1特有】 混合GC 收集整个young gen以及部分old gen的GC。只有G1有这个模式 8 查看GC日志8.1 简单日志查看要看得懂并理解GC，需要看懂GC日志。 这边我在idea上试了个小例子，需要在idea配置参数(-XX:+PrintGCDetails)。 publicclassGCtest&#123;publicstaticvoidmain(String[] args)&#123;for(int i =0; i &lt;10000; i++)&#123;List&lt;String&gt; list =newArrayList&lt;&gt;(); list.add(&quot;aaaaaaaaaaaaa&quot;);&#125;System.gc();&#125;&#125; [GC (System.gc()) [PSYoungGen: 3998K-&gt;688K(38400K)] 3998K-&gt;696K(125952K), 0.0016551 secs[本次回收时间]] [Times: user=0.01 sys=0.00, real=0.00 secs] [Full GC (System.gc()) [PSYoungGen: 688K-&gt;0K(38400K)] [ParOldGen: 8K-&gt;603K(87552K)] 696K-&gt;603K(125952K), [Metaspace: 3210K-&gt;3210K(1056768K)], 0.0121034 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] Heap PSYoungGen[年轻代] total 38400K, used 333K [0x0000000795580000, 0x0000000798000000, 0x00000007c0000000) eden space 33280K, 1% used [0x0000000795580000,0x00000007955d34a8,0x0000000797600000) from space 5120K, 0% used [0x0000000797600000,0x0000000797600000,0x0000000797b00000) to space 5120K, 0% used [0x0000000797b00000,0x0000000797b00000,0x0000000798000000) ParOldGen[老年代] total 87552K, used 603K [0x0000000740000000, 0x0000000745580000, 0x0000000795580000) object space 87552K, 0% used [0x0000000740000000,0x0000000740096fe8,0x0000000745580000) Metaspace[元空间] used 3217K, capacity 4496K, committed 4864K, reserved 1056768K class space used 352K, capacity 388K, committed 512K, reserved 1048576K 8.2 离线工具查看比如sun的gchisto，gcviewer离线分析工具，做个笔记先了解下还没用过，可视化好像很好用的样子。 8.3 自带的jconsole工具、jstat命令终端输入jconsole就会出现jdk自带的gui监控工具 |-jconsole-| 可以根据内存使用情况间接了解内存使用和gc情况 |-jconsole-| jstat命令 比如jstat -gcutil pid查看对应java进程gc情况 |-jstat-| s0: 新生代survivor space0简称 就是准备复制的那块 单位为% s1:指新生代s1已使用百分比，为0的话说明没有存活对象到这边 e:新生代eden(伊甸园)区域(%)o:老年代(%)ygc:新生代 次数 ygct:minor gc耗时 fgct:full gc耗时(秒)GCT: ygct+fgct 耗时 几个疑问1.GC是怎么判断对象是被标记的通过枚举根节点的方式，通过jvm提供的一种oopMap的数据结构，简单来说就是不要再通过去遍历内存里的东西，而是通过OOPMap的数据结构去记录该记录的信息,比如说它可以不用去遍历整个栈，而是扫描栈上面引用的信息并记录下来。 总结:通过OOPMap把栈上代表引用的位置全部记录下来，避免全栈扫描，加快枚举根节点的速度，除此之外还有一个极为重要的作用，可以帮HotSpot实现准确式GC【这边的准确关键就是类型，可以根据给定位置的某块数据知道它的准确类型，HotSpot是通过oopMap外部记录下这些信息，存成映射表一样的东西】。 2.什么时候触发GC简单来说，触发的条件就是GC算法区域满了或将满了。 minor GC(young GC):当年轻代中eden区分配满的时候触发[值得一提的是因为young GC后部分存活的对象会已到老年代(比如对象熬过15轮)，所以过后old gen的占用量通常会变高] full GC: ①手动调用System.gc()方法 [增加了full GC频率，不建议使用而是让jvm自己管理内存，可以设置-XX:+ DisableExplicitGC来禁止RMI调用System.gc] ②发现perm gen（如果存在永久代的话)需分配空间但已经没有足够空间 ③老年代空间不足，比如说新生代的大对象大数组晋升到老年代就可能导致老年代空间不足。 ④CMS GC时出现Promotion Faield[pf] ⑤统计得到的Minor GC晋升到旧生代的平均大小大于老年代的剩余空间。 这个比较难理解，这是HotSpot为了避免由于新生代晋升到老年代导致老年代空间不足而触发的FUll GC。 比如程序第一次触发Minor GC后，有5m的对象晋升到老年代，姑且现在平均算5m，那么下次Minor GC发生时，先判断现在老年代剩余空间大小是否超过5m，如果小于5m，则HotSpot则会触发full GC(这点挺智能的) Promotion Faield:minor GC时 survivor space放不下[满了或对象太大]，对象只能放到老年代，而老年代也放不下会导致这个错误。 Concurrent Model Failure:cms时特有的错误，因为cms时垃圾清理和用户线程可以是并发执行的，如果在清理的过程中 可能原因： 1 cms触发太晚，可以把XX:CMSInitiatingOccupancyFraction调小[比如-XX:CMSInitiatingOccupancyFraction=70 是指设定CMS在对内存占用率达到70%的时候开始GC(因为CMS会有浮动垃圾,所以一般都较早启动GC)] 2 垃圾产生速度大于清理速度，可能是晋升阈值设置过小，Survivor空间小导致跑到老年代，eden区太小，存在大对象、数组对象等情况 3.空间碎片过多，可以开启空间碎片整理并合理设置周期时间 full gc导致了concurrent mode failure，而不是因为concurrent mode failure错误导致触发full gc，真正触发full gc的原因可能是ygc时发生的promotion failure。 3.cms收集器是否会扫描年轻代 会，在初始标记的时候会扫描新生代。 虽然cms是老年代收集器，但是我们知道年轻代的对象是可以晋升为老年代的，为了空间分配担保，还是有必要去扫描年轻代。 4.什么是空间分配担保在minor gc前，jvm会先检查老年代最大可用空间是否大于新生代所有对象总空间，如果是的话，则minor gc可以确保是安全的， 如果担保失败,会检查一个配置(HandlePromotionFailire),即是否允许担保失败。 如果允许:继续检查老年代最大可用可用的连续空间是否大于之前晋升的平均大小，比如说剩10m，之前每次都有9m左右的新生代到老年代，那么将尝试一次minor gc(大于的情况)，这会比较冒险。 如果不允许，而且还小于的情况，则会触发full gc。【为了避免经常full GC 该参数建议打开】 这边为什么说是冒险是因为minor gc过后如果出现大对象，由于新生代采用复制算法，survivor无法容纳将跑到老年代，所以才会去计算之前的平均值作为一种担保的条件与老年代剩余空间比较，这就是分配担保。 这种担保是动态概率的手段，但是也有可能出现之前平均都比较低，突然有一次minor gc对象变得很多远高于以往的平均值，这个时候就会导致担保失败【Handle Promotion Failure】，这就只好再失败后再触发一次FULL GC， 5.为什么复制算法要分两个Survivor，而不直接移到老年代这样做的话效率可能会更高，但是old区一般都是熬过多次可达性分析算法过后的存活的对象，要求比较苛刻且空间有限，而不能直接移过去，这将导致一系列问题(比如老年代容易被撑爆) 分两个Survivor(from/to)，自然是为了保证复制算法运行以提高效率。 6.各个版本的JVM使用的垃圾收集器是怎么样的准确来说，垃圾收集器的使用跟当前jvm也有很大的关系，比如说g1是jdk7以后的版本才开始出现。 并不是所有的垃圾收集器都是默认开启的，有些得通过设置相应的开关参数才会使用。比如说cms，需设置(XX:+UseConcMarkSweepGC) 这边有几个实用的命令，比如说server模式下 #UnlockExperimentalVMOptions UnlockDiagnosticVMOptions解锁获取jvm参数，PrintFlagsFinal用于输出xx相关参数，以Benchmark类测试，这边会有很多结果 大都看不懂- - 在这边查(usexxxxxxgc会看到jvm不同收集器的开关情况) java -server -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions -XX:+PrintFlagsFinal Benchmark #后面跟| grep &quot;:&quot;获取已赋值的参数[加:代表被赋值过] java -server -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions -XX:+PrintFlagsFinal Benchmark| grep &quot;:&quot; #获得用户自定义的设置或者jvm设置的详细的xx参数和值 java -server -XX:+PrintCommandLineFlags Benchmark 本人用的jdk8，这边UseParallelGC为true，参考深入理解jvm那本书说这个是Parallel Scavenge+Serial old搭配组合的开关，但是网上又说8默认是Parallel Scavenge+Parallel Old,我还是信书的吧 - -。 更多相关参数来源 |-常用参数-| 据说更高版本的jvm默认使用g1 7 stop the world具体是什么，有没有办法避免stop the world简单来说就是gc的时候，停掉除gc外的java线程。 无论什么gc都难以避免停顿，即使是g1也会在初始标记阶段发生，stw并不可怕，可以尽可能的减少停顿时间。 8 新生代什么样的情况会晋升为老年代对象优先分配在eden区，eden区满时会触发一次minor GC 对象晋升规则 1 长期存活的对象进入老年代，对象每熬过一次GC年龄+1(默认年龄阈值15，可配置)。 2 对象太大新生代无法容纳则会分配到老年代 3 eden区满了，进行minor gc后，eden和一个survivor区仍然存活的对象无法放到(to survivor区)则会通过分配担保机制放到老年代，这种情况一般是minor gc后新生代存活的对象太多。 4 动态年龄判定，为了使内存分配更灵活，jvm不一定要求对象年龄达到MaxTenuringThreshold(15)才晋升为老年代，若survior区相同年龄对象总大小大于survior区空间的一半，则大于等于这个年龄的对象将会在minor gc时移到老年代 8.怎么理解g1，适用于什么场景 G1 GC 是区域化、并行-并发、增量式垃圾回收器，相比其他 HotSpot 垃圾回收器，可提供更多可预测的暂停。增量的特性使 G1 GC 适用于更大的堆，在最坏的情况下仍能提供不错的响应。G1 GC 的自适应特性使 JVM 命令行只需要软实时暂停时间目标的最大值以及 Java 堆大小的最大值和最小值，即可开始工作。 g1不再区分老年代、年轻代这样的内存空间，这是较以往收集器很大的差异，所有的内存空间就是一块划分为不同子区域，每个区域大小为1m-32m，最多支持的内存为64g左右，且由于它为了的特性适用于大内存机器。 |-g1回收时堆内存情况-| 适用场景: 1.像cms能与应用程序并发执行，GC停顿短【短而且可控】，用户体验好的场景。 2.面向服务端，大内存，高cpu的应用机器。【网上说差不多是6g或更大】 3.应用在运行过程中经常会产生大量内存碎片，需要压缩空间【比cms好的地方之一，g1具备压缩功能】。 参考深入理解Java虚拟机 JVM内存模型、指令重排、内存屏障概念解析 Java对象头 GC收集器 Major GC和Full GC的区别 JVM 垃圾回收 Minor gc vs Major gc vs Full gc 关于准确式GC、保守式GC 关于CMS垃圾收集算法的一些疑惑 图解cms G1垃圾收集器介绍 详解cms回收机制 总结JMM 是一种规范，是解决由于多线程通过共享内存进行通信时，存在的本地内存数据不一致、编译器会对代码指令重排序、处理器会对代码乱序执行等带来的问题，而且写java代码的时候难免会经常和内存打交道，遇到各种内存溢出问题，有时候又难以定位问题，因此是一定要学习jmm以及GC的。 由于博主本人水平有限【目前还是小菜鸡】，所以花了点时间，写下这篇博客当做为笔记总结归纳，但是写博客这种事如果全都是照抄别人的成果就很没意思了，吸收别人的成果的同时，也希望自己有能力多写点自己独特的理解和干货后续继续更新，所以如果有哪里写的不好或写错请指出，以便我继续学习和改进。","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"jvm","slug":"jvm","permalink":"https://blog.fenxiangz.com/tags/jvm/"},{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://blog.fenxiangz.com/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"}]},{"title":"Netty 知识点思维导图","slug":"java/netty/2020-05-26_Netty 知识点思维导图","date":"2020-05-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.968Z","comments":true,"path":"post/java/netty/2020-05-26_Netty 知识点思维导图.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/netty/2020-05-26_Netty%20%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.html","excerpt":"","text":"右键新 Tab页打开查看大图","categories":[{"name":"Netty","slug":"Netty","permalink":"https://blog.fenxiangz.com/categories/Netty/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"思维导图","slug":"思维导图","permalink":"https://blog.fenxiangz.com/tags/%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/"},{"name":"Netty","slug":"Netty","permalink":"https://blog.fenxiangz.com/tags/Netty/"}]},{"title":"Netty 笔记汇总","slug":"java/netty/2020-05-24_Netty 笔记汇总","date":"2020-05-24T00:00:00.000Z","updated":"2020-12-20T16:47:02.968Z","comments":true,"path":"post/java/netty/2020-05-24_Netty 笔记汇总.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/netty/2020-05-24_Netty%20%E7%AC%94%E8%AE%B0%E6%B1%87%E6%80%BB.html","excerpt":"","text":"netty概要netty是一个异步非阻塞的通信框架（类似的框架还有mina，grizzly）。 底层封装了NIO2.0模块，即AIO。 好处：使用起来无需关注如何连接，只需关注业务逻辑处理代码，而且性能高效。遵循servlet3.0标准规范的，和tomcat等web容器不同的是异步非阻塞。tomcat容器支持最大并发量大概在800左右。而且netty也不算是web容器。 使用流程：服务端：配置端口，长连接还是短连接（针对业务，比如聊天一般用长连接，和客户端保持连接，短连接是针对是数据量大连接次数少的情况，比如一小时上传文件4到5次），最后要调用下异步关闭，否则服务端会关闭的，一般我们是不允许服务端关闭的。 应用场景：作为路由网关、文件上传和下载，消息通信，心跳检测等。 拆包粘包问题：设置定长（一次只能传输200K以下的数据，小于200k的后面以空格补全）、设置特殊分割符（编解码）、自定义协议（实现编解码的接口，进行扩展而已）。 编解码（序列化）：传输数据如果为对象，实体类必须要实现serialable接口，但java序列化码流大，传输慢，性能低。可以使用joss框架Marshalling+压缩解压缩这样的方式实现。 业务问题：1、若客户端和服务端为短连接，规定时间内（比如5秒）不传输数据默认断开，如何实现？调用readTimeoutHandler(5)。2、若客户端和服务端断开连接后，客户端再次发送请求，如何才能保证服务端接收到数据？在客户端初始化的时候，判断下客户端是否服务端保持连接，如果没有，重新连接下。 摘自：https://github.com/ouzhrm/netty_study/blob/master/README.md 组件 ByteBufChannel 和 UnsafeOioServerSocketChannel 和 OioSocketChannelNioServerSocketChannel 和 NioSocketChannelChannelPipeline 和 ChannelHandlerFuture 和 PromiseEventLoop 和 EventLoopGroupServerBootstrap 和 Bootstrap启动类 Encode 和 Decoder协议～","categories":[{"name":"Netty","slug":"Netty","permalink":"https://blog.fenxiangz.com/categories/Netty/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"Netty","slug":"Netty","permalink":"https://blog.fenxiangz.com/tags/Netty/"},{"name":"笔记","slug":"笔记","permalink":"https://blog.fenxiangz.com/tags/%E7%AC%94%E8%AE%B0/"}]},{"title":"Java 中单例模式DCL的缺陷及单例的正确写法","slug":"java/advance/2019-05-21_java中单例模式DCL的缺陷及单例的正确写法","date":"2020-05-21T00:00:00.000Z","updated":"2020-12-20T16:47:02.961Z","comments":true,"path":"post/java/advance/2019-05-21_java中单例模式DCL的缺陷及单例的正确写法.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2019-05-21_java%E4%B8%AD%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8FDCL%E7%9A%84%E7%BC%BA%E9%99%B7%E5%8F%8A%E5%8D%95%E4%BE%8B%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%86%99%E6%B3%95.html","excerpt":"","text":"首先在说明单例设计模式中的 DCL 问题之前我们首先看看实现单例设计模式的两种方式：饿汉式和懒汉式。 什么是饿汉式？饿汉式就是不管你是否用的上，一开始就先初始化对象（也叫做提前初始化） 代码示例： public class EagerInitialization&#123; private EagerInitialization() &#123;&#125; private static Resource resource = new Resource(); public static Resource getResource()&#123; return resource; &#125; &#125; 什么是懒汉式？懒汉式就是当你真正需要使用时才创建对象。 于是，关于懒汉式的问题也就随之产生了～～～ 我们先看一下有问题的代码： 代码示例： public class LazyInitialization&#123; private static Resource resource; public static Resource getResource()&#123; if (resource == null) resource = new Resource();//不安全！ return resource; &#125; &#125; 我们都知道上面的这个代码在单线程中运行是没有问题的，但是在平时的开发中常常会使用多线程，此时这个方法就会出现问题，假设有两个线程 A、B，当 A 线程满足判断还未来得及执行到 resource = new Resource() 时，线程执行资格被 B 拿走，此时线程 B 进入 getResource(), 而此时它也满足 resource 的值为 null, 于是导致最后产生两个实例。 针对上面的问题，于是有了相应的解决方案，即线程安全的延迟初始化，可以解决懒汉式出现的上述问题： 代码示例： public class LazyInitialization&#123; private static Resource resource; public synchronized static Resource getResource()&#123; if (resource == null) resource = new Resource(); return resource; &#125; &#125; 上面代码通过使用 synchronized 关键字将 getResource 变成同步函数来保证方法的原子性，从而保证了线程安全而防止最后多个线程产生多个实例的现象。 我们都知道，在上述例子当中，每次在调用 getResource() 时都需要进行同步，而且在大多数时这种同步是没有必要的，并且大量无用的同步会对性能造成极大的影响。为什么呢？因为在第一次调用 getResource() 方法时就已经创建了 resource 实例了，之后 resource 就不再为空，然而之后再调用 getResource 时都需要进行同步，从而对性能造成了很大的影响。基于这些问题，一个新的方法也就产生了，这也是我们需要着重讨论的一个方法——双重检查加锁 (Double Check Locking) DCL。 双重检查加锁 DCL (Double Check Locking)首先我们看看 DCL 的代码： 示例代码： public class DoubleCheckedLocking&#123; private static Resource resource; public static Resource getResource()&#123; if (resource == null) &#123; synchronized (DoubleCheckedLocking.class) &#123; if (resource == null) resource = new Resource(); &#125; &#125; return resource; &#125; &#125; 你可能会疑惑，这样做不是挺好么，这样就可以解决刚刚说的那些问题了么，当 resource 被实例化后再调用 getResource() 方法不就不会再进行同步，这样不就节约了资源，提升了性能么？ 说的对，DCL 确实存在着这些优点，但是与此同时，这个方法也会带来相应的问题，因为这个方法是含有缺陷的。再次之前，先了解一下JVM内存模型。 JVM内存模型JVM模型如下图： Thread Stack 是线程私有的区域。他是java方法执行时的字典：它里面记录了局部变量表、 操作数栈、 动态链接、 方法出口等信息。 在《java虚拟机规范》一书中对这部分的描述如下： 栈帧（ Frame）是用来存储数据和部分过程结果的数据结构，同时也被用来处理动态链接 (Dynamic Linking)、 方法返回值和异常分派（ Dispatch Exception）。 栈帧随着方法调用而创建，随着方法结束而销毁——无论方法是正常完成还是异常完成（抛出了在方法内未被捕获的异常）都算作方法结束。 栈帧的存储空间分配在 Java 虚拟机栈（ §2.5.5）之中，每一个栈帧都有自己的局部变量表（ Local Variables， §2.6.1）、操作数栈（ OperandStack， §2.6.2）和指向当前方法所属的类的运行时常量池（ §2.5.5）的引用。 Java 中某个线程在访问堆中的线程共享变量时，为了加快访问速度，提升效率，会把该变量临时拷贝一份到自己的 Thread Stack 中，并保持和堆中数据的同步。 缺陷首先我们看到，DCL 方法包含了层判断语句，第一层判断语句用于判断 resource 对象是否为空，也就是是否被实例化，如果为空时就进入同步代码块进一步判断，问题就出在了 resource 的实例化语句 resource = new Resource() 上，因为这个语句实际上不是原子性的。这句话可以大致分解为如下步骤： 1231. 给 Resource 的实例分配内存2. 初始化 Resource 构造器3. 将 resource 实例指向分配的内存空间，此时 resource 实例就不再为空 我们都希望这条语句的执行顺序是上述的 1——&gt;2——&gt;3，但是，由于 Java 编译器允许处理器乱序执行，以及 JDK1.5 之前 JMM（Java Memory Medel，即 Java 内存模型）中 Cache、寄存器到主内存回写顺序的规定，上面的第二点和第三点的顺序是无法保证的，也就是说，执行顺序可能是 1——&gt;2——&gt;3 也可能是 1——&gt;3——&gt;2。 如果有两个线程 A 和 B，如果 A 线程执行完 1 后先执行 3 然后执行 2，并且在 3 执行完毕、2 未执行之前，被切换到线程 B 上，这时候 resource 因为已经在线程 A 内执行过了第三点（jvm将未完成 Resource 构造器的值拷贝回堆中），resource 已经是非空了，所以线程 B 直接拿走 resource，然后使用，然后顺理成章地报错，而且这种难以跟踪难以重现的错误很可能会隐藏很久。 好了，关于 DCL 的问题阐述完了，那么这个方法既然有问题，那么该如何修改呢？ Happen-Before 原则通过遵守 Happen-Before 原则，解决并发顺序问题。 123451. 同一个线程中，书写在前面的操作happen-before书写在后面的操作。这条规则是说，在单线程中操作间happen-before关系完全是由源代码的顺序决定的，这里的前提“在同一个线程中”是很重要的，这条规则也称为单线程规则 。这个规则多少说得有些简单了，考虑到控制结构和循环结构，书写在后面的操作可能happen-before书写在前面的操作，不过我想读者应该明白我的意思。2. 对锁的unlock操作happen-before后续的对同一个锁的lock操作。这里的“后续”指的是时间上的先后关系，unlock操作发生在退出同步块之后，lock操作发生在进入同步块之前。这是条最关键性的规则，线程安全性主要依赖于这条规则。但是仅仅是这条规则仍然不起任何作用，它必须和下面这条规则联合起来使用才显得意义重大。这里关键条件是必须对“同一个锁”的lock和unlock。如果操作A happen-before操作B，操作B happen-before操作C，那么操作A happen-before操作C。这条规则也称为传递规。 通过 volatile 防止指令重排序在 JMM 的后续版本（Java 5.0 及以上）中，如果把 resource 声明为 volatile 类型，因为 volatile 可以防止指令的重排序（对 volatile 字段的写操作 happen-before 后续的对同一个字段的读操作），那么这样就可以启用 DCL，并且这种方式对性能的影响很小，因为 volatile 变量读取操作的性能通常只是略高于非 volatile 变量读取操作的性能。改进后的 DCL 方法如下代码所示 代码示例： public class DoubleCheckedLocking&#123; private static volatile Resource resource; public static Resource getResource&#123; if (resource == null) &#123; synchronized (DoubleCheckedLocking.class) &#123; if (resource == null) resource = new Resource(); &#125; &#125; return resource; &#125; &#125; 但是，DCL 的这种方法已经被广泛地遗弃了，因为促使该模式出现的驱动力（无竞争同步的执行速度很慢，以及 JVM 启动时很慢）已经不复存在，因为它不是一种高效的优化措施。延迟初始化占位类模式能带来同样的优势，并且更容易理解，延迟初始化占位类模式代码如下： 代码示例： public class ResourceFactory&#123; private static class ResourceHolder &#123; public static Resource resource = new Resource(); &#125; public static Resource getResource()&#123; return ResourceHolder.resource; &#125; &#125; 关于单例和 DCL 问题就分析到这里了，在实际开发当中由于经常要考虑到代码的效率和安全性，一般使用饿汉式和延长初始化占位类模式，而延迟占位类模式更是优势明显并且容易使用和理解，是良好的单例设计模式的实现方法。 参考资料： 《java 并发编程实战》 关于 volatile 的问题可以参考：http://blog.csdn.net/wxwzy738/article/details/43238089 关于 DCL 的其他问题可以参考：http://blog.csdn.net/ns_code/article/details/17359719https://blog.csdn.net/qiyei2009/article/details/71813069https://blog.csdn.net/u013393958/article/details/70941579","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"单例","slug":"单例","permalink":"https://blog.fenxiangz.com/tags/%E5%8D%95%E4%BE%8B/"},{"name":"DCL","slug":"DCL","permalink":"https://blog.fenxiangz.com/tags/DCL/"}]},{"title":"Spring Boot 排坑","slug":"java/spring/2020-05-20_SpringBoot排坑","date":"2020-05-20T00:00:00.000Z","updated":"2020-12-20T16:47:02.970Z","comments":true,"path":"post/java/spring/2020-05-20_SpringBoot排坑.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/spring/2020-05-20_SpringBoot%E6%8E%92%E5%9D%91.html","excerpt":"","text":"1 2.0.3 有内存暂用 不释放的问题，小版本升级到2.0.5就可以解决； 2SpringBoot 2.2.x 版本有CPU增高的bug；2.2.1-2.2.5 版本是会造成频繁的拿锁与解锁 ；2.2.6 版本 cpu会持续增高建议选择： 2.1 版本； 3SpringBoot 自动依赖 mysql-connector-java 5.1.47 (有坑)，改成强制依赖 5.1.46解决；相关问题：https://bugs.mysql.com/bug.php?id=92089https://github.com/spring-projects/spring-boot/issues/14638","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://blog.fenxiangz.com/categories/Spring-Boot/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://blog.fenxiangz.com/tags/Spring/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://blog.fenxiangz.com/tags/Spring-Boot/"}]},{"title":"请给Sprint Boot多一些内存","slug":"java/spring/2020-05-20_请给SprintBoot多一些内存","date":"2020-05-20T00:00:00.000Z","updated":"2020-12-20T16:47:02.971Z","comments":true,"path":"post/java/spring/2020-05-20_请给SprintBoot多一些内存.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/spring/2020-05-20_%E8%AF%B7%E7%BB%99SprintBoot%E5%A4%9A%E4%B8%80%E4%BA%9B%E5%86%85%E5%AD%98.html","excerpt":"","text":"概述SprintBoot总体来说，搭建还是比较容易的，特别是SpringCloud全家桶，简称亲民微服务，但在发展趋势中，容器化技术已经成熟，面对巨耗内存的SprintBoot，小公司表示用不起。如今，很多刚诞生的JAVA微服务框架大多主打“轻量级”，主要还是因为SprintBoot太重。 JAVA系微服务框架No1-Spring Cloud介绍有Spring大靠山在，更新、稳定性、成熟度的问题根本不需要考虑。在JAVA系混的技术人员大约都听说过Spring的大名吧，所以不缺程序员……，而且这入手的难度十分低，完全可以省去一个架构师。但是，你必然在服务器上付出： 至少一台“服务发现 ”的服务器； 可能有一个统一的网关Gateway; 可能需要一个用于“分布式配置管理”的配置中心； 可能进行“服务追踪”，知道我的请求从哪里来，到哪里去； 可能需要“集群监控”；项目上线后发现，我们需要好多服务器，每次在集群中增加服务器时，都感觉心疼； 压测30秒压测前的内存占用如图，内存占用304M。 压测时的内存占用如图，内存占用1520M（1.5G），CPU上升到321% 概览 总结一个SprintBoot的简单应用，最少1G内存，一个业务点比较少的微服务编译后的JAR会大约50M；而SprintCloud引入的组件会相对多一些，消耗的资源也会相对更多一些。 启动时间大约10秒左右: Started Application in 10.153 seconds (JVM running for 10.915) JAVA系响应式编程的工具包Vert.x介绍背靠Eclipse的Eclipse Vert.x是一个用于在JVM上构建响应式应用程序的工具包。定位上与SprintBoot不冲突，甚至可以将Vert.x结合SprintBoot使用。众多Vert.x模块提供了大量微服务的组件，在很多人眼里是一种微服务架构的选择。 华为微服务框架Apache ServiceComb就是以Vert.x为底层框架实现的，在”基准测试网站TechEmpower”中，Vert.x的表现也十分亮眼。 压测30秒压测前的内存占用如图，内存占用65M。 压测时的内存占用如图，内存占139M，CPU占2.1%，给人的感觉似乎并没有进行压测。 概览 总结Vert.x单个服务打包完成后大约7M左右的JAR，不依赖Tomcat、Jetty之类的容器，直接在JVM上跑。 Vert.x消耗的资源很低，感觉一个1核2G的服务器已经能够部署许多个Vert.x服务。除去编码方面的问题，真心符合小项目和小模块。git市场上已经出现了基于Vert.x实现的开源网关- VX-API-Gateway帮助文档对多语言支持，很适合小型项目快速上线。 启动时间不到1秒：Started Vert.x in 0.274 seconds (JVM running for 0.274) JAVA系其他微服务框架SparkJava jar比较小，大约10M 占内存小，大约30~60MB； 性能还可以，与SprintBoot相仿； Micronaut Grails团队新宠； 可以用 Java、Groovy 和 Kotlin 编写的基于微服务的应用程序； 相比SprintBoot已经比较全面； 性能较优，编码方式与SprintBoot比较类似； 启动时间和内存消耗方面比其他框架更高效； 多语言； 依赖注入； 内置多种云本地功能； 很新，刚发布1.0.0 Javalin 上手极为容易； 灵活，可以兼容同步和异步两种编程思路； JAR小，4～5M； 多语言； 有KOA的影子； 只有大约2000行源代码，源代码足够简单，可以理解和修复； 符合当今趋势； 多语言； 嵌入式服务器Jetty； Quarkus 启动快； JAR小，大约10M； 文档很少； 原文：https://juejin.im/post/5c89f266f265da2d8763b5f9","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://blog.fenxiangz.com/categories/Spring-Boot/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://blog.fenxiangz.com/tags/Spring/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://blog.fenxiangz.com/tags/Spring-Boot/"}]},{"title":"Spring IOC 容器源码分析","slug":"java/spring/2020-11-20_SpringIOC源码分析","date":"2020-05-20T00:00:00.000Z","updated":"2020-12-20T16:47:02.974Z","comments":true,"path":"post/java/spring/2020-11-20_SpringIOC源码分析.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/spring/2020-11-20_SpringIOC%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.html","excerpt":"","text":"Spring 最重要的概念是 IOC 和 AOP，本篇文章其实就是要带领大家来分析下 Spring 的 IOC 容器。既然大家平时都要用到 Spring，怎么可以不好好了解 Spring 呢？阅读本文并不能让你成为 Spring 专家，不过一定有助于大家理解 Spring 的很多概念，帮助大家排查应用中和 Spring 相关的一些问题。 本文采用的源码版本是 4.3.11.RELEASE，算是 5.0.x 前比较新的版本了。为了降低难度，本文所说的所有的内容都是基于 xml 的配置的方式，实际使用已经很少人这么做了，至少不是纯 xml 配置，不过从理解源码的角度来看用这种方式来说无疑是最合适的。 阅读建议：读者至少需要知道怎么配置 Spring，了解 Spring 中的各种概念，少部分内容我还假设读者使用过 SpringMVC。本文要说的 IOC 总体来说有两处地方最重要，一个是创建 Bean 容器，一个是初始化 Bean，如果读者觉得一次性看完本文压力有点大，那么可以按这个思路分两次消化。读者不一定对 Spring 容器的源码感兴趣，也许附录部分介绍的知识对读者有些许作用。 希望通过本文可以让读者不惧怕阅读 Spring 源码，也希望大家能反馈表述错误或不合理的地方。 引言先看下最基本的启动 Spring 容器的例子： public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;classpath:applicationfile.xml&quot;); &#125; 以上代码就可以利用配置文件来启动一个 Spring 容器了，请使用 maven 的小伙伴直接在 dependencies 中加上以下依赖即可，个人比较反对那些不知道要添加什么依赖，然后把 Spring 的所有相关的东西都加进来的方式。 &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.3.11.RELEASE&lt;/version&gt; &lt;/dependency&gt; spring-context 会自动将 spring-core、spring-beans、spring-aop、spring-expression 这几个基础 jar 包带进来。 多说一句，很多开发者入门就直接接触的 SpringMVC，对 Spring 其实不是很了解，Spring 是渐进式的工具，并不具有很强的侵入性，它的模块也划分得很合理，即使你的应用不是 web 应用，或者之前完全没有使用到 Spring，而你就想用 Spring 的依赖注入这个功能，其实完全是可以的，它的引入不会对其他的组件产生冲突。 废话说完，我们继续。ApplicationContext context = new ClassPathXmlApplicationContext(...) 其实很好理解，从名字上就可以猜出一二，就是在 ClassPath 中寻找 xml 配置文件，根据 xml 文件内容来构建 ApplicationContext。当然，除了 ClassPathXmlApplicationContext 以外，我们也还有其他构建 ApplicationContext 的方案可供选择，我们先来看看大体的继承结构是怎么样的： 读者可以大致看一下类名，源码分析的时候不至于找不着看哪个类，因为 Spring 为了适应各种使用场景，提供的各个接口都可能有很多的实现类。对于我们来说，就是揪着一个完整的分支看完。 当然，读本文的时候读者也不必太担心，每个代码块分析的时候，我都会告诉读者我们在说哪个类第几行。 我们可以看到，ClassPathXmlApplicationContext 兜兜转转了好久才到 ApplicationContext 接口，同样的，我们也可以使用绿颜色的 FileSystemXmlApplicationContext 和 AnnotationConfigApplicationContext 这两个类。 FileSystemXmlApplicationContext 的构造函数需要一个 xml 配置文件在系统中的路径，其他和 ClassPathXmlApplicationContext 基本上一样。 AnnotationConfigApplicationContext 是基于注解来使用的，它不需要配置文件，采用 java 配置类和各种注解来配置，是比较简单的方式，也是大势所趋吧。 不过本文旨在帮助大家理解整个构建流程，所以决定使用 ClassPathXmlApplicationContext 进行分析。 我们先来一个简单的例子来看看怎么实例化 ApplicationContext。 首先，定义一个接口： public interface MessageService &#123; String getMessage(); &#125; 定义接口实现类： public class MessageServiceImpl implements MessageService &#123; public String getMessage() &#123; return&quot;hello world&quot;; &#125; &#125; 接下来，我们在 resources 目录新建一个配置文件，文件名随意，通常叫 application.xml 或 application-xxx.xml 就可以了： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot; default-autowire=&quot;byName&quot;&gt; &lt;bean id=&quot;messageService&quot; class=&quot;com.javadoop.example.MessageServiceImpl&quot;/&gt; &lt;/beans&gt; 这样，我们就可以跑起来了： public class App &#123; public static void main(String[] args) &#123; // 用我们的配置文件来启动一个 ApplicationContext ApplicationContext context = new ClassPathXmlApplicationContext(&quot;classpath:application.xml&quot;); System.out.println(&quot;context 启动成功&quot;); // 从 context 中取出我们的 Bean，而不是用 new MessageServiceImpl() 这种方式 MessageService messageService = context.getBean(MessageService.class); // 这句将输出: hello world System.out.println(messageService.getMessage()); &#125; &#125; 以上例子很简单，不过也够引出本文的主题了，就是怎么样通过配置文件来启动 Spring 的 ApplicationContext？也就是我们今天要分析的 IOC 的核心了。ApplicationContext 启动过程中，会负责创建实例 Bean，往各个 Bean 中注入依赖等。 BeanFactory 简介BeanFactory，从名字上也很好理解，生产 bean 的工厂，它负责生产和管理各个 bean 实例。 初学者可别以为我之前说那么多和 BeanFactory 无关，前面说的 ApplicationContext 其实就是一个 BeanFactory。我们来看下和 BeanFactory 接口相关的主要的继承结构： 我想，大家看完这个图以后，可能就不是很开心了。ApplicationContext 往下的继承结构前面一张图说过了，这里就不重复了。这张图呢，背下来肯定是不需要的，有几个重点和大家说明下就好。 ApplicationContext 继承了 ListableBeanFactory，这个 Listable 的意思就是，通过这个接口，我们可以获取多个 Bean，大家看源码会发现，最顶层 BeanFactory 接口的方法都是获取单个 Bean 的。 ApplicationContext 继承了 HierarchicalBeanFactory，Hierarchical 单词本身已经能说明问题了，也就是说我们可以在应用中起多个 BeanFactory，然后可以将各个 BeanFactory 设置为父子关系。 AutowireCapableBeanFactory 这个名字中的 Autowire 大家都非常熟悉，它就是用来自动装配 Bean 用的，但是仔细看上图，ApplicationContext 并没有继承它，不过不用担心，不使用继承，不代表不可以使用组合，如果你看到 ApplicationContext 接口定义中的最后一个方法 getAutowireCapableBeanFactory() 就知道了。 ConfigurableListableBeanFactory 也是一个特殊的接口，看图，特殊之处在于它继承了第二层所有的三个接口，而 ApplicationContext 没有。这点之后会用到。 请先不用花时间在其他的接口和类上，先理解我说的这几点就可以了。 然后，请读者打开编辑器，翻一下 BeanFactory、ListableBeanFactory、HierarchicalBeanFactory、AutowireCapableBeanFactory、ApplicationContext 这几个接口的代码，大概看一下各个接口中的方法，大家心里要有底，限于篇幅，我就不贴代码介绍了。 启动过程分析下面将会是冗长的代码分析，记住，一定要自己打开源码来看，不然纯看是很累的。 第一步，我们肯定要从 ClassPathXmlApplicationContext 的构造方法说起。 public class ClassPathXmlApplicationContext extends AbstractXmlApplicationContext &#123; private Resource[] configResources; // 如果已经有 ApplicationContext 并需要配置成父子关系，那么调用这个构造方法 public ClassPathXmlApplicationContext(ApplicationContext parent) &#123; super(parent); &#125; ... public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); // 根据提供的路径，处理成配置文件数组(以分号、逗号、空格、tab、换行符分割) setConfigLocations(configLocations); if (refresh) &#123; refresh(); // 核心方法 &#125; &#125; ... &#125; 接下来，就是 refresh()，这里简单说下为什么是 refresh()，而不是 init() 这种名字的方法。因为 ApplicationContext 建立起来以后，其实我们是可以通过调用 refresh() 这个方法重建的，refresh() 会将原来的 ApplicationContext 销毁，然后再重新执行一次初始化操作。 往下看，refresh() 方法里面调用了那么多方法，就知道肯定不简单了，请读者先看个大概，细节之后会详细说。 @Override public void refresh() throws BeansException, IllegalStateException &#123; // 来个锁，不然 refresh() 还没结束，你又来个启动或销毁容器的操作，那不就乱套了嘛 synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录下容器的启动时间、标记“已启动”状态、处理配置文件中的占位符 prepareRefresh(); // 这步比较关键，这步完成后，配置文件就会解析成一个个 Bean 定义，注册到 BeanFactory 中， // 当然，这里说的 Bean 还没有初始化，只是配置信息都提取出来了， // 注册也只是将这些信息都保存到了注册中心(说到底核心是一个 beanName-&gt; beanDefinition 的 map) ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean // 这块待会会展开说 prepareBeanFactory(beanFactory); try &#123; // 【这里需要知道 BeanFactoryPostProcessor 这个知识点，Bean 如果实现了此接口， // 那么在容器初始化以后，Spring 会负责调用里面的 postProcessBeanFactory 方法。】 // 这里是提供给子类的扩展点，到这里的时候，所有的 Bean 都加载、注册完成了，但是都还没有初始化 // 具体的子类可以在这步的时候添加一些特殊的 BeanFactoryPostProcessor 的实现类或做点什么事 postProcessBeanFactory(beanFactory); // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类，注意看和 BeanFactoryPostProcessor 的区别 // 此接口两个方法: postProcessBeforeInitialization 和 postProcessAfterInitialization // 两个方法分别在 Bean 初始化之前和初始化之后得到执行。注意，到这里 Bean 还没初始化 registerBeanPostProcessors(beanFactory); // 初始化当前 ApplicationContext 的 MessageSource，国际化这里就不展开说了，不然没完没了了 initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器，这里也不展开了 initApplicationEventMulticaster(); // 从方法名就可以知道，典型的模板方法(钩子方法)， // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前） onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口。这也不是我们的重点，过 registerListeners(); // 重点，重点，重点 // 初始化所有的 singleton beans //（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 最后，广播事件，ApplicationContext 初始化完成 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); &#125; // Destroy already created singletons to avoid dangling resources. // 销毁已经初始化的 singleton 的 Beans，以免有些 bean 会一直占用资源 destroyBeans(); // Reset &#39;active&#39; flag. cancelRefresh(ex); // 把异常往外抛 throw ex; &#125; finally &#123; // Reset common introspection caches in Spring&#39;s core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125; &#125; 下面，我们开始一步步来肢解这个 refresh() 方法。 创建 Bean 容器前的准备工作这个比较简单，直接看代码中的几个注释即可。 protected void prepareRefresh() &#123; // 记录启动时间， // 将 active 属性设置为 true，closed 属性设置为 false，它们都是 AtomicBoolean 类型 this.startupDate = System.currentTimeMillis(); this.closed.set(false); this.active.set(true); if (logger.isInfoEnabled()) &#123; logger.info(&quot;Refreshing &quot; + this); &#125; // Initialize any placeholder property sources in the context environment initPropertySources(); // 校验 xml 配置文件 getEnvironment().validateRequiredProperties(); this.earlyApplicationEvents = new LinkedHashSet&lt;ApplicationEvent&gt;(); &#125; 创建 Bean 容器，加载并注册 Bean我们回到 refresh() 方法中的下一行 obtainFreshBeanFactory()。 注意，这个方法是全文最重要的部分之一，这里将会初始化 BeanFactory、加载 Bean、注册 Bean 等等。 当然，这步结束后，Bean 并没有完成初始化。这里指的是 Bean 实例并未在这一步生成。 // AbstractApplicationContext.java protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; // 关闭旧的 BeanFactory (如果有)，创建新的 BeanFactory，加载 Bean 定义、注册 Bean 等等 refreshBeanFactory(); // 返回刚刚创建的 BeanFactory ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Bean factory for &quot; + getDisplayName() + &quot;: &quot; + beanFactory); &#125; return beanFactory; &#125; // AbstractRefreshableApplicationContext.java 120 @Override protected final void refreshBeanFactory() throws BeansException &#123; // 如果 ApplicationContext 中已经加载过 BeanFactory 了，销毁所有 Bean，关闭 BeanFactory // 注意，应用中 BeanFactory 本来就是可以多个的，这里可不是说应用全局是否有 BeanFactory，而是当前 // ApplicationContext 是否有 BeanFactory if (hasBeanFactory()) &#123; destroyBeans(); closeBeanFactory(); &#125; try &#123; // 初始化一个 DefaultListableBeanFactory，为什么用这个，我们马上说。 DefaultListableBeanFactory beanFactory = createBeanFactory(); // 用于 BeanFactory 的序列化，我想不部分人应该都用不到 beanFactory.setSerializationId(getId()); // 下面这两个方法很重要，别跟丢了，具体细节之后说 // 设置 BeanFactory 的两个配置属性：是否允许 Bean 覆盖、是否允许循环引用 customizeBeanFactory(beanFactory); // 加载 Bean 到 BeanFactory 中 loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException ex) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + getDisplayName(), ex); &#125; &#125; 看到这里的时候，我觉得读者就应该站在高处看 ApplicationContext 了，ApplicationContext 继承自 BeanFactory，但是它不应该被理解为 BeanFactory 的实现类，而是说其内部持有一个实例化的 BeanFactory（DefaultListableBeanFactory）。以后所有的 BeanFactory 相关的操作其实是委托给这个实例来处理的。 我们说说为什么选择实例化 DefaultListableBeanFactory ？前面我们说了有个很重要的接口 ConfigurableListableBeanFactory，它实现了 BeanFactory 下面一层的所有三个接口，我把之前的继承图再拿过来大家再仔细看一下： 我们可以看到 ConfigurableListableBeanFactory 只有一个实现类 DefaultListableBeanFactory，而且实现类 DefaultListableBeanFactory 还通过实现右边的 AbstractAutowireCapableBeanFactory 通吃了右路。所以结论就是，最底下这个家伙 DefaultListableBeanFactory 基本上是最牛的 BeanFactory 了，这也是为什么这边会使用这个类来实例化的原因。 如果你想要在程序运行的时候动态往 Spring IOC 容器注册新的 bean，就会使用到这个类。那我们怎么在运行时获得这个实例呢？ 之前我们说过 ApplicationContext 接口能获取到 AutowireCapableBeanFactory，就是最右上角那个，然后它向下转型就能得到 DefaultListableBeanFactory 了。 在继续往下之前，我们需要先了解 BeanDefinition。我们说 BeanFactory 是 Bean 容器，那么 Bean 又是什么呢？ 这里的 BeanDefinition 就是我们所说的 Spring 的 Bean，我们自己定义的各个 Bean 其实会转换成一个个 BeanDefinition 存在于 Spring 的 BeanFactory 中。 所以，如果有人问你 Bean 是什么的时候，你要知道 Bean 在代码层面上可以认为是 BeanDefinition 的实例。 BeanDefinition 中保存了我们的 Bean 信息，比如这个 Bean 指向的是哪个类、是否是单例的、是否懒加载、这个 Bean 依赖了哪些 Bean 等等。 BeanDefinition 接口定义 我们来看下 BeanDefinition 的接口定义： public interface BeanDefinition extends AttributeAccessor, BeanMetadataElement &#123; // 我们可以看到，默认只提供 sington 和 prototype 两种， // 很多读者可能知道还有 request, session, globalSession, application, websocket 这几种， // 不过，它们属于基于 web 的扩展。 String SCOPE_SINGLETON = ConfigurableBeanFactory.SCOPE_SINGLETON; String SCOPE_PROTOTYPE = ConfigurableBeanFactory.SCOPE_PROTOTYPE; // 比较不重要，直接跳过吧 int ROLE_APPLICATION = 0; int ROLE_SUPPORT = 1; int ROLE_INFRASTRUCTURE = 2; // 设置父 Bean，这里涉及到 bean 继承，不是 java 继承。请参见附录的详细介绍 // 一句话就是：继承父 Bean 的配置信息而已 void setParentName(String parentName); // 获取父 Bean String getParentName(); // 设置 Bean 的类名称，将来是要通过反射来生成实例的 void setBeanClassName(String beanClassName); // 获取 Bean 的类名称 String getBeanClassName(); // 设置 bean 的 scope void setScope(String scope); String getScope(); // 设置是否懒加载 void setLazyInit(boolean lazyInit); boolean isLazyInit(); // 设置该 Bean 依赖的所有的 Bean，注意，这里的依赖不是指属性依赖(如 @Autowire 标记的)， // 是 depends-on=&quot;&quot; 属性设置的值。 void setDependsOn(String... dependsOn); // 返回该 Bean 的所有依赖 String[] getDependsOn(); // 设置该 Bean 是否可以注入到其他 Bean 中，只对根据类型注入有效， // 如果根据名称注入，即使这边设置了 false，也是可以的 void setAutowireCandidate(boolean autowireCandidate); // 该 Bean 是否可以注入到其他 Bean 中 boolean isAutowireCandidate(); // 主要的。同一接口的多个实现，如果不指定名字的话，Spring 会优先选择设置 primary 为 true 的 bean void setPrimary(boolean primary); // 是否是 primary 的 boolean isPrimary(); // 如果该 Bean 采用工厂方法生成，指定工厂名称。对工厂不熟悉的读者，请参加附录 // 一句话就是：有些实例不是用反射生成的，而是用工厂模式生成的 void setFactoryBeanName(String factoryBeanName); // 获取工厂名称 String getFactoryBeanName(); // 指定工厂类中的 工厂方法名称 void setFactoryMethodName(String factoryMethodName); // 获取工厂类中的 工厂方法名称 String getFactoryMethodName(); // 构造器参数 ConstructorArgumentValues getConstructorArgumentValues(); // Bean 中的属性值，后面给 bean 注入属性值的时候会说到 MutablePropertyValues getPropertyValues(); // 是否 singleton boolean isSingleton(); // 是否 prototype boolean isPrototype(); // 如果这个 Bean 是被设置为 abstract，那么不能实例化， // 常用于作为 父bean 用于继承，其实也很少用...... boolean isAbstract(); int getRole(); String getDescription(); String getResourceDescription(); BeanDefinition getOriginatingBeanDefinition(); &#125; 这个 BeanDefinition 其实已经包含很多的信息了，暂时不清楚所有的方法对应什么东西没关系，希望看完本文后读者可以彻底搞清楚里面的所有东西。 这里接口虽然那么多，但是没有类似 getInstance() 这种方法来获取我们定义的类的实例，真正的我们定义的类生成的实例到哪里去了呢？别着急，这个要很后面才能讲到。 有了 BeanDefinition 的概念以后，我们再往下看 refreshBeanFactory() 方法中的剩余部分： customizeBeanFactory(beanFactory); loadBeanDefinitions(beanFactory); 虽然只有两个方法，但路还很长啊。。。 customizeBeanFactory customizeBeanFactory(beanFactory) 比较简单，就是配置是否允许 BeanDefinition 覆盖、是否允许循环引用。 protected void customizeBeanFactory(DefaultListableBeanFactory beanFactory) &#123; if (this.allowBeanDefinitionOverriding != null) &#123; // 是否允许 Bean 定义覆盖 beanFactory.setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); &#125; if (this.allowCircularReferences != null) &#123; // 是否允许 Bean 间的循环依赖 beanFactory.setAllowCircularReferences(this.allowCircularReferences); &#125; &#125; BeanDefinition 的覆盖问题可能会有开发者碰到这个坑，就是在配置文件中定义 bean 时使用了相同的 id 或 name，默认情况下，allowBeanDefinitionOverriding 属性为 null，如果在同一配置文件中重复了，会抛错，但是如果不是同一配置文件中，会发生覆盖。 循环引用也很好理解：A 依赖 B，而 B 依赖 A。或 A 依赖 B，B 依赖 C，而 C 依赖 A。 默认情况下，Spring 允许循环依赖，当然如果你在 A 的构造方法中依赖 B，在 B 的构造方法中依赖 A 是不行的。 至于这两个属性怎么配置？我在附录中进行了介绍，尤其对于覆盖问题，很多人都希望禁止出现 Bean 覆盖，可是 Spring 默认是不同文件的时候可以覆盖的。 之后的源码中还会出现这两个属性，读者有个印象就可以了。 加载 Bean: loadBeanDefinitions 接下来是最重要的 loadBeanDefinitions(beanFactory) 方法了，这个方法将根据配置，加载各个 Bean，然后放到 BeanFactory 中。 读取配置的操作在 XmlBeanDefinitionReader 中，其负责加载配置、解析。 // AbstractXmlApplicationContext.java 80 /** 我们可以看到，此方法将通过一个 XmlBeanDefinitionReader 实例来加载各个 Bean。*/ @Override protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // 给这个 BeanFactory 实例化一个 XmlBeanDefinitionReader XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // Configure the bean definition reader with this context&#39;s // resource loading environment. beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // 初始化 BeanDefinitionReader，其实这个是提供给子类覆写的， // 我看了一下，没有类覆写这个方法，我们姑且当做不重要吧 initBeanDefinitionReader(beanDefinitionReader); // 重点来了，继续往下 loadBeanDefinitions(beanDefinitionReader); &#125; 现在还在这个类中，接下来用刚刚初始化的 Reader 开始来加载 xml 配置，这块代码读者可以选择性跳过，不是很重要。也就是说，下面这个代码块，读者可以很轻松地略过。 // AbstractXmlApplicationContext.java 120 protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; Resource[] configResources = getConfigResources(); if (configResources != null) &#123; // 往下看 reader.loadBeanDefinitions(configResources); &#125; String[] configLocations = getConfigLocations(); if (configLocations != null) &#123; // 2 reader.loadBeanDefinitions(configLocations); &#125; &#125; // 上面虽然有两个分支，不过第二个分支很快通过解析路径转换为 Resource 以后也会进到这里 @Override public int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException &#123; Assert.notNull(resources, &quot;Resource array must not be null&quot;); int counter = 0; // 注意这里是个 for 循环，也就是每个文件是一个 resource for (Resource resource : resources) &#123; // 继续往下看 counter += loadBeanDefinitions(resource); &#125; // 最后返回 counter，表示总共加载了多少的 BeanDefinition return counter; &#125; // XmlBeanDefinitionReader 303 @Override public int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException &#123; return loadBeanDefinitions(new EncodedResource(resource)); &#125; // XmlBeanDefinitionReader 314 public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException &#123; Assert.notNull(encodedResource, &quot;EncodedResource must not be null&quot;); if (logger.isInfoEnabled()) &#123; logger.info(&quot;Loading XML bean definitions from &quot; + encodedResource.getResource()); &#125; // 用一个 ThreadLocal 来存放配置文件资源 Set&lt;EncodedResource&gt; currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (currentResources == null) &#123; currentResources = new HashSet&lt;EncodedResource&gt;(4); this.resourcesCurrentlyBeingLoaded.set(currentResources); &#125; if (!currentResources.add(encodedResource)) &#123; throw new BeanDefinitionStoreException( &quot;Detected cyclic loading of &quot; + encodedResource + &quot; - check your import definitions!&quot;); &#125; try &#123; InputStream inputStream = encodedResource.getResource().getInputStream(); try &#123; InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; // 核心部分是这里，往下面看 returndoLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; finally &#123; inputStream.close(); &#125; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( &quot;IOException parsing XML document from &quot; + encodedResource.getResource(), ex); &#125; finally &#123; currentResources.remove(encodedResource); if (currentResources.isEmpty()) &#123; this.resourcesCurrentlyBeingLoaded.remove(); &#125; &#125; &#125; // 还在这个文件中，第 388 行 protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException &#123; try &#123; // 这里就不看了，将 xml 文件转换为 Document 对象 Document doc = doLoadDocument(inputSource, resource); // 继续 return registerBeanDefinitions(doc, resource); &#125; catch (... &#125; // 还在这个文件中，第 505 行 // 返回值：返回从当前配置文件加载了多少数量的 Bean public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); int countBefore = getRegistry().getBeanDefinitionCount(); // 这里 documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); return getRegistry().getBeanDefinitionCount() - countBefore; &#125; // DefaultBeanDefinitionDocumentReader 90 @Override public void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; this.readerContext = readerContext; logger.debug(&quot;Loading bean definitions&quot;); Element root = doc.getDocumentElement(); // 从 xml 根节点开始解析文件 doRegisterBeanDefinitions(root); &#125; 经过漫长的链路，一个配置文件终于转换为一颗 DOM 树了，注意，这里指的是其中一个配置文件，不是所有的，读者可以看到上面有个 for 循环的。下面开始从根节点开始解析： doRegisterBeanDefinitions： // DefaultBeanDefinitionDocumentReader 116 protected void doRegisterBeanDefinitions(Element root) &#123; // 我们看名字就知道，BeanDefinitionParserDelegate 必定是一个重要的类，它负责解析 Bean 定义， // 这里为什么要定义一个 parent? 看到后面就知道了，是递归问题， // 因为 &lt;beans /&gt; 内部是可以定义 &lt;beans /&gt; 的，所以这个方法的 root 其实不一定就是 xml 的根节点，也可以是嵌套在里面的 &lt;beans /&gt; 节点，从源码分析的角度，我们当做根节点就好了 BeanDefinitionParserDelegate parent = this.delegate; this.delegate = createDelegate(getReaderContext(), root, parent); if (this.delegate.isDefaultNamespace(root)) &#123; // 这块说的是根节点 &lt;beans ... profile=&quot;dev&quot; /&gt; 中的 profile 是否是当前环境需要的， // 如果当前环境配置的 profile 不包含此 profile，那就直接 return 了，不对此 &lt;beans /&gt; 解析 // 不熟悉 profile 为何物，不熟悉怎么配置 profile 读者的请移步附录区 String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); if (StringUtils.hasText(profileSpec)) &#123; String[] specifiedProfiles = StringUtils.tokenizeToStringArray( profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); if (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) &#123; if (logger.isInfoEnabled()) &#123; logger.info(&quot;Skipped XML bean definition file due to specified profiles [&quot; + profileSpec + &quot;] not matching: &quot; + getReaderContext().getResource()); &#125; return; &#125; &#125; &#125; preProcessXml(root); // 钩子 // 往下看 parseBeanDefinitions(root, this.delegate); postProcessXml(root); // 钩子 this.delegate = parent; &#125; preProcessXml(root) 和 postProcessXml(root) 是给子类用的钩子方法，鉴于没有被使用到，也不是我们的重点，我们直接跳过。 这里涉及到了 profile 的问题，对于不了解的读者，我在附录中对 profile 做了简单的解释，读者可以参考一下。 接下来，看核心解析方法 parseBeanDefinitions(root, this.delegate) : // default namespace 涉及到的就四个标签 &lt;import /&gt;、&lt;alias /&gt;、&lt;bean /&gt; 和 &lt;beans /&gt;， // 其他的属于 custom 的 protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; if (delegate.isDefaultNamespace(root)) &#123; NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) &#123; // 解析 default namespace 下面的几个元素 parseDefaultElement(ele, delegate); &#125; else &#123; // 解析其他 namespace 的元素 delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125; &#125; 从上面的代码，我们可以看到，对于每个配置来说，分别进入到 parseDefaultElement(ele, delegate); 和 delegate.parseCustomElement(ele); 这两个分支了。 parseDefaultElement(ele, delegate) 代表解析的节点是 &lt;import /&gt;、&lt;alias /&gt;、&lt;bean /&gt;、&lt;beans /&gt; 这几个。 这里的四个标签之所以是 default 的，是因为它们是处于这个 namespace 下定义的： http://www.springframework.org/schema/beans 又到初学者科普时间，不熟悉 namespace 的读者请看下面贴出来的 xml，这里的第二行 xmlns 就是咯。 &lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot; default-autowire=&quot;byName&quot;&gt; 而对于其他的标签，将进入到 delegate.parseCustomElement(element) 这个分支。如我们经常会使用到的 &lt;mvc /&gt;、&lt;task /&gt;、&lt;context /&gt;、&lt;aop /&gt;等。 这些属于扩展，如果需要使用上面这些 ”非 default“ 标签，那么上面的 xml 头部的地方也要引入相应的 namespace 和 .xsd 文件的路径，如下所示。同时代码中需要提供相应的 parser 来解析，如 MvcNamespaceHandler、TaskNamespaceHandler、ContextNamespaceHandler、AopNamespaceHandler 等。 假如读者想分析 &lt;context:property-placeholder location=&quot;classpath:xx.properties&quot; /&gt; 的实现原理，就应该到 ContextNamespaceHandler 中找答案。 &lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd &quot; default-autowire=&quot;byName&quot;&gt; 回过神来，看看处理 default 标签的方法： private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; // 处理 &lt;import /&gt; 标签 importBeanDefinitionResource(ele); &#125; elseif (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; // 处理 &lt;alias /&gt; 标签定义 // &lt;alias name=&quot;fromName&quot;alias=&quot;toName&quot;/&gt; processAliasRegistration(ele); &#125; elseif (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; // 处理 &lt;bean /&gt; 标签定义，这也算是我们的重点吧 processBeanDefinition(ele, delegate); &#125; elseif (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123; // 如果碰到的是嵌套的 &lt;beans /&gt; 标签，需要递归 doRegisterBeanDefinitions(ele); &#125; &#125; 如果每个标签都说，那我不吐血，你们都要吐血了。我们挑我们的重点 &lt;bean /&gt; 标签出来说。 processBeanDefinition 解析 bean 标签 下面是 processBeanDefinition 解析 &lt;bean /&gt; 标签： // DefaultBeanDefinitionDocumentReader 298 protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // 将 &lt;bean /&gt; 节点中的信息提取出来，然后封装到一个 BeanDefinitionHolder 中，细节往下看 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); // 下面的几行先不要看，跳过先，跳过先，跳过先，后面会继续说的 if (bdHolder != null) &#123; bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; // Register the final decorated instance. BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error(&quot;Failed to register bean definition with name &#39;&quot; + bdHolder.getBeanName() + &quot;&#39;&quot;, ele, ex); &#125; // Send registration event. getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125; &#125; 继续往下看怎么解析之前，我们先看下 &lt;bean /&gt; 标签中可以定义哪些属性：Propertyclass类的全限定名name可指定 id、name(用逗号、分号、空格分隔)scope作用域constructor arguments指定构造参数properties设置属性的值autowiring modeno(默认值)、byName、byType、 constructorlazy-initialization mode是否懒加载(如果被非懒加载的bean依赖了那么其实也就不能懒加载了)initialization methodbean 属性设置完成后，会调用这个方法destruction methodbean 销毁后的回调方法上面表格中的内容我想大家都非常熟悉吧，如果不熟悉，那就是你不够了解 Spring 的配置了。 简单地说就是像下面这样子： &lt;bean id=&quot;exampleBean&quot; name=&quot;name1, name2, name3&quot; class=&quot;com.javadoop.ExampleBean&quot; scope=&quot;singleton&quot; lazy-init=&quot;true&quot; init-method=&quot;init&quot; destroy-method=&quot;cleanup&quot;&gt; &lt;!-- 可以用下面三种形式指定构造参数 --&gt; &lt;constructor-arg type=&quot;int&quot; value=&quot;7500000&quot;/&gt; &lt;constructor-arg name=&quot;years&quot; value=&quot;7500000&quot;/&gt; &lt;constructor-arg index=&quot;0&quot; value=&quot;7500000&quot;/&gt; &lt;!-- property 的几种情况 --&gt; &lt;property name=&quot;beanOne&quot;&gt; &lt;ref bean=&quot;anotherExampleBean&quot;/&gt; &lt;/property&gt; &lt;property name=&quot;beanTwo&quot; ref=&quot;yetAnotherBean&quot;/&gt; &lt;property name=&quot;integerProperty&quot; value=&quot;1&quot;/&gt; &lt;/bean&gt; 当然，除了上面举例出来的这些，还有 factory-bean、factory-method、&lt;lockup-method /&gt;、&lt;replaced-method /&gt;、&lt;meta /&gt;、&lt;qualifier /&gt; 这几个，大家是不是熟悉呢？自己检验一下自己对 Spring 中 bean 的了解程度。 有了以上这些知识以后，我们再继续往里看怎么解析 bean 元素，是怎么转换到 BeanDefinitionHolder 的。 // BeanDefinitionParserDelegate 428 public BeanDefinitionHolder parseBeanDefinitionElement(Element ele) &#123; return parseBeanDefinitionElement(ele, null); &#125; public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, BeanDefinition containingBean) &#123; String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List&lt;String&gt; aliases = new ArrayList&lt;String&gt;(); // 将 name 属性的定义按照 “逗号、分号、空格” 切分，形成一个 别名列表数组， // 当然，如果你不定义 name 属性的话，就是空的了 // 我在附录中简单介绍了一下 id 和 name 的配置，大家可以看一眼，有个20秒就可以了 if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; // 如果没有指定id, 那么用别名列表的第一个名字作为beanName if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; beanName = aliases.remove(0); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;No XML &#39;id&#39; specified - using &#39;&quot; + beanName + &quot;&#39; as bean name and &quot; + aliases + &quot; as aliases&quot;); &#125; &#125; if (containingBean == null) &#123; checkNameUniqueness(beanName, aliases, ele); &#125; // 根据 &lt;bean ...&gt;...&lt;/bean&gt; 中的配置创建 BeanDefinition，然后把配置中的信息都设置到实例中, // 细节后面细说，先知道下面这行结束后，一个 BeanDefinition 实例就出来了。 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); // 到这里，整个 &lt;bean /&gt; 标签就算解析结束了，一个 BeanDefinition 就形成了。 if (beanDefinition != null) &#123; // 如果都没有设置 id 和 name，那么此时的 beanName 就会为 null，进入下面这块代码产生 // 如果读者不感兴趣的话，我觉得不需要关心这块代码，对本文源码分析来说，这些东西不重要 if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123;// 按照我们的思路，这里 containingBean 是 null 的 beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; // 如果我们不定义 id 和 name，那么我们引言里的那个例子： // 1. beanName 为：com.javadoop.example.MessageServiceImpl#0 // 2. beanClassName 为：com.javadoop.example.MessageServiceImpl beanName = this.readerContext.generateBeanName(beanDefinition); String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; // 把 beanClassName 设置为 Bean 的别名 aliases.add(beanClassName); &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Neither XML &#39;id&#39; nor &#39;name&#39; specified - &quot; + &quot;using generated bean name [&quot; + beanName + &quot;]&quot;); &#125; &#125; catch (Exception ex) &#123; error(ex.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); // 返回 BeanDefinitionHolder return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; return null; &#125; 然后，我们再看看怎么根据配置创建 BeanDefinition 实例的： public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, BeanDefinition containingBean) &#123; this.parseState.push(new BeanEntry(beanName)); String className = null; if (ele.hasAttribute(CLASS_ATTRIBUTE)) &#123; className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); &#125; try &#123; String parent = null; if (ele.hasAttribute(PARENT_ATTRIBUTE)) &#123; parent = ele.getAttribute(PARENT_ATTRIBUTE); &#125; // 创建 BeanDefinition，然后设置类信息而已，很简单，就不贴代码了 AbstractBeanDefinition bd = createBeanDefinition(className, parent); // 设置 BeanDefinition 的一堆属性，这些属性定义在 AbstractBeanDefinition 中 parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); /** * 下面的一堆是解析 &lt;bean&gt;......&lt;/bean&gt; 内部的子元素， * 解析出来以后的信息都放到 bd 的属性中 */ // 解析 &lt;meta /&gt; parseMetaElements(ele, bd); // 解析 &lt;lookup-method /&gt; parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); // 解析 &lt;replaced-method /&gt; parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); // 解析 &lt;constructor-arg /&gt; parseConstructorArgElements(ele, bd); // 解析 &lt;property /&gt; parsePropertyElements(ele, bd); // 解析 &lt;qualifier /&gt; parseQualifierElements(ele, bd); bd.setResource(this.readerContext.getResource()); bd.setSource(extractSource(ele)); return bd; &#125; catch (ClassNotFoundException ex) &#123; error(&quot;Bean class [&quot; + className + &quot;] not found&quot;, ele, ex); &#125; catch (NoClassDefFoundError err) &#123; error(&quot;Class that bean class [&quot; + className + &quot;] depends on not found&quot;, ele, err); &#125; catch (Throwable ex) &#123; error(&quot;Unexpected failure during bean definition parsing&quot;, ele, ex); &#125; finally &#123; this.parseState.pop(); &#125; return null; &#125; 到这里，我们已经完成了根据 &lt;bean /&gt; 配置创建了一个 BeanDefinitionHolder 实例。注意，是一个。 我们回到解析 &lt;bean /&gt; 的入口方法: protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // 将 &lt;bean /&gt; 节点转换为 BeanDefinitionHolder，就是上面说的一堆 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; // 如果有自定义属性的话，进行相应的解析，先忽略 bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; // 我们把这步叫做 注册Bean 吧 BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error(&quot;Failed to register bean definition with name &#39;&quot; + bdHolder.getBeanName() + &quot;&#39;&quot;, ele, ex); &#125; // 注册完成后，发送事件，本文不展开说这个 getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125; &#125; 大家再仔细看一下这块吧，我们后面就不回来说这个了。这里已经根据一个 &lt;bean /&gt; 标签产生了一个 BeanDefinitionHolder 的实例，这个实例里面也就是一个 BeanDefinition 的实例和它的 beanName、aliases 这三个信息，注意，我们的关注点始终在 BeanDefinition 上： public class BeanDefinitionHolder implements BeanMetadataElement &#123; private final BeanDefinition beanDefinition; private final String beanName; private final String[] aliases; ... 然后我们准备注册这个 BeanDefinition，最后，把这个注册事件发送出去。 下面，我们开始说说注册 Bean 吧。 注册 Bean // BeanDefinitionReaderUtils 143 public static void registerBeanDefinition( BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException &#123; String beanName = definitionHolder.getBeanName(); // 注册这个 Bean registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); // 如果还有别名的话，也要根据别名全部注册一遍，不然根据别名就会找不到 Bean 了 String[] aliases = definitionHolder.getAliases(); if (aliases != null) &#123; for (String alias : aliases) &#123; // alias -&gt; beanName 保存它们的别名信息，这个很简单，用一个 map 保存一下就可以了， // 获取的时候，会先将 alias 转换为 beanName，然后再查找 registry.registerAlias(beanName, alias); &#125; &#125; &#125; 别名注册的放一边，毕竟它很简单，我们看看怎么注册 Bean。 // DefaultListableBeanFactory 793 @Override public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123; Assert.hasText(beanName, &quot;Bean name must not be empty&quot;); Assert.notNull(beanDefinition, &quot;BeanDefinition must not be null&quot;); if (beanDefinition instanceof AbstractBeanDefinition) &#123; try &#123; ((AbstractBeanDefinition) beanDefinition).validate(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(...); &#125; &#125; // old? 还记得 “允许 bean 覆盖” 这个配置吗？allowBeanDefinitionOverriding BeanDefinition oldBeanDefinition; // 之后会看到，所有的 Bean 注册后会放入这个 beanDefinitionMap 中 oldBeanDefinition = this.beanDefinitionMap.get(beanName); // 处理重复名称的 Bean 定义的情况 if (oldBeanDefinition != null) &#123; if (!isAllowBeanDefinitionOverriding()) &#123; // 如果不允许覆盖的话，抛异常 throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription()... &#125; elseif (oldBeanDefinition.getRole() &lt; beanDefinition.getRole()) &#123; // log...用框架定义的 Bean 覆盖用户自定义的 Bean &#125; elseif (!beanDefinition.equals(oldBeanDefinition)) &#123; // log...用新的 Bean 覆盖旧的 Bean &#125; else &#123; // log...用同等的 Bean 覆盖旧的 Bean，这里指的是 equals 方法返回 true 的 Bean &#125; // 覆盖 this.beanDefinitionMap.put(beanName, beanDefinition); &#125; else &#123; // 判断是否已经有其他的 Bean 开始初始化了. // 注意，&quot;注册Bean&quot; 这个动作结束，Bean 依然还没有初始化，我们后面会有大篇幅说初始化过程， // 在 Spring 容器启动的最后，会 预初始化 所有的 singleton beans if (hasBeanCreationStarted()) &#123; // Cannot modify startup-time collection elements anymore (for stable iteration) synchronized (this.beanDefinitionMap) &#123; this.beanDefinitionMap.put(beanName, beanDefinition); List&lt;String&gt; updatedDefinitions = new ArrayList&lt;String&gt;(this.beanDefinitionNames.size() + 1); updatedDefinitions.addAll(this.beanDefinitionNames); updatedDefinitions.add(beanName); this.beanDefinitionNames = updatedDefinitions; if (this.manualSingletonNames.contains(beanName)) &#123; Set&lt;String&gt; updatedSingletons = new LinkedHashSet&lt;String&gt;(this.manualSingletonNames); updatedSingletons.remove(beanName); this.manualSingletonNames = updatedSingletons; &#125; &#125; &#125; else &#123; // 最正常的应该是进到这个分支。 // 将 BeanDefinition 放到这个 map 中，这个 map 保存了所有的 BeanDefinition this.beanDefinitionMap.put(beanName, beanDefinition); // 这是个 ArrayList，所以会按照 bean 配置的顺序保存每一个注册的 Bean 的名字 this.beanDefinitionNames.add(beanName); // 这是个 LinkedHashSet，代表的是手动注册的 singleton bean， // 注意这里是 remove 方法，到这里的 Bean 当然不是手动注册的 // 手动指的是通过调用以下方法注册的 bean ： // registerSingleton(String beanName, Object singletonObject) // 这不是重点，解释只是为了不让大家疑惑。Spring 会在后面&quot;手动&quot;注册一些 Bean， // 如 &quot;environment&quot;、&quot;systemProperties&quot; 等 bean，我们自己也可以在运行时注册 Bean 到容器中的 this.manualSingletonNames.remove(beanName); &#125; // 这个不重要，在预初始化的时候会用到，不必管它。 this.frozenBeanDefinitionNames = null; &#125; if (oldBeanDefinition != null || containsSingleton(beanName)) &#123; resetBeanDefinition(beanName); &#125; &#125; 总结一下，到这里已经初始化了 Bean 容器，&lt;bean /&gt; 配置也相应的转换为了一个个 BeanDefinition，然后注册了各个 BeanDefinition 到注册中心，并且发送了注册事件。 到这里是一个分水岭，前面的内容都还算比较简单，大家要清楚地知道前面都做了哪些事情。 Bean 容器实例化完成后说到这里，我们回到 refresh() 方法，我重新贴了一遍代码，看看我们说到哪了。是的，我们才说完 obtainFreshBeanFactory() 方法。 考虑到篇幅，这里开始大幅缩减掉没必要详细介绍的部分，大家直接看下面的代码中的注释就好了。 @Override public void refresh() throws BeansException, IllegalStateException &#123; // 来个锁，不然 refresh() 还没结束，你又来个启动或销毁容器的操作，那不就乱套了嘛 synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录下容器的启动时间、标记“已启动”状态、处理配置文件中的占位符 prepareRefresh(); // 这步比较关键，这步完成后，配置文件就会解析成一个个 Bean 定义，注册到 BeanFactory 中， // 当然，这里说的 Bean 还没有初始化，只是配置信息都提取出来了， // 注册也只是将这些信息都保存到了注册中心(说到底核心是一个 beanName-&gt; beanDefinition 的 map) ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean // 这块待会会展开说 prepareBeanFactory(beanFactory); try &#123; // 【这里需要知道 BeanFactoryPostProcessor 这个知识点，Bean 如果实现了此接口， // 那么在容器初始化以后，Spring 会负责调用里面的 postProcessBeanFactory 方法。】 // 这里是提供给子类的扩展点，到这里的时候，所有的 Bean 都加载、注册完成了，但是都还没有初始化 // 具体的子类可以在这步的时候添加一些特殊的 BeanFactoryPostProcessor 的实现类或做点什么事 postProcessBeanFactory(beanFactory); // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 回调方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类，注意看和 BeanFactoryPostProcessor 的区别 // 此接口两个方法: postProcessBeforeInitialization 和 postProcessAfterInitialization // 两个方法分别在 Bean 初始化之前和初始化之后得到执行。这里仅仅是注册，之后会看到回调这两方法的时机 registerBeanPostProcessors(beanFactory); // 初始化当前 ApplicationContext 的 MessageSource，国际化这里就不展开说了，不然没完没了了 initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器，这里也不展开了 initApplicationEventMulticaster(); // 从方法名就可以知道，典型的模板方法(钩子方法)，不展开说 // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前） onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口。这也不是我们的重点，过 registerListeners(); // 重点，重点，重点 // 初始化所有的 singleton beans //（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 最后，广播事件，ApplicationContext 初始化完成，不展开 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); &#125; // Destroy already created singletons to avoid dangling resources. // 销毁已经初始化的 singleton 的 Beans，以免有些 bean 会一直占用资源 destroyBeans(); // Reset &#39;active&#39; flag. cancelRefresh(ex); // 把异常往外抛 throw ex; &#125; finally &#123; // Reset common introspection caches in Spring&#39;s core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125; &#125; 准备 Bean 容器: prepareBeanFactory之前我们说过，Spring 把我们在 xml 配置的 bean 都注册以后，会”手动”注册一些特殊的 bean。 这里简单介绍下 prepareBeanFactory(factory) 方法： /** * Configure the factory&#39;s standard context characteristics, * such as the context&#39;s ClassLoader and post-processors. * @param beanFactory the BeanFactory to configure */ protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; // 设置 BeanFactory 的类加载器，我们知道 BeanFactory 需要加载类，也就需要类加载器， // 这里设置为加载当前 ApplicationContext 类的类加载器 beanFactory.setBeanClassLoader(getClassLoader()); // 设置 BeanExpressionResolver beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); // beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // 添加一个 BeanPostProcessor，这个 processor 比较简单： // 实现了 Aware 接口的 beans 在初始化的时候，这个 processor 负责回调， // 这个我们很常用，如我们会为了获取 ApplicationContext 而 implement ApplicationContextAware // 注意：它不仅仅回调 ApplicationContextAware， // 还会负责回调 EnvironmentAware、ResourceLoaderAware 等，看下源码就清楚了 beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); // 下面几行的意思就是，如果某个 bean 依赖于以下几个接口的实现类，在自动装配的时候忽略它们， // Spring 会通过其他方式来处理这些依赖。 beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); /** * 下面几行就是为特殊的几个 bean 赋值，如果有 bean 依赖了以下几个，会注入这边相应的值， * 之前我们说过，&quot;当前 ApplicationContext 持有一个 BeanFactory&quot;，这里解释了第一行 * ApplicationContext 还继承了 ResourceLoader、ApplicationEventPublisher、MessageSource * 所以对于这几个依赖，可以赋值为 this，注意 this 是一个 ApplicationContext * 那这里怎么没看到为 MessageSource 赋值呢？那是因为 MessageSource 被注册成为了一个普通的 bean */ beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); // 这个 BeanPostProcessor 也很简单，在 bean 实例化后，如果是 ApplicationListener 的子类， // 那么将其添加到 listener 列表中，可以理解成：注册 事件监听器 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); // 这里涉及到特殊的 bean，名为：loadTimeWeaver，这不是我们的重点，忽略它 // tips: ltw 是 AspectJ 的概念，指的是在运行期进行织入，这个和 Spring AOP 不一样， // 感兴趣的读者请参考我写的关于 AspectJ 的另一篇文章 https://www.javadoop.com/post/aspectj if (beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) &#123; beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); // Set a temporary ClassLoader fortype matching. beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); &#125; /** * 从下面几行代码我们可以知道，Spring 往往很 &quot;智能&quot; 就是因为它会帮我们默认注册一些有用的 bean， * 我们也可以选择覆盖 */ // 如果没有定义 &quot;environment&quot; 这个 bean，那么 Spring 会 &quot;手动&quot; 注册一个 if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); &#125; // 如果没有定义 &quot;systemProperties&quot; 这个 bean，那么 Spring 会 &quot;手动&quot; 注册一个 if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); &#125; // 如果没有定义 &quot;systemEnvironment&quot; 这个 bean，那么 Spring 会 &quot;手动&quot; 注册一个 if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); &#125; &#125; 在上面这块代码中，Spring 对一些特殊的 bean 进行了处理，读者如果暂时还不能消化它们也没有关系，慢慢往下看。 初始化所有的 singleton beans我们的重点当然是 finishBeanFactoryInitialization(beanFactory); 这个巨头了，这里会负责初始化所有的 singleton beans。 注意，后面的描述中，我都会使用初始化或预初始化来代表这个阶段，Spring 会在这个阶段完成所有的 singleton beans 的实例化。 我们来总结一下，到目前为止，应该说 BeanFactory 已经创建完成，并且所有的实现了 BeanFactoryPostProcessor 接口的 Bean 都已经初始化并且其中的 postProcessBeanFactory(factory) 方法已经得到回调执行了。而且 Spring 已经“手动”注册了一些特殊的 Bean，如 ‘environment’、‘systemProperties’ 等。 剩下的就是初始化 singleton beans 了，我们知道它们是单例的，如果没有设置懒加载，那么 Spring 会在接下来初始化所有的 singleton beans。 // AbstractApplicationContext.java 834 // 初始化剩余的 singleton beans protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; // 首先，初始化名字为 conversionService 的 Bean。本着送佛送到西的精神，我在附录中简单介绍了一下 ConversionService，因为这实在太实用了 // 什么，看代码这里没有初始化 Bean 啊！ // 注意了，初始化的动作包装在 beanFactory.getBean(...) 中，这里先不说细节，先往下看吧 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); &#125; // Register a default embedded value resolver if no bean post-processor // (such as a PropertyPlaceholderConfigurer bean) registered any before: // at this point, primarily for resolution in annotation attribute values. if (!beanFactory.hasEmbeddedValueResolver()) &#123; beanFactory.addEmbeddedValueResolver(new StringValueResolver() &#123; @Override public String resolveStringValue(String strVal) &#123; return getEnvironment().resolvePlaceholders(strVal); &#125; &#125;); &#125; // 先初始化 LoadTimeWeaverAware 类型的 Bean // 之前也说过，这是 AspectJ 相关的内容，放心跳过吧 String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) &#123; getBean(weaverAwareName); &#125; // Stop using the temporary ClassLoader fortype matching. beanFactory.setTempClassLoader(null); // 没什么别的目的，因为到这一步的时候，Spring 已经开始预初始化 singleton beans 了， // 肯定不希望这个时候还出现 bean 定义解析、加载、注册。 beanFactory.freezeConfiguration(); // 开始初始化 beanFactory.preInstantiateSingletons(); &#125; 从上面最后一行往里看，我们就又回到 DefaultListableBeanFactory 这个类了，这个类大家应该都不陌生了吧。 preInstantiateSingletons // DefaultListableBeanFactory 728 @Override public void preInstantiateSingletons() throws BeansException &#123; if (this.logger.isDebugEnabled()) &#123; this.logger.debug(&quot;Pre-instantiating singletons in &quot; + this); &#125; // this.beanDefinitionNames 保存了所有的 beanNames List&lt;String&gt; beanNames = new ArrayList&lt;String&gt;(this.beanDefinitionNames); // 触发所有的非懒加载的 singleton beans 的初始化操作 for (String beanName : beanNames) &#123; // 合并父 Bean 中的配置，注意 &lt;bean id=&quot;&quot; class=&quot;&quot; parent=&quot;&quot; /&gt; 中的 parent，用的不多吧， // 考虑到这可能会影响大家的理解，我在附录中解释了一下 &quot;Bean 继承&quot;，不了解的请到附录中看一下 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); // 非抽象、非懒加载的 singletons。如果配置了 &#39;abstract = true&#39;，那是不需要初始化的 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; // 处理 FactoryBean(读者如果不熟悉 FactoryBean，请移步附录区了解) if (isFactoryBean(beanName)) &#123; // FactoryBean 的话，在 beanName 前面加上 ‘&amp;’ 符号。再调用 getBean，getBean 方法别急 final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) getBean(FACTORY_BEAN_PREFIX + beanName); // 判断当前 FactoryBean 是否是 SmartFactoryBean 的实现，此处忽略，直接跳过 boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123; isEagerInit = AccessController.doPrivileged(new PrivilegedAction&lt;Boolean&gt;() &#123; @Override public Boolean run() &#123; return ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit(); &#125; &#125;, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; if (isEagerInit) &#123; getBean(beanName); &#125; &#125; else &#123; // 对于普通的 Bean，只要调用 getBean(beanName) 这个方法就可以进行初始化了 getBean(beanName); &#125; &#125; &#125; // 到这里说明所有的非懒加载的 singleton beans 已经完成了初始化 // 如果我们定义的 bean 是实现了 SmartInitializingSingleton 接口的，那么在这里得到回调，忽略 for (String beanName : beanNames) &#123; Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) &#123; final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; @Override public Object run() &#123; smartSingleton.afterSingletonsInstantiated(); return null; &#125; &#125;, getAccessControlContext()); &#125; else &#123; smartSingleton.afterSingletonsInstantiated(); &#125; &#125; &#125; &#125; 接下来，我们就进入到 getBean(beanName) 方法了，这个方法我们经常用来从 BeanFactory 中获取一个 Bean，而初始化的过程也封装到了这个方法里。 getBean 在继续前进之前，读者应该具备 FactoryBean 的知识，如果读者还不熟悉，请移步附录部分了解 FactoryBean。 // AbstractBeanFactory 196 @Override public Object getBean(String name) throws BeansException &#123; returndoGetBean(name, null, null, false); &#125; // 我们在剖析初始化 Bean 的过程，但是 getBean 方法我们经常是用来从容器中获取 Bean 用的，注意切换思路， // 已经初始化过了就从容器中直接返回，否则就先初始化再返回 @SuppressWarnings(&quot;unchecked&quot;) protected &lt;T&gt; T doGetBean( final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException &#123; // 获取一个 “正统的” beanName，处理两种情况，一个是前面说的 FactoryBean(前面带 ‘&amp;’)， // 一个是别名问题，因为这个方法是 getBean，获取 Bean 用的，你要是传一个别名进来，是完全可以的 final String beanName = transformedBeanName(name); // 注意跟着这个，这个是返回值 Object bean; // 检查下是不是已经创建过了 Object sharedInstance = getSingleton(beanName); // 这里说下 args 呗，虽然看上去一点不重要。前面我们一路进来的时候都是 getBean(beanName)， // 所以 args 传参其实是 null 的，但是如果 args 不为空的时候，那么意味着调用方不是希望获取 Bean，而是创建 Bean if (sharedInstance != null &amp;&amp; args == null) &#123; if (logger.isDebugEnabled()) &#123; if (isSingletonCurrentlyInCreation(beanName)) &#123; logger.debug(&quot;...&quot;); &#125; else &#123; logger.debug(&quot;Returning cached instance of singleton bean &#39;&quot; + beanName + &quot;&#39;&quot;); &#125; &#125; // 下面这个方法：如果是普通 Bean 的话，直接返回 sharedInstance， // 如果是 FactoryBean 的话，返回它创建的那个实例对象 // (FactoryBean 知识，读者若不清楚请移步附录) bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); &#125; else &#123; if (isPrototypeCurrentlyInCreation(beanName)) &#123; // 创建过了此 beanName 的 prototype 类型的 bean，那么抛异常， // 往往是因为陷入了循环引用 throw new BeanCurrentlyInCreationException(beanName); &#125; // 检查一下这个 BeanDefinition 在容器中是否存在 BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; // 如果当前容器不存在这个 BeanDefinition，试试父容器中有没有 String nameToLookup = originalBeanName(name); if (args != null) &#123; // 返回父容器的查询结果 return (T) parentBeanFactory.getBean(nameToLookup, args); &#125; else &#123; // No args -&gt; delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); &#125; &#125; if (!typeCheckOnly) &#123; // typeCheckOnly 为 false，将当前 beanName 放入一个 alreadyCreated 的 Set 集合中。 markBeanAsCreated(beanName); &#125; /* * 稍稍总结一下： * 到这里的话，要准备创建 Bean 了，对于 singleton 的 Bean 来说，容器中还没创建过此 Bean； * 对于 prototype 的 Bean 来说，本来就是要创建一个新的 Bean。 */ try &#123; final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); // 先初始化依赖的所有 Bean，这个很好理解。 // 注意，这里的依赖指的是 depends-on 中定义的依赖 String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) &#123; for (String dep : dependsOn) &#123; // 检查是不是有循环依赖，这里的循环依赖和我们前面说的循环依赖又不一样，这里肯定是不允许出现的，不然要乱套了，读者想一下就知道了 if (isDependent(beanName, dep)) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Circular depends-on relationship between &#39;&quot; + beanName + &quot;&#39; and &#39;&quot; + dep + &quot;&#39;&quot;); &#125; // 注册一下依赖关系 registerDependentBean(dep, beanName); // 先初始化被依赖项 getBean(dep); &#125; &#125; // 如果是 singleton scope 的，创建 singleton 的实例 if (mbd.isSingleton()) &#123; sharedInstance = getSingleton(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; try &#123; // 执行创建 Bean，详情后面再说 return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; destroySingleton(beanName); throw ex; &#125; &#125; &#125;); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); &#125; // 如果是 prototype scope 的，创建 prototype 的实例 elseif (mbd.isPrototype()) &#123; // It&#39;s a prototype -&gt; create a new instance. Object prototypeInstance = null; try &#123; beforePrototypeCreation(beanName); // 执行创建 Bean prototypeInstance = createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); &#125; // 如果不是 singleton 和 prototype 的话，需要委托给相应的实现类来处理 else &#123; String scopeName = mbd.getScope(); final Scope scope = this.scopes.get(scopeName); if (scope == null) &#123; throw new IllegalStateException(&quot;No Scope registered for scope name &#39;&quot; + scopeName + &quot;&#39;&quot;); &#125; try &#123; Object scopedInstance = scope.get(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; beforePrototypeCreation(beanName); try &#123; // 执行创建 Bean return createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; &#125; &#125;); bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); &#125; catch (IllegalStateException ex) &#123; throw new BeanCreationException(beanName, &quot;Scope &#39;&quot; + scopeName + &quot;&#39; is not active for the current thread; consider &quot; + &quot;defining a scoped proxy for this bean if you intend to refer to it from a singleton&quot;, ex); &#125; &#125; &#125; catch (BeansException ex) &#123; cleanupAfterBeanCreationFailure(beanName); throw ex; &#125; &#125; // 最后，检查一下类型对不对，不对的话就抛异常，对的话就返回了 if (requiredType != null &amp;&amp; bean != null &amp;&amp; !requiredType.isInstance(bean)) &#123; try &#123; return getTypeConverter().convertIfNecessary(bean, requiredType); &#125; catch (TypeMismatchException ex) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Failed to convert bean &#39;&quot; + name + &quot;&#39; to required type &#39;&quot; + ClassUtils.getQualifiedName(requiredType) + &quot;&#39;&quot;, ex); &#125; throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); &#125; &#125; return (T) bean; &#125; 大家应该也猜到了，接下来当然是分析 createBean 方法： protected abstract Object createBean(String beanName, RootBeanDefinition mbd, Object[] args) throws BeanCreationException; 第三个参数 args 数组代表创建实例需要的参数，不就是给构造方法用的参数，或者是工厂 Bean 的参数嘛，不过要注意，在我们的初始化阶段，args 是 null。 这回我们要到一个新的类了 AbstractAutowireCapableBeanFactory，看类名，AutowireCapable？类名是不是也说明了点问题了。 主要是为了以下场景，采用 @Autowired 注解注入属性值： public class MessageServiceImpl implements MessageService &#123; @Autowired private UserService userService; public String getMessage() &#123; return userService.getMessage(); &#125; &#125; &lt;bean id=&quot;messageService&quot; class=&quot;com.javadoop.example.MessageServiceImpl&quot; /&gt; 以上这种属于混用了 xml 和 注解 两种方式的配置方式，Spring 会处理这种情况。 好了，读者要知道这么回事就可以了，继续向前。 // AbstractAutowireCapableBeanFactory 447 /** * Central method of this class: creates a bean instance, * populates the bean instance, applies post-processors, etc. * @see #doCreateBean */ @Override protected Object createBean(String beanName, RootBeanDefinition mbd, Object[] args) throws BeanCreationException &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Creating instance of bean &#39;&quot; + beanName + &quot;&#39;&quot;); &#125; RootBeanDefinition mbdToUse = mbd; // 确保 BeanDefinition 中的 Class 被加载 Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName); if (resolvedClass != null &amp;&amp; !mbd.hasBeanClass() &amp;&amp; mbd.getBeanClassName() != null) &#123; mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); &#125; // 准备方法覆写，这里又涉及到一个概念：MethodOverrides，它来自于 bean 定义中的 &lt;lookup-method /&gt; // 和 &lt;replaced-method /&gt;，如果读者感兴趣，回到 bean 解析的地方看看对这两个标签的解析。 // 我在附录中也对这两个标签的相关知识点进行了介绍，读者可以移步去看看 try &#123; mbdToUse.prepareMethodOverrides(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(mbdToUse.getResourceDescription(), beanName, &quot;Validation of method overrides failed&quot;, ex); &#125; try &#123; // 让 InstantiationAwareBeanPostProcessor 在这一步有机会返回代理， // 在 《Spring AOP 源码分析》那篇文章中有解释，这里先跳过 Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) &#123; return bean; &#125; &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, &quot;BeanPostProcessor before instantiation of bean failed&quot;, ex); &#125; // 重头戏，创建 bean Object beanInstance = doCreateBean(beanName, mbdToUse, args); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Finished creating instance of bean &#39;&quot; + beanName + &quot;&#39;&quot;); &#125; return beanInstance; &#125; 创建 Bean 我们继续往里看 doCreateBean 这个方法： /** * Actually create the specified bean. Pre-creation processing has already happened * at this point, e.g. checking &#123;@code postProcessBeforeInstantiation&#125; callbacks. * &lt;p&gt;Differentiates between default bean instantiation, use of a * factory method, and autowiring a constructor. * @param beanName the name of the bean * @param mbd the merged bean definition for the bean * @param args explicit arguments to use for constructor or factory method invocation * @return a new instance of the bean * @throws BeanCreationException if the bean could not be created * @see #instantiateBean * @see #instantiateUsingFactoryMethod * @see #autowireConstructor */ protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) throws BeanCreationException &#123; // Instantiate the bean. BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) &#123; instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); &#125; if (instanceWrapper == null) &#123; // 说明不是 FactoryBean，这里实例化 Bean，这里非常关键，细节之后再说 instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; // 这个就是 Bean 里面的 我们定义的类 的实例，很多地方我直接描述成 &quot;bean 实例&quot; final Object bean = (instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null); // 类型 Class&lt;?&gt; beanType = (instanceWrapper != null ? instanceWrapper.getWrappedClass() : null); mbd.resolvedTargetType = beanType; // 建议跳过吧，涉及接口：MergedBeanDefinitionPostProcessor synchronized (mbd.postProcessingLock) &#123; if (!mbd.postProcessed) &#123; try &#123; // MergedBeanDefinitionPostProcessor，这个我真不展开说了，直接跳过吧，很少用的 applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Post-processing of merged bean definition failed&quot;, ex); &#125; mbd.postProcessed = true; &#125; &#125; // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. // 下面这块代码是为了解决循环依赖的问题，以后有时间，我再对循环依赖这个问题进行解析吧 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Eagerly caching bean &#39;&quot; + beanName + &quot;&#39; to allow for resolving potential circular references&quot;); &#125; addSingletonFactory(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; return getEarlyBeanReference(beanName, mbd, bean); &#125; &#125;); &#125; // Initialize the bean instance. Object exposedObject = bean; try &#123; // 这一步也是非常关键的，这一步负责属性装配，因为前面的实例只是实例化了，并没有设值，这里就是设值 populateBean(beanName, mbd, instanceWrapper); if (exposedObject != null) &#123; // 还记得 init-method 吗？还有 InitializingBean 接口？还有 BeanPostProcessor 接口？ // 这里就是处理 bean 初始化完成后的各种回调 exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; &#125; catch (Throwable ex) &#123; if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123; throw (BeanCreationException) ex; &#125; else &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Initialization of bean failed&quot;, ex); &#125; &#125; if (earlySingletonExposure) &#123; // Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) &#123; if (exposedObject == bean) &#123; exposedObject = earlySingletonReference; &#125; elseif (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) &#123; String[] dependentBeans = getDependentBeans(beanName); Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;String&gt;(dependentBeans.length); for (String dependentBean : dependentBeans) &#123; if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) &#123; actualDependentBeans.add(dependentBean); &#125; &#125; if (!actualDependentBeans.isEmpty()) &#123; throw new BeanCurrentlyInCreationException(beanName, &quot;Bean with name &#39;&quot; + beanName + &quot;&#39; has been injected into other beans [&quot; + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + &quot;] in its raw version as part of a circular reference, but has eventually been &quot; + &quot;wrapped. This means that said other beans do not use the final version of the &quot; + &quot;bean. This is often the result of over-eager type matching - consider using &quot; + &quot;&#39;getBeanNamesOfType&#39; with the &#39;allowEagerInit&#39; flag turned off, for example.&quot;); &#125; &#125; &#125; &#125; // Register bean as disposable. try &#123; registerDisposableBeanIfNecessary(beanName, bean, mbd); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Invalid destruction signature&quot;, ex); &#125; return exposedObject; &#125; 到这里，我们已经分析完了 doCreateBean 方法，总的来说，我们已经说完了整个初始化流程。 接下来我们挑 doCreateBean 中的三个细节出来说说。一个是创建 Bean 实例的 createBeanInstance 方法，一个是依赖注入的 populateBean 方法，还有就是回调方法 initializeBean。 注意了，接下来的这三个方法要认真说那也是极其复杂的，很多地方我就点到为止了，感兴趣的读者可以自己往里看，最好就是碰到不懂的，自己写代码去调试它。 创建 Bean 实例 我们先看看 createBeanInstance 方法。需要说明的是，这个方法如果每个分支都分析下去，必然也是极其复杂冗长的，我们挑重点说。此方法的目的就是实例化我们指定的类。 protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, Object[] args) &#123; // 确保已经加载了此 class Class&lt;?&gt; beanClass = resolveBeanClass(mbd, beanName); // 校验一下这个类的访问权限 if (beanClass != null &amp;&amp; !Modifier.isPublic(beanClass.getModifiers()) &amp;&amp; !mbd.isNonPublicAccessAllowed()) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Bean class isn&#39;t public, and non-public access not allowed: &quot; + beanClass.getName()); &#125; if (mbd.getFactoryMethodName() != null) &#123; // 采用工厂方法实例化，不熟悉这个概念的读者请看附录，注意，不是 FactoryBean return instantiateUsingFactoryMethod(beanName, mbd, args); &#125; // 如果不是第一次创建，比如第二次创建 prototype bean。 // 这种情况下，我们可以从第一次创建知道，采用无参构造函数，还是构造函数依赖注入 来完成实例化 boolean resolved = false; boolean autowireNecessary = false; if (args == null) &#123; synchronized (mbd.constructorArgumentLock) &#123; if (mbd.resolvedConstructorOrFactoryMethod != null) &#123; resolved = true; autowireNecessary = mbd.constructorArgumentsResolved; &#125; &#125; &#125; if (resolved) &#123; if (autowireNecessary) &#123; // 构造函数依赖注入 return autowireConstructor(beanName, mbd, null, null); &#125; else &#123; // 无参构造函数 return instantiateBean(beanName, mbd); &#125; &#125; // 判断是否采用有参构造函数 Constructor&lt;?&gt;[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); if (ctors != null || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) &#123; // 构造函数依赖注入 return autowireConstructor(beanName, mbd, ctors, args); &#125; // 调用无参构造函数 return instantiateBean(beanName, mbd); &#125; 挑个简单的无参构造函数构造实例来看看： protected BeanWrapper instantiateBean(final String beanName, final RootBeanDefinition mbd) &#123; try &#123; Object beanInstance; final BeanFactory parent = this; if (System.getSecurityManager() != null) &#123; beanInstance = AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; @Override public Object run() &#123; return getInstantiationStrategy().instantiate(mbd, beanName, parent); &#125; &#125;, getAccessControlContext()); &#125; else &#123; // 实例化 beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); &#125; // 包装一下，返回 BeanWrapper bw = new BeanWrapperImpl(beanInstance); initBeanWrapper(bw); return bw; &#125; catch (Throwable ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Instantiation of bean failed&quot;, ex); &#125; &#125; 我们可以看到，关键的地方在于： beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); 这里会进行实际的实例化过程，我们进去看看: // SimpleInstantiationStrategy 59 @Override public Object instantiate(RootBeanDefinition bd, String beanName, BeanFactory owner) &#123; // 如果不存在方法覆写，那就使用 java 反射进行实例化，否则使用 CGLIB, // 方法覆写 请参见附录&quot;方法注入&quot;中对 lookup-method 和 replaced-method 的介绍 if (bd.getMethodOverrides().isEmpty()) &#123; Constructor&lt;?&gt; constructorToUse; synchronized (bd.constructorArgumentLock) &#123; constructorToUse = (Constructor&lt;?&gt;) bd.resolvedConstructorOrFactoryMethod; if (constructorToUse == null) &#123; final Class&lt;?&gt; clazz = bd.getBeanClass(); if (clazz.isInterface()) &#123; throw new BeanInstantiationException(clazz, &quot;Specified class is an interface&quot;); &#125; try &#123; if (System.getSecurityManager() != null) &#123; constructorToUse = AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Constructor&lt;?&gt;&gt;() &#123; @Override public Constructor&lt;?&gt; run() throws Exception &#123; return clazz.getDeclaredConstructor((Class[]) null); &#125; &#125;); &#125; else &#123; constructorToUse = clazz.getDeclaredConstructor((Class[]) null); &#125; bd.resolvedConstructorOrFactoryMethod = constructorToUse; &#125; catch (Throwable ex) &#123; throw new BeanInstantiationException(clazz, &quot;No default constructor found&quot;, ex); &#125; &#125; &#125; // 利用构造方法进行实例化 return BeanUtils.instantiateClass(constructorToUse); &#125; else &#123; // 存在方法覆写，利用 CGLIB 来完成实例化，需要依赖于 CGLIB 生成子类，这里就不展开了。 // tips: 因为如果不使用 CGLIB 的话，存在 override 的情况 JDK 并没有提供相应的实例化支持 return instantiateWithMethodInjection(bd, beanName, owner); &#125; &#125; 到这里，我们就算实例化完成了。我们开始说怎么进行属性注入。 bean 属性注入 看完了 createBeanInstance(…) 方法，我们来看看 populateBean(…) 方法，该方法负责进行属性设值，处理依赖。 // AbstractAutowireCapableBeanFactory 1203 protected void populateBean(String beanName, RootBeanDefinition mbd, BeanWrapper bw) &#123; // bean 实例的所有属性都在这里了 PropertyValues pvs = mbd.getPropertyValues(); if (bw == null) &#123; if (!pvs.isEmpty()) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Cannot apply property values to null instance&quot;); &#125; else &#123; // Skip property population phase for null instance. return; &#125; &#125; // 到这步的时候，bean 实例化完成（通过工厂方法或构造方法），但是还没开始属性设值， // InstantiationAwareBeanPostProcessor 的实现类可以在这里对 bean 进行状态修改， // 我也没找到有实际的使用，所以我们暂且忽略这块吧 boolean continueWithPropertyPopulation = true; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; // 如果返回 false，代表不需要进行后续的属性设值，也不需要再经过其他的 BeanPostProcessor 的处理 if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; continueWithPropertyPopulation = false; break; &#125; &#125; &#125; &#125; if (!continueWithPropertyPopulation) &#123; return; &#125; if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // 通过名字找到所有属性值，如果是 bean 依赖，先初始化依赖的 bean。记录依赖关系 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) &#123; autowireByName(beanName, mbd, bw, newPvs); &#125; // 通过类型装配。复杂一些 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; autowireByType(beanName, mbd, bw, newPvs); &#125; pvs = newPvs; &#125; boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); boolean needsDepCheck = (mbd.getDependencyCheck() != RootBeanDefinition.DEPENDENCY_CHECK_NONE); if (hasInstAwareBpps || needsDepCheck) &#123; PropertyDescriptor[] filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); if (hasInstAwareBpps) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; // 这里有个非常有用的 BeanPostProcessor 进到这里: AutowiredAnnotationBeanPostProcessor // 对采用 @Autowired、@Value 注解的依赖进行设值，这里的内容也是非常丰富的，不过本文不会展开说了，感兴趣的读者请自行研究 pvs = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvs == null) &#123; return; &#125; &#125; &#125; &#125; if (needsDepCheck) &#123; checkDependencies(beanName, mbd, filteredPds, pvs); &#125; &#125; // 设置 bean 实例的属性值 applyPropertyValues(beanName, mbd, bw, pvs); &#125; initializeBean 属性注入完成后，这一步其实就是处理各种回调了，这块代码比较简单。 protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) &#123; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; @Override public Object run() &#123; invokeAwareMethods(beanName, bean); return null; &#125; &#125;, getAccessControlContext()); &#125; else &#123; // 如果 bean 实现了 BeanNameAware、BeanClassLoaderAware 或 BeanFactoryAware 接口，回调 invokeAwareMethods(beanName, bean); &#125; Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) &#123; // BeanPostProcessor 的 postProcessBeforeInitialization 回调 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; try &#123; // 处理 bean 中定义的 init-method， // 或者如果 bean 实现了 InitializingBean 接口，调用 afterPropertiesSet() 方法 invokeInitMethods(beanName, wrappedBean, mbd); &#125; catch (Throwable ex) &#123; throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, &quot;Invocation of init method failed&quot;, ex); &#125; if (mbd == null || !mbd.isSynthetic()) &#123; // BeanPostProcessor 的 postProcessAfterInitialization 回调 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean; &#125; 大家发现没有，BeanPostProcessor 的两个回调都发生在这边，只不过中间处理了 init-method，是不是和读者原来的认知有点不一样了？ 附录id 和 name每个 Bean 在 Spring 容器中都有一个唯一的名字（beanName）和 0 个或多个别名（aliases）。 我们从 Spring 容器中获取 Bean 的时候，可以根据 beanName，也可以通过别名。 beanFactory.getBean(&quot;beanName or alias&quot;); 在配置 &lt;bean /&gt; 的过程中，我们可以配置 id 和 name，看几个例子就知道是怎么回事了。 &lt;bean id=&quot;messageService&quot; name=&quot;m1, m2, m3&quot; class=&quot;com.javadoop.example.MessageServiceImpl&quot;&gt; 以上配置的结果就是：beanName 为 messageService，别名有 3 个，分别为 m1、m2、m3。 &lt;bean name=&quot;m1, m2, m3&quot; class=&quot;com.javadoop.example.MessageServiceImpl&quot; /&gt; 以上配置的结果就是：beanName 为 m1，别名有 2 个，分别为 m2、m3。 &lt;bean class=&quot;com.javadoop.example.MessageServiceImpl&quot;&gt; beanName 为：com.javadoop.example.MessageServiceImpl#0， 别名 1 个，为： com.javadoop.example.MessageServiceImpl &lt;bean id=&quot;messageService&quot; class=&quot;com.javadoop.example.MessageServiceImpl&quot;&gt; 以上配置的结果就是：beanName 为 messageService，没有别名。 配置是否允许 Bean 覆盖、是否允许循环依赖我们说过，默认情况下，allowBeanDefinitionOverriding 属性为 null。如果在同一配置文件中 Bean id 或 name 重复了，会抛错，但是如果不是同一配置文件中，会发生覆盖。 可是有些时候我们希望在系统启动的过程中就严格杜绝发生 Bean 覆盖，因为万一出现这种情况，会增加我们排查问题的成本。 循环依赖说的是 A 依赖 B，而 B 又依赖 A。或者是 A 依赖 B，B 依赖 C，而 C 却依赖 A。默认 allowCircularReferences 也是 null。 它们两个属性是一起出现的，必然可以在同一个地方一起进行配置。 添加这两个属性的作者 Juergen Hoeller 在这个 jira 的讨论中说明了怎么配置这两个属性。 public class NoBeanOverridingContextLoader extends ContextLoader &#123; @Override protected void customizeContext(ServletContext servletContext, ConfigurableWebApplicationContext applicationContext) &#123; super.customizeContext(servletContext, applicationContext); AbstractRefreshableApplicationContext arac = (AbstractRefreshableApplicationContext) applicationContext; arac.setAllowBeanDefinitionOverriding(false); &#125; &#125; public class MyContextLoaderListener extends org.springframework.web.context.ContextLoaderListener &#123; @Override protected ContextLoader createContextLoader() &#123; return new NoBeanOverridingContextLoader(); &#125; &#125; &lt;listener&gt; &lt;listener-class&gt;com.javadoop.MyContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; 如果以上方式不能满足你的需求，请参考这个链接：解决spring中不同配置文件中存在name或者id相同的bean可能引起的问题 profile我们可以把不同环境的配置分别配置到单独的文件中，举个例子： &lt;beans profile=&quot;development&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xsi:schemaLocation=&quot;...&quot;&gt; &lt;jdbc:embedded-database id=&quot;dataSource&quot;&gt; &lt;jdbc:script location=&quot;classpath:com/bank/config/sql/schema.sql&quot;/&gt; &lt;jdbc:script location=&quot;classpath:com/bank/config/sql/test-data.sql&quot;/&gt; &lt;/jdbc:embedded-database&gt; &lt;/beans&gt; &lt;beans profile=&quot;production&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:jee=&quot;http://www.springframework.org/schema/jee&quot; xsi:schemaLocation=&quot;...&quot;&gt; &lt;jee:jndi-lookup id=&quot;dataSource&quot; jndi-name=&quot;java:comp/env/jdbc/datasource&quot;/&gt; &lt;/beans&gt; 应该不必做过多解释了吧，看每个文件第一行的 profile=””。 当然，我们也可以在一个配置文件中使用： &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:jee=&quot;http://www.springframework.org/schema/jee&quot; xsi:schemaLocation=&quot;...&quot;&gt; &lt;beans profile=&quot;development&quot;&gt; &lt;jdbc:embedded-database id=&quot;dataSource&quot;&gt; &lt;jdbc:script location=&quot;classpath:com/bank/config/sql/schema.sql&quot;/&gt; &lt;jdbc:script location=&quot;classpath:com/bank/config/sql/test-data.sql&quot;/&gt; &lt;/jdbc:embedded-database&gt; &lt;/beans&gt; &lt;beans profile=&quot;production&quot;&gt; &lt;jee:jndi-lookup id=&quot;dataSource&quot; jndi-name=&quot;java:comp/env/jdbc/datasource&quot;/&gt; &lt;/beans&gt; &lt;/beans&gt; 理解起来也很简单吧。 接下来的问题是，怎么使用特定的 profile 呢？Spring 在启动的过程中，会去寻找 “spring.profiles.active” 的属性值，根据这个属性值来的。那怎么配置这个值呢？ Spring 会在这几个地方寻找 spring.profiles.active 的属性值：操作系统环境变量、JVM 系统变量、web.xml 中定义的参数、JNDI。 最简单的方式莫过于在程序启动的时候指定： -Dspring.profiles.active=&quot;profile1,profile2&quot; profile 可以激活多个 当然，我们也可以通过代码的形式从 Environment 中设置 profile： AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(); ctx.getEnvironment().setActiveProfiles(&quot;development&quot;); ctx.register(SomeConfig.class, StandaloneDataConfig.class, JndiDataConfig.class); ctx.refresh(); // 重启 如果是 Spring Boot 的话更简单，我们一般会创建 application.properties、application-dev.properties、application-prod.properties 等文件，其中 application.properties 配置各个环境通用的配置，application-{profile}.properties 中配置特定环境的配置，然后在启动的时候指定 profile： java -Dspring.profiles.active=prod -jar JavaDoop.jar 如果是单元测试中使用的话，在测试类中使用 @ActiveProfiles 指定，这里就不展开了。 工厂模式生成 Bean请读者注意 factory-bean 和 FactoryBean 的区别。这节说的是前者，是说静态工厂或实例工厂，而后者是 Spring 中的特殊接口，代表一类特殊的 Bean，附录的下面一节会介绍 FactoryBean。 设计模式里，工厂方法模式分静态工厂和实例工厂，我们分别看看 Spring 中怎么配置这两个，来个代码示例就什么都清楚了。 静态工厂： &lt;bean id=&quot;clientService&quot; class=&quot;examples.ClientService&quot; factory-method=&quot;createInstance&quot;/&gt; public class ClientService &#123; private static ClientService clientService = new ClientService(); private ClientService() &#123;&#125; // 静态方法 public static ClientService createInstance() &#123; return clientService; &#125; &#125; 实例工厂： &lt;bean id=&quot;serviceLocator&quot; class=&quot;examples.DefaultServiceLocator&quot;&gt; &lt;!-- inject any dependencies required by this locator bean --&gt; &lt;/bean&gt; &lt;bean id=&quot;clientService&quot; factory-bean=&quot;serviceLocator&quot; factory-method=&quot;createClientServiceInstance&quot;/&gt; &lt;bean id=&quot;accountService&quot; factory-bean=&quot;serviceLocator&quot; factory-method=&quot;createAccountServiceInstance&quot;/&gt; public class DefaultServiceLocator &#123; private static ClientService clientService = new ClientServiceImpl(); private static AccountService accountService = new AccountServiceImpl(); public ClientService createClientServiceInstance() &#123; return clientService; &#125; public AccountService createAccountServiceInstance() &#123; return accountService; &#125; &#125; FactoryBeanFactoryBean 适用于 Bean 的创建过程比较复杂的场景，比如数据库连接池的创建。 public interface FactoryBean&lt;T&gt; &#123; T getObject() throws Exception; Class&lt;T&gt; getObjectType(); boolean isSingleton(); &#125; public class Person &#123; private Car car ; private void setCar(Car car)&#123; this.car = car; &#125; &#125; 我们假设现在需要创建一个 Person 的 Bean，首先我们需要一个 Car 的实例，我们这里假设 Car 的实例创建很麻烦，那么我们可以把创建 Car 的复杂过程包装起来： public class MyCarFactoryBean implements FactoryBean&lt;Car&gt;&#123; private String make; private int year ; public void setMake(String m)&#123; this.make =m ; &#125; public void setYear(int y)&#123; this.year = y; &#125; public Car getObject()&#123; // 这里我们假设 Car 的实例化过程非常复杂，反正就不是几行代码可以写完的那种 CarBuilder cb = CarBuilder.car(); if(year!=0) cb.setYear(this.year); if(StringUtils.hasText(this.make)) cb.setMake( this.make ); return cb.factory(); &#125; public Class&lt;Car&gt; getObjectType() &#123; return Car.class ; &#125; public boolean isSingleton() &#123; returnfalse; &#125; &#125; 我们看看装配的时候是怎么配置的： &lt;bean class = &quot;com.javadoop.MyCarFactoryBean&quot; id = &quot;car&quot;&gt; &lt;property name = &quot;make&quot; value =&quot;Honda&quot;/&gt; &lt;property name = &quot;year&quot; value =&quot;1984&quot;/&gt; &lt;/bean&gt; &lt;bean class = &quot;com.javadoop.Person&quot; id = &quot;josh&quot;&gt; &lt;property name = &quot;car&quot; ref = &quot;car&quot;/&gt; &lt;/bean&gt; 看到不一样了吗？id 为 “car” 的 bean 其实指定的是一个 FactoryBean，不过配置的时候，我们直接让配置 Person 的 Bean 直接依赖于这个 FactoryBean 就可以了。中间的过程 Spring 已经封装好了。 说到这里，我们再来点干货。我们知道，现在还用 xml 配置 Bean 依赖的越来越少了，更多时候，我们可能会采用 java config 的方式来配置，这里有什么不一样呢？ @Configuration public class CarConfiguration &#123; @Bean public MyCarFactoryBean carFactoryBean()&#123; MyCarFactoryBean cfb = new MyCarFactoryBean(); cfb.setMake(&quot;Honda&quot;); cfb.setYear(1984); return cfb; &#125; @Bean public Person aPerson()&#123; Person person = new Person(); // 注意这里的不同 person.setCar(carFactoryBean().getObject()); return person; &#125; &#125; 这个时候，其实我们的思路也很简单，把 MyCarFactoryBean 看成是一个简单的 Bean 就可以了，不必理会什么 FactoryBean，它是不是 FactoryBean 和我们没关系。 初始化 Bean 的回调有以下四种方案： &lt;bean id=&quot;exampleInitBean&quot; class=&quot;examples.ExampleBean&quot; init-method=&quot;init&quot;/&gt; public class AnotherExampleBean implements InitializingBean &#123; public void afterPropertiesSet() &#123; // do some initialization work &#125; &#125; @Bean(initMethod = &quot;init&quot;) public Foo foo() &#123; return new Foo(); &#125; @PostConstruct public void init() &#123; &#125; 销毁 Bean 的回调&lt;bean id=&quot;exampleInitBean&quot; class=&quot;examples.ExampleBean&quot; destroy-method=&quot;cleanup&quot;/&gt; public class AnotherExampleBean implements DisposableBean &#123; public void destroy() &#123; // do some destruction work (like releasing pooled connections) &#125; &#125; @Bean(destroyMethod = &quot;cleanup&quot;) public Bar bar() &#123; return new Bar(); &#125; @PreDestroy public void cleanup() &#123; &#125; ConversionService既然文中说到了这个，顺便提一下好了。 最有用的场景就是，它用来将前端传过来的参数和后端的 controller 方法上的参数进行绑定的时候用。 像前端传过来的字符串、整数要转换为后端的 String、Integer 很容易，但是如果 controller 方法需要的是一个枚举值，或者是 Date 这些非基础类型（含基础类型包装类）值的时候，我们就可以考虑采用 ConversionService 来进行转换。 &lt;bean id=&quot;conversionService&quot; class=&quot;org.springframework.context.support.ConversionServiceFactoryBean&quot;&gt; &lt;property name=&quot;converters&quot;&gt; &lt;list&gt; &lt;bean class=&quot;com.javadoop.learning.utils.StringToEnumConverterFactory&quot;/&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; ConversionService 接口很简单，所以要自定义一个 convert 的话也很简单。 下面再说一个实现这种转换很简单的方式，那就是实现 Converter 接口。 来看一个很简单的例子，这样比什么都管用。 public class StringToDateConverter implements Converter&lt;String, Date&gt; &#123; @Override public Date convert(String source) &#123; try &#123; return DateUtils.parseDate(source, &quot;yyyy-MM-dd&quot;, &quot;yyyy-MM-dd HH:mm:ss&quot;, &quot;yyyy-MM-dd HH:mm&quot;, &quot;HH:mm:ss&quot;, &quot;HH:mm&quot;); &#125; catch (ParseException e) &#123; return null; &#125; &#125; &#125; 只要注册这个 Bean 就可以了。这样，前端往后端传的时间描述字符串就很容易绑定成 Date 类型了，不需要其他任何操作。 Bean 继承在初始化 Bean 的地方，我们说过了这个： RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); 这里涉及到的就是 &lt;bean parent=&quot;&quot; /&gt; 中的 parent 属性，我们来看看 Spring 中是用这个来干什么的。 首先，我们要明白，这里的继承和 java 语法中的继承没有任何关系，不过思路是相通的。child bean 会继承 parent bean 的所有配置，也可以覆盖一些配置，当然也可以新增额外的配置。 Spring 中提供了继承自 AbstractBeanDefinition 的 ChildBeanDefinition 来表示 child bean。 看如下一个例子: &lt;bean id=&quot;inheritedTestBean&quot; abstract=&quot;true&quot; class=&quot;org.springframework.beans.TestBean&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;parent&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;1&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;inheritsWithDifferentClass&quot; class=&quot;org.springframework.beans.DerivedTestBean&quot; parent=&quot;inheritedTestBean&quot; init-method=&quot;initialize&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;override&quot;/&gt; &lt;/bean&gt; parent bean 设置了 abstract=&quot;true&quot; 所以它不会被实例化，child bean 继承了 parent bean 的两个属性，但是对 name 属性进行了覆写。 child bean 会继承 scope、构造器参数值、属性值、init-method、destroy-method 等等。 当然，我不是说 parent bean 中的 abstract = true 在这里是必须的，只是说如果加上了以后 Spring 在实例化 singleton beans 的时候会忽略这个 bean。 比如下面这个极端 parent bean，它没有指定 class，所以毫无疑问，这个 bean 的作用就是用来充当模板用的 parent bean，此处就必须加上 abstract = true。 &lt;bean id=&quot;inheritedTestBeanWithoutClass&quot; abstract=&quot;true&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;parent&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;1&quot;/&gt; &lt;/bean&gt; 方法注入一般来说，我们的应用中大多数的 Bean 都是 singleton 的。singleton 依赖 singleton，或者 prototype 依赖 prototype 都很好解决，直接设置属性依赖就可以了。 但是，如果是 singleton 依赖 prototype 呢？这个时候不能用属性依赖，因为如果用属性依赖的话，我们每次其实拿到的还是第一次初始化时候的 bean。 一种解决方案就是不要用属性依赖，每次获取依赖的 bean 的时候从 BeanFactory 中取。这个也是大家最常用的方式了吧。怎么取，我就不介绍了，大部分 Spring 项目大家都会定义那么个工具类的。 另一种解决方案就是这里要介绍的通过使用 Lookup method。 lookup-method 我们来看一下 Spring Reference 中提供的一个例子： package fiona.apple; // no more Spring imports! public abstract class CommandManager &#123; public Object process(Object commandState) &#123; // grab a new instance of the appropriate Command interface Command command = createCommand(); // set the state on the (hopefully brand new) Command instance command.setState(commandState); return command.execute(); &#125; // okay... but where is the implementation of this method? protected abstract Command createCommand(); &#125; xml 配置 &lt;lookup-method /&gt;： &lt;!-- a stateful bean deployed as a prototype (non-singleton) --&gt; &lt;bean id=&quot;myCommand&quot; class=&quot;fiona.apple.AsyncCommand&quot; scope=&quot;prototype&quot;&gt; &lt;!-- inject dependencies here as required --&gt; &lt;/bean&gt; &lt;!-- commandProcessor uses statefulCommandHelper --&gt; &lt;bean id=&quot;commandManager&quot; class=&quot;fiona.apple.CommandManager&quot;&gt; &lt;lookup-method name=&quot;createCommand&quot; bean=&quot;myCommand&quot;/&gt; &lt;/bean&gt; Spring 采用 CGLIB 生成字节码的方式来生成一个子类。我们定义的类不能定义为 final class，抽象方法上也不能加 final。 lookup-method 上的配置也可以采用注解来完成，这样就可以不用配置 &lt;lookup-method /&gt; 了，其他不变： public abstract class CommandManager &#123; public Object process(Object commandState) &#123; MyCommand command = createCommand(); command.setState(commandState); return command.execute(); &#125; @Lookup(&quot;myCommand&quot;) protected abstract Command createCommand(); &#125; 注意，既然用了注解，要配置注解扫描：&lt;context:component-scan base-package=&quot;com.javadoop&quot; /&gt; 甚至，我们可以像下面这样： public abstract class CommandManager &#123; public Object process(Object commandState) &#123; MyCommand command = createCommand(); command.setState(commandState); return command.execute(); &#125; @Lookup protected abstract MyCommand createCommand(); &#125; 上面的返回值用了 MyCommand，当然，如果 Command 只有一个实现类，那返回值也可以写 Command。 replaced-method 记住它的功能，就是替换掉 bean 中的一些方法。 public class MyValueCalculator &#123; public String computeValue(String input) &#123; // some real code... &#125; // some other methods... &#125; 方法覆写，注意要实现 MethodReplacer 接口： public class ReplacementComputeValue implements org.springframework.beans.factory.support.MethodReplacer &#123; public Object reimplement(Object o, Method m, Object[] args) throws Throwable &#123; // get the input value, work with it, and return a computed result String input = (String) args[0]; ... return ...; &#125; &#125; 配置也很简单： &lt;bean id=&quot;myValueCalculator&quot; class=&quot;x.y.z.MyValueCalculator&quot;&gt; &lt;!-- 定义 computeValue 这个方法要被替换掉 --&gt; &lt;replaced-method name=&quot;computeValue&quot; replacer=&quot;replacementComputeValue&quot;&gt; &lt;arg-type&gt;String&lt;/arg-type&gt; &lt;/replaced-method&gt; &lt;/bean&gt; &lt;bean id=&quot;replacementComputeValue&quot; class=&quot;a.b.c.ReplacementComputeValue&quot;/&gt; arg-type 明显不是必须的，除非存在方法重载，这样必须通过参数类型列表来判断这里要覆盖哪个方法。 BeanPostProcessor应该说 BeanPostProcessor 概念在 Spring 中也是比较重要的。我们看下接口定义： public interface BeanPostProcessor &#123; Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException; &#125; 看这个接口中的两个方法名字我们大体上可以猜测 bean 在初始化之前会执行 postProcessBeforeInitialization 这个方法，初始化完成之后会执行 postProcessAfterInitialization 这个方法。但是，这么理解是非常片面的。 首先，我们要明白，除了我们自己定义的 BeanPostProcessor 实现外，Spring 容器在启动时自动给我们也加了几个。如在获取 BeanFactory 的 obtainFactory() 方法结束后的 prepareBeanFactory(factory)，大家仔细看会发现，Spring 往容器中添加了这两个 BeanPostProcessor：ApplicationContextAwareProcessor、ApplicationListenerDetector。 我们回到这个接口本身，读者请看第一个方法，这个方法接受的第一个参数是 bean 实例，第二个参数是 bean 的名字，重点在返回值将会作为新的 bean 实例，所以，没事的话这里不能随便返回个 null。 那意味着什么呢？我们很容易想到的就是，我们这里可以对一些我们想要修饰的 bean 实例做一些事情。但是对于 Spring 框架来说，它会决定是不是要在这个方法中返回 bean 实例的代理，这样就有更大的想象空间了。 最后，我们说说如果我们自己定义一个 bean 实现 BeanPostProcessor 的话，它的执行时机是什么时候？ 如果仔细看了代码分析的话，其实很容易知道了，在 bean 实例化完成、属性注入完成之后，会执行回调方法，具体请参见类 AbstractAutowireCapableBeanFactory#initBean 方法。 首先会回调几个实现了 Aware 接口的 bean，然后就开始回调 BeanPostProcessor 的 postProcessBeforeInitialization 方法，之后是回调 init-method，然后再回调 BeanPostProcessor 的 postProcessAfterInitialization 方法。 总结按理说，总结应该写在附录前面，我就不讲究了。 在花了那么多时间后，这篇文章终于算是基本写完了，大家在惊叹 Spring 给我们做了那么多的事的时候，应该透过现象看本质，去理解 Spring 写得好的地方，去理解它的设计思想。 本文的缺陷在于对 Spring 预初始化 singleton beans 的过程分析不够，主要是代码量真的比较大，分支旁路众多。同时，虽然附录条目不少，但是庞大的 Spring 真的引出了很多的概念，希望日后有精力可以慢慢补充一些。 （全文完） 转载自： https://juejin.cn/post/6844903694039793672","categories":[{"name":"Spring Core","slug":"Spring-Core","permalink":"https://blog.fenxiangz.com/categories/Spring-Core/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://blog.fenxiangz.com/tags/Spring/"},{"name":"Spring Core","slug":"Spring-Core","permalink":"https://blog.fenxiangz.com/tags/Spring-Core/"},{"name":"源码分析","slug":"源码分析","permalink":"https://blog.fenxiangz.com/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]},{"title":"JUC 概览","slug":"java/advance/2020-05-02_java_juc","date":"2020-05-02T00:00:00.000Z","updated":"2020-12-20T16:47:02.962Z","comments":true,"path":"post/java/advance/2020-05-02_java_juc.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2020-05-02_java_juc.html","excerpt":"","text":"在Java中，线程部分是一个重点，本篇文章说的JUC也是关于线程的。JUC就是java.util .concurrent工具包的简称。这是一个处理线程的工具包，JDK 1.5开始出现的。下面一起来看看它怎么使用。 一、volatile关键字与内存可见性1、内存可见性： 先来看看下面的一段代码： public class TestVolatile &#123; public static void main(String[] args)&#123; //这个线程是用来读取flag的值的 ThreadDemo threadDemo = new ThreadDemo(); Thread thread = new Thread(threadDemo); thread.start(); while (true)&#123; if (threadDemo.isFlag())&#123; System.out.println(&quot;主线程读取到的flag = &quot; + threadDemo.isFlag()); break; &#125; &#125; &#125; &#125; @Data class ThreadDemo implements Runnable&#123; //这个线程是用来修改flag的值的 public boolean flag = false; @Override public void run() &#123; try &#123; Thread.sleep(200); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; flag = true; System.out.println(&quot;ThreadDemo线程修改后的flag = &quot; + isFlag()); &#125; &#125; 这段代码很简单，就是一个ThreadDemo类继承Runnable创建一个线程。它有一个成员变量flag为false，然后重写run方法，在run方法里面将flag改为true，同时还有一条输出语句。然后就是main方法主线程去读取flag。如果flag为true，就会break掉while循环，否则就是死循环。按道理，下面那个线程将flag改为true了，主线程读取到的应该也是true，循环应该会结束。看看运行结果： 从图中可以看到，该程序并没有结束，也就是死循环。说明主线程读取到的flag还是false，可是另一个线程明明将flag改为true了，而且打印出来了，这是什么原因呢？这就是内存可见性问题。 内存可见性问题：当多个线程操作共享数据时，彼此不可见。 看下图理解上述代码： 要解决这个问题，可以加锁。如下： while (true)&#123; synchronized (threadDemo)&#123; if (threadDemo.isFlag())&#123; System.out.println(&quot;主线程读取到的flag = &quot; + threadDemo.isFlag()); break; &#125; &#125; &#125; 加了锁，就可以让while循环每次都从主存中去读取数据，这样就能读取到true了。但是一加锁，每次只能有一个线程访问，当一个线程持有锁时，其他的就会阻塞，效率就非常低了。不想加锁，又要解决内存可见性问题，那么就可以使用volatile关键字。 2、volatile关键字： 用法： volatile关键字：当多个线程操作共享数据时，可以保证内存中的数据可见。用这个关键字修饰共享数据，就会及时的把线程缓存中的数据刷新到主存中去，也可以理解为，就是直接操作主存中的数据。所以在不使用锁的情况下，可以使用volatile。如下： public volatile boolean flag = false; 这样就可以解决内存可见性问题了。 volatile和synchronized的区别： volatile不具备互斥性(当一个线程持有锁时，其他线程进不来，这就是互斥性)。 volatile不具备原子性。 二、原子性1、理解原子性： 上面说到volatile不具备原子性，那么原子性到底是什么呢？先看如下代码： public class TestIcon &#123; public static void main(String[] args)&#123; AtomicDemo atomicDemo = new AtomicDemo(); for (int x = 0;x &lt; 10; x++)&#123; new Thread(atomicDemo).start(); &#125; &#125; &#125; class AtomicDemo implements Runnable&#123; private int i = 0; public int getI()&#123; return i++; &#125; @Override public void run() &#123; try &#123; Thread.sleep(200); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(getI()); &#125; &#125; 这段代码就是在run方法里面让i++，然后启动十个线程去访问。看看结果： 可以发现，出现了重复数据。明显产生了多线程安全问题，或者说原子性问题。所谓原子性就是操作不可再细分，而i++操作分为读改写三步，如下： int temp = i; i = i+1; i = temp; 所以i++明显不是原子操作。上面10个线程进行i++时，内存图解如下： 看到这里，好像和上面的内存可见性问题一样。是不是加个volatile关键字就可以了呢？其实不是的，因为加了volatile，只是相当于所有线程都是在主存中操作数据而已，但是不具备互斥性。比如两个线程同时读取主存中的0，然后又同时自增，同时写入主存，结果还是会出现重复数据。 2、原子变量： JDK 1.5之后，Java提供了原子变量，在java.util.concurrent.atomic包下。原子变量具备如下特点： 有volatile保证内存可见性。 用CAS算法保证原子性。 3、CAS算法： CAS算法是计算机硬件对并发操作共享数据的支持，CAS包含3个操作数： 内存值V 预估值A 更新值B 当且仅当V==B时，才会把B的值赋给V，即V = B，否则不做任何操作。就上面的i++问题，CAS算法是这样处理的：首先V是主存中的值0，然后预估值A也是0，因为此时还没有任何操作，这时V=B，所以进行自增，同时把主存中的值变为1。如果第二个线程读取到主存中的还是0也没关系，因为此时预估值已经变成1，V不等于B，所以不进行任何操作。 4、使用原子变量改进i++问题： 原子变量用法和包装类差不多，如下： //private int i = 0; AtomicInteger i = new AtomicInteger(); public int getI()&#123; return i.getAndIncrement(); &#125; 只改这两处即可。 三、锁分段机制JDK 1.5之后，在java.util.concurrent包中提供了多种并发容器类来改进同步容器类的性能。其中最主要的就是ConcurrentHashMap。 1、ConcurrentHashMap： ConcurrentHashMap就是一个线程安全的hash表。我们知道HashMap是线程不安全的，Hash Table加了锁，是线程安全的，因此它效率低。HashTable加锁就是将整个hash表锁起来，当有多个线程访问时，同一时间只能有一个线程访问，并行变成串行，因此效率低。所以JDK1.5后提供了ConcurrentHashMap，它采用了锁分段机制。 如上图所示，ConcurrentHashMap默认分成了16个segment，每个Segment都对应一个Hash表，且都有独立的锁。所以这样就可以每个线程访问一个Segment，就可以并行访问了，从而提高了效率。这就是锁分段。但是，java 8 又更新了，不再采用锁分段机制，也采用CAS算法了。 2、用法: java.util.concurrent包还提供了设计用于多线程上下文中的 Collection 实现： ConcurrentHashMap、ConcurrentSkipListMap、ConcurrentSkipListSet、CopyOnWriteArrayList 和 CopyOnWriteArraySet。当期望许多线程访问一个给 定 collection 时，ConcurrentHashMap 通常优于同步的 HashMap， ConcurrentSkipListMap 通常优于同步的 TreeMap。当期望的读数和遍历远远 大于列表的更新数时，CopyOnWriteArrayList 优于同步的 ArrayList。下面看看部分用法： public class TestConcurrent &#123; public static void main(String[] args)&#123; ThreadDemo2 threadDemo2 = new ThreadDemo2(); for (int i=0;i&lt;10;i++)&#123; new Thread(threadDemo2).start(); &#125; &#125; &#125; //10个线程同时访问 class ThreadDemo2 implements Runnable&#123; private static List&lt;String&gt; list = Collections.synchronizedList(new ArrayList&lt;&gt;());//普通做法 static &#123; list.add(&quot;aaa&quot;); list.add(&quot;bbb&quot;); list.add(&quot;ccc&quot;); &#125; @Override public void run() &#123; Iterator&lt;String&gt; iterator = list.iterator(); while (iterator.hasNext())&#123; System.out.println(iterator.next());//读 list.add(&quot;ddd&quot;);//写 &#125; &#125; &#125; 10个线程并发访问这个集合，读取集合数据的同时再往集合中添加数据。运行这段代码会报错，并发修改异常。 将创建集合方式改成： private static CopyOnWriteArrayList&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); 这样就不会有并发修改异常了。因为这个是写入并复制，每次生成新的，所以如果添加操作比较多的话，开销非常大，适合迭代操作比较多的时候使用。 四、闭锁java.util.concurrent包中提供了多种并发容器类来改进同步容器的性能。ContDownLatch是一个同步辅助类，在完成某些运算时，只有其他所有线程的运算全部完成，当前运算才继续执行，这就叫闭锁。看下面代码： public class TestCountDownLatch &#123; public static void main(String[] args)&#123; LatchDemo ld = new LatchDemo(); long start = System.currentTimeMillis(); for (int i = 0;i&lt;10;i++)&#123; new Thread(ld).start(); &#125; long end = System.currentTimeMillis(); System.out.println(&quot;耗费时间为：&quot;+(end - start)+&quot;秒&quot;); &#125; &#125; class LatchDemo implements Runnable&#123; private CountDownLatch latch; public LatchDemo()&#123; &#125; @Override public void run() &#123; for (int i = 0;i&lt;5000;i++)&#123; if (i % 2 == 0)&#123;//50000以内的偶数 System.out.println(i); &#125; &#125; &#125; &#125; 这段代码就是10个线程同时去输出5000以内的偶数，然后在主线程那里计算执行时间。其实这是计算不了那10个线程的执行时间的，因为主线程与这10个线程也是同时执行的，可能那10个线程才执行到一半，主线程就已经输出“耗费时间为x秒”这句话了。所有要想计算这10个线程执行的时间，就得让主线程先等待，等10个分线程都执行完了才能执行主线程。这就要用到闭锁。看如何使用： public class TestCountDownLatch &#123; public static void main(String[] args) &#123; final CountDownLatch latch = new CountDownLatch(10);//有多少个线程这个参数就是几 LatchDemo ld = new LatchDemo(latch); long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10; i++) &#123; new Thread(ld).start(); &#125; try &#123; latch.await();//这10个线程执行完之前先等待 &#125; catch (InterruptedException e) &#123; &#125; long end = System.currentTimeMillis(); System.out.println(&quot;耗费时间为：&quot; + (end - start)); &#125; &#125; class LatchDemo implements Runnable &#123; private CountDownLatch latch; public LatchDemo(CountDownLatch latch) &#123; this.latch = latch; &#125; @Override public void run() &#123; synchronized (this) &#123; try &#123; for (int i = 0; i &lt; 50000; i++) &#123; if (i % 2 == 0) &#123;//50000以内的偶数 System.out.println(i); &#125; &#125; &#125; finally &#123; latch.countDown();//每执行完一个就递减一个 &#125; &#125; &#125; &#125; 如上代码，主要就是用latch.countDown()和latch.await()实现闭锁，详细请看上面注释即可。 五、创建线程的方式 — 实现Callable接口直接看代码： public class TestCallable &#123; public static void main(String[] args)&#123; CallableDemo callableDemo = new CallableDemo(); //执行callable方式，需要FutureTask实现类的支持，用来接收运算结果 FutureTask&lt;Integer&gt; result = new FutureTask&lt;&gt;(callableDemo); new Thread(result).start(); //接收线程运算结果 try &#123; Integer sum = result.get();//当上面的线程执行完后，才会打印结果。跟闭锁一样。所有futureTask也可以用于闭锁 System.out.println(sum); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; class CallableDemo implements Callable&lt;Integer&gt;&#123; @Override public Integer call() throws Exception &#123; int sum = 0; for (int i = 0;i&lt;=100;i++)&#123; sum += i; &#125; return sum; &#125; &#125; 现在Callable接口和实现Runable接口的区别就是，Callable带泛型，其call方法有返回值。使用的时候，需要用FutureTask来接收返回值。而且它也要等到线程执行完调用get方法才会执行，也可以用于闭锁操作。 六、Lock同步锁在JDK1.5之前，解决多线程安全问题有两种方式(sychronized隐式锁)： 同步代码块 同步方法 在JDK1.5之后，出现了更加灵活的方式(Lock显式锁)： 同步锁 Lock需要通过lock()方法上锁，通过unlock()方法释放锁。为了保证锁能释放，所有unlock方法一般放在finally中去执行。 再来看一下卖票案例： public class TestLock &#123; public static void main(String[] args) &#123; Ticket td = new Ticket(); new Thread(td, &quot;窗口1&quot;).start(); new Thread(td, &quot;窗口2&quot;).start(); new Thread(td, &quot;窗口3&quot;).start(); &#125; &#125; class Ticket implements Runnable &#123; private int ticket = 100; @Override public void run() &#123; while (true) &#123; if (ticket &gt; 0) &#123; try &#123; Thread.sleep(200); &#125; catch (Exception e) &#123; &#125; System.out.println(Thread.currentThread().getName() + &quot;完成售票，余票为：&quot; + (--ticket)); &#125; &#125; &#125; &#125; 多个线程同时操作共享数据ticket，所以会出现线程安全问题。会出现同一张票卖了好几次或者票数为负数的情况。以前用同步代码块和同步方法解决，现在看看用同步锁怎么解决。 class Ticket implements Runnable &#123; private Lock lock = new ReentrantLock();//创建lock锁 private int ticket = 100; @Override public void run() &#123; while (true) &#123; lock.lock();//上锁 try &#123; if (ticket &gt; 0) &#123; try &#123; Thread.sleep(200); &#125; catch (Exception e) &#123; &#125; System.out.println(Thread.currentThread().getName() + &quot;完成售票，余票为：&quot; + (--ticket)); &#125; &#125;finally &#123; lock.unlock();//释放锁 &#125; &#125; &#125; &#125; 直接创建lock对象，然后用lock()方法上锁，最后用unlock()方法释放锁即可。 七、等待唤醒机制1、虚假唤醒问题： 生产消费模式是等待唤醒机制的一个经典案例，看下面的代码： public class TestProductorAndconsumer &#123; public static void main(String[] args)&#123; Clerk clerk = new Clerk(); Productor productor = new Productor(clerk); Consumer consumer = new Consumer(clerk); new Thread(productor,&quot;生产者A&quot;).start(); new Thread(consumer,&quot;消费者B&quot;).start(); &#125; &#125; //店员 class Clerk&#123; private int product = 0;//共享数据 public synchronized void get()&#123; //进货 if(product &gt;= 10)&#123; System.out.println(&quot;产品已满&quot;); &#125;else &#123; System.out.println(Thread.currentThread().getName()+&quot;:&quot;+ (++product)); &#125; &#125; public synchronized void sell()&#123;//卖货 if (product &lt;= 0)&#123; System.out.println(&quot;缺货&quot;); &#125;else &#123; System.out.println(Thread.currentThread().getName()+&quot;:&quot;+ (--product)); &#125; &#125; &#125; //生产者 class Productor implements Runnable&#123; private Clerk clerk; public Productor(Clerk clerk)&#123; this.clerk = clerk; &#125; @Override public void run() &#123; for (int i = 0;i&lt;20;i++)&#123; clerk.get(); &#125; &#125; &#125; //消费者 class Consumer implements Runnable&#123; private Clerk clerk; public Consumer(Clerk clerk)&#123; this.clerk = clerk; &#125; @Override public void run() &#123; for (int i = 0;i&lt;20;i++)&#123; clerk.sell(); &#125; &#125; &#125; 这就是生产消费模式的案例，这里没有使用等待唤醒机制，运行结果就是即使是缺货状态，它也会不断的去消费，也会一直打印“缺货”，即使是产品已满状态，也会不断地进货。用等待唤醒机制改进： //店员 class Clerk&#123; private int product = 0;//共享数据 public synchronized void get()&#123; //进货 if(product &gt;= 10)&#123; System.out.println(&quot;产品已满&quot;); try &#123; this.wait();//满了就等待 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;else &#123; System.out.println(Thread.currentThread().getName()+&quot;:&quot;+ (++product)); this.notifyAll();//没满就可以进货 &#125; &#125; public synchronized void sell()&#123;//卖货 if (product &lt;= 0)&#123; System.out.println(&quot;缺货&quot;); try &#123; this.wait();//缺货就等待 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;else &#123; System.out.println(Thread.currentThread().getName()+&quot;:&quot;+ (--product)); this.notifyAll();//不缺货就可以卖 &#125; &#125; &#125; 这样就不会出现上述问题了。没有的时候就生产，生产满了就通知消费，消费完了再通知生产。但是这样还是有点问题，将上述代码做如下改动： if(product &gt;= 1)&#123; //把原来的10改成1 System.out.println(&quot;产品已满&quot;); ...... public void run() &#123; try &#123; Thread.sleep(200);//睡0.2秒 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; for (int i = 0;i&lt;20;i++)&#123; clerk.sell(); &#125; &#125; 就做这两处修改，再次运行，发现虽然结果没问题，但是程序却一直没停下来。出现这种情况是因为有一个线程在等待，而另一个线程没有执行机会了，唤醒不了这个等待的线程了，所以程序就无法结束。解决办法就是把get和sell方法里面的else去掉，不要用else包起来。但是，即使这样，如果再多加两个线程，就会出现负数了。 new Thread(productor, &quot;生产者C&quot;).start(); new Thread(consumer, &quot;消费者D&quot;).start(); 运行结果： 一个消费者线程抢到执行权，发现product是0，就等待，这个时候，另一个消费者又抢到了执行权，product是0，还是等待，此时两个消费者线程在同一处等待。然后当生产者生产了一个product后，就会唤醒两个消费者，发现product是1，同时消费，结果就出现了0和-1。这就是虚假唤醒。解决办法就是把if判断改成while。如下： public synchronized void get() &#123; //进货 while (product &gt;= 1) &#123; System.out.println(&quot;产品已满&quot;); try &#123; this.wait();//满了就等待 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + (++product)); this.notifyAll();//没满就可以进货 &#125; public synchronized void sell() &#123;//卖货 while (product &lt;= 0) &#123;//为了避免虚假唤醒问题，wait方法应该总是在循环中使用 System.out.println(&quot;缺货&quot;); try &#123; this.wait();//缺货就等待 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + (--product)); this.notifyAll();//不缺货就可以卖 &#125; 只需要把if改成while，每次都再去判断一下，就可以了。 2、用Lock锁实现等待唤醒： class Clerk &#123; private int product = 0;//共享数据 private Lock lock = new ReentrantLock();//创建锁对象 private Condition condition = lock.newCondition();//获取condition实例 public void get() &#123; //进货 lock.lock();//上锁 try &#123; while (product &gt;= 1) &#123; System.out.println(&quot;产品已满&quot;); try &#123; condition.await();//满了就等待 &#125; catch (InterruptedException e) &#123; &#125; &#125; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + (++product)); condition.signalAll();//没满就可以进货 &#125;finally &#123; lock.unlock();//释放锁 &#125; &#125; public void sell() &#123;//卖货 lock.lock();//上锁 try &#123; while (product &lt;= 0) &#123; System.out.println(&quot;缺货&quot;); try &#123; condition.await();//缺货就等待 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + (--product)); condition.signalAll();//不缺货就可以卖 &#125;finally &#123; lock.unlock();//释放锁 &#125; &#125; &#125; 使用lock同步锁，就不需要sychronized关键字了，需要创建lock对象和condition实例。condition的await()方法、signal()方法和signalAll()方法分别与wait()方法、notify()方法和notifyAll()方法对应。 3、线程按序交替： 首先来看一道题： 编写一个程序，开启 3 个线程，这三个线程的 ID 分别为 A、B、C， 每个线程将自己的 ID 在屏幕上打印 10 遍，要求输出的结果必须按顺序显示。 如：ABCABCABC…… 依次递归 分析： 线程本来是抢占式进行的，要按序交替，所以必须实现线程通信， 那就要用到等待唤醒。可以使用同步方法，也可以用同步锁。 编码实现： public class TestLoopPrint &#123; public static void main(String[] args) &#123; AlternationDemo ad = new AlternationDemo(); new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; ad.loopA(); &#125; &#125; &#125;, &quot;A&quot;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; ad.loopB(); &#125; &#125; &#125;, &quot;B&quot;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; ad.loopC(); &#125; &#125; &#125;, &quot;C&quot;).start(); &#125; &#125; class AlternationDemo &#123; private int number = 1;//当前正在执行的线程的标记 private Lock lock = new ReentrantLock(); Condition condition1 = lock.newCondition(); Condition condition2 = lock.newCondition(); Condition condition3 = lock.newCondition(); public void loopA() &#123; lock.lock(); try &#123; if (number != 1) &#123; //判断 condition1.await(); &#125; System.out.println(Thread.currentThread().getName());//打印 number = 2; condition2.signal(); &#125; catch (Exception e) &#123; &#125; finally &#123; lock.unlock(); &#125; &#125; public void loopB() &#123; lock.lock(); try &#123; if (number != 2) &#123; //判断 condition2.await(); &#125; System.out.println(Thread.currentThread().getName());//打印 number = 3; condition3.signal(); &#125; catch (Exception e) &#123; &#125; finally &#123; lock.unlock(); &#125; &#125; public void loopC() &#123; lock.lock(); try &#123; if (number != 3) &#123; //判断 condition3.await(); &#125; System.out.println(Thread.currentThread().getName());//打印 number = 1; condition1.signal(); &#125; catch (Exception e) &#123; &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; 以上编码就满足需求。创建三个线程，分别调用loopA、loopB和loopC方法，这三个线程使用condition进行通信。 八、ReadWriterLock读写锁我们在读数据的时候，可以多个线程同时读，不会出现问题，但是写数据的时候，如果多个线程同时写数据，那么到底是写入哪个线程的数据呢？所以，如果有两个线程，写写/读写需要互斥，读读不需要互斥。这个时候可以用读写锁。看例子： public class TestReadWriterLock &#123; public static void main(String[] args)&#123; ReadWriterLockDemo rw = new ReadWriterLockDemo(); new Thread(new Runnable() &#123;//一个线程写 @Override public void run() &#123; rw.set((int)Math.random()*101); &#125; &#125;,&quot;write:&quot;).start(); for (int i = 0;i&lt;100;i++)&#123;//100个线程读 Runnable runnable = () -&gt; rw.get(); Thread thread = new Thread(runnable); thread.start(); &#125; &#125; &#125; class ReadWriterLockDemo&#123; private int number = 0; private ReadWriteLock readWriteLock = new ReentrantReadWriteLock(); //读(可以多个线程同时操作) public void get()&#123; readWriteLock.readLock().lock();//上锁 try &#123; System.out.println(Thread.currentThread().getName()+&quot;:&quot;+number); &#125;finally &#123; readWriteLock.readLock().unlock();//释放锁 &#125; &#125; //写(一次只能有一个线程操作) public void set(int number)&#123; readWriteLock.writeLock().lock(); try &#123; System.out.println(Thread.currentThread().getName()); this.number = number; &#125;finally &#123; readWriteLock.writeLock().unlock(); &#125; &#125; &#125; 这个就是读写锁的用法。上面的代码实现了一个线程写，一百个线程同时读的操作。 九、线程池我们使用线程时，需要new一个，用完了又要销毁，这样频繁的创建销毁也很耗资源，所以就提供了线程池。道理和连接池差不多，连接池是为了避免频繁的创建和释放连接，所以在连接池中就有一定数量的连接，要用时从连接池拿出，用完归还给连接池。线程池也一样。线程池中有一个线程队列，里面保存着所有等待状态的线程。下面来看一下用法： public class TestThreadPool &#123; public static void main(String[] args) &#123; ThreadPoolDemo tp = new ThreadPoolDemo(); //1.创建线程池 ExecutorService pool = Executors.newFixedThreadPool(5); //2.为线程池中的线程分配任务 pool.submit(tp); //3.关闭线程池 pool.shutdown(); &#125; &#125; class ThreadPoolDemo implements Runnable &#123; private int i = 0; @Override public void run() &#123; while (i &lt; 100) &#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + (i++)); &#125; &#125; &#125; 线程池用法很简单，分为三步。首先用工具类Executors创建线程池，然后给线程池分配任务，最后关闭线程池就行了。 总结以上为本文全部内容，涉及到了JUC的大部分内容。 本人也是初次接触，如有错误，希望大佬指点一二！ 原文：https://www.jianshu.com/p/1f19835e05c0","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"并发编程","slug":"并发编程","permalink":"https://blog.fenxiangz.com/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"JUC","slug":"JUC","permalink":"https://blog.fenxiangz.com/tags/JUC/"}]},{"title":"接口性能优化思路","slug":"java/advance/2020-04-25_java_optimize","date":"2020-04-25T00:00:00.000Z","updated":"2020-12-20T16:47:02.961Z","comments":true,"path":"post/java/advance/2020-04-25_java_optimize.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2020-04-25_java_optimize.html","excerpt":"","text":"1. 数据库查询是否有问题，是不是慢查？索引？数据量？2. 中间件：缓存，命中率？3. 代码：过长，多层循环？4. 多线程并发 开多少线程 如何控制线程数量 线程安全问题 Executors 慎用，内部无界队列；","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"性能优化","slug":"性能优化","permalink":"https://blog.fenxiangz.com/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"}]},{"title":"使用 Tampermonkey 在【弹琴吧】非会员扒谱技巧","slug":"其他/2020-01-29_弹琴吧非会员扒谱技巧","date":"2020-01-29T00:00:00.000Z","updated":"2020-12-20T16:47:02.988Z","comments":true,"path":"post/其他/2020-01-29_弹琴吧非会员扒谱技巧.html","link":"","permalink":"https://blog.fenxiangz.com/post/%E5%85%B6%E4%BB%96/2020-01-29_%E5%BC%B9%E7%90%B4%E5%90%A7%E9%9D%9E%E4%BC%9A%E5%91%98%E6%89%92%E8%B0%B1%E6%8A%80%E5%B7%A7.html","excerpt":"","text":"分享一个最近发现的技巧，我在一个吉他谱网站找谱子，发现要会员权限，需要付费。 这样： 通过网页源码查看曲谱地址： 因为非会员，只加载了一张图片，但是观察发现图片资源有后缀 “1_1.png”，所以猜测后面的是“1_2.png”，“1_3.png”，等等。所以自己组装URL测试了一下： http://oss.tan8.com/jtpnew/13/55513/44ec76f75eaf80d632e21abb91a6041cimage_1_1.png http://oss.tan8.com/jtpnew/13/55513/44ec76f75eaf80d632e21abb91a6041cimage_1_2.png …… 结果是正确的。 于是产生了如下js代码来处理页面： // ==UserScript== // @name 弹琴吧-扒谱 // @namespace http://tampermonkey.net/ // @version 0.1 // @description try to take over the world! // @author You // @match http://www.77music.com/jitapu-*.html // @match http://www.tan8.com/jitapu-*.html // @require http://code.jquery.com/jquery-1.11.0.min.js // @grant GM_xmlhttpRequest // @grant GM_download // @grant none // ==/UserScript== (function() &#123; &#39;use strict&#39;; var url = $(&quot;#img1&quot;).find(&quot;img&quot;).attr(&quot;src&quot;); if(url)&#123; //分析url，目标格式：http://oss.tan8.com/jtpnew/13/55513/44ec76f75eaf80d632e21abb91a6041cimage_1_1.png var index = &quot;image_&quot;; var codeStart = url.indexOf(index) + index.length; var codeEnd = url.indexOf(&quot;.png&quot;); var urlA = url.substring(0,codeStart); var urlB = url.substring(codeEnd); var code = url.substring(codeStart, codeEnd);//拿到“1_1” var aa = code.split(&quot;_&quot;); var bb = aa[1];//拿到 “1_1” 后半部分自增序列起始值 var html = &quot;&quot;; for(var i = 0; i &lt; 20; i++)&#123; var t = urlA + aa[0] + &quot;_&quot; + (parseInt(bb) + i) + urlB;//组装url html += (&quot;&lt;img src=&#39;&quot;+t+&quot;&#39; style=&#39;float:left;display: block;width: auto;height: auto;margin-top: 70px;&#39; /&gt;&quot;); &#125; $(&quot;#headerMenu&quot;).after(html); &#125; $(&quot;#audio_mask&quot;).remove(); $($(&quot;.vspace&quot;)[0]).html(&#39;&lt;audio controls=&quot;&quot; style=&quot;width: 30%;height: 100%;&quot; src=&quot;&#39;+$(&quot;.audio_box2&quot;).find(&quot;audio&quot;).attr(&#39;src&#39;)+&#39;&quot;&gt;&lt;/audio&gt;&#39;); &#125;)(); 效果： 把谱子图片重新布局了一下，加载到页面上。 总结：由于大多数网站（大部分中小互联网公司）对媒体资源的访问不会做权限限制，所以这个思路可以应用大多数限制了权限但 url 是有规则的场景。","categories":[{"name":"其他/Tampermonkey","slug":"其他-Tampermonkey","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96-Tampermonkey/"}],"tags":[{"name":"Tampermonkey","slug":"Tampermonkey","permalink":"https://blog.fenxiangz.com/tags/Tampermonkey/"},{"name":"弹琴吧","slug":"弹琴吧","permalink":"https://blog.fenxiangz.com/tags/%E5%BC%B9%E7%90%B4%E5%90%A7/"},{"name":"吉他","slug":"吉他","permalink":"https://blog.fenxiangz.com/tags/%E5%90%89%E4%BB%96/"}]},{"title":"【译】Spring Boot 从 classpath 加载文件","slug":"java/spring/2019-12-28_【译】Spring Boot 从 classpath 加载文件","date":"2019-12-28T00:00:00.000Z","updated":"2020-12-20T16:47:02.970Z","comments":true,"path":"post/java/spring/2019-12-28_【译】Spring Boot 从 classpath 加载文件.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/spring/2019-12-28_%E3%80%90%E8%AF%91%E3%80%91Spring%20Boot%20%E4%BB%8E%20classpath%20%E5%8A%A0%E8%BD%BD%E6%96%87%E4%BB%B6.html","excerpt":"","text":"介绍在创建Spring Boot web应用程序时，有时需要从 classpath 加载文件，例如，数据仅作为文件可用的时候。在我的例子中，我使用 MAXMIND GeoLite2 数据库进行地理位置检索。因此，我需要加载文件并创建一个 DatabaseReader 对象，该对象存储在服务器内存中。下面您将找到在WAR和JAR中加载文件的解决方案。 The ResourceLoaderJava本身提供了线程 classLoader 可以用来尝试加载文件，但 Spring 框架提供了更加优雅的方式，比如 ： ResourceLoader。 你只需要在需要的地方注入 ResourceLoader ，然后调用 getResource(“some path”) 方法即可。 从 resource 目录或 classpath 中加载文件 的例子 (WAR)@Service(&quot;geolocationservice&quot;) public class GeoLocationServiceImpl implements GeoLocationService &#123; private static final Logger LOGGER = LoggerFactory.getLogger(GeoLocationServiceImpl.class); private static DatabaseReader reader = null; private ResourceLoader resourceLoader; @Autowired public GeoLocationServiceImpl(ResourceLoader resourceLoader) &#123; this.resourceLoader = resourceLoader; &#125; @PostConstruct public void init() &#123; try &#123; LOGGER.info(&quot;GeoLocationServiceImpl: Trying to load GeoLite2-Country database...&quot;); Resource resource = resourceLoader.getResource(&quot;classpath:GeoLite2-Country.mmdb&quot;); File dbAsFile = resource.getFile(); // Initialize the reader reader = new DatabaseReader .Builder(dbAsFile) .fileMode(Reader.FileMode.MEMORY) .build(); LOGGER.info(&quot;GeoLocationServiceImpl: Database was loaded successfully.&quot;); &#125; catch (IOException | NullPointerException e) &#123; LOGGER.error(&quot;Database reader cound not be initialized. &quot;, e); &#125; &#125; @PreDestroy public void preDestroy() &#123; if (reader != null) &#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; LOGGER.error(&quot;Failed to close the reader.&quot;); &#125; &#125; &#125; &#125; 从 Spring Boot JAR 中加载文件的例子@Service(&quot;geolocationservice&quot;) public class GeoLocationServiceImpl implements GeoLocationService &#123; private static final Logger LOGGER = LoggerFactory.getLogger(GeoLocationServiceImpl.class); private static DatabaseReader reader = null; private ResourceLoader resourceLoader; @Inject public GeoLocationServiceImpl(ResourceLoader resourceLoader) &#123; this.resourceLoader = resourceLoader; &#125; @PostConstruct public void init() &#123; try &#123; LOGGER.info(&quot;GeoLocationServiceImpl: Trying to load GeoLite2-Country database...&quot;); Resource resource = resourceLoader.getResource(&quot;classpath:GeoLite2-Country.mmdb&quot;); InputStream dbAsStream = resource.getInputStream(); // &lt;-- this is the difference // Initialize the reader reader = new DatabaseReader .Builder(dbAsStream) .fileMode(Reader.FileMode.MEMORY) .build(); LOGGER.info(&quot;GeoLocationServiceImpl: Database was loaded successfully.&quot;); &#125; catch (IOException | NullPointerException e) &#123; LOGGER.error(&quot;Database reader cound not be initialized. &quot;, e); &#125; &#125; @PreDestroy public void preDestroy() &#123; if (reader != null) &#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; LOGGER.error(&quot;Failed to close the reader.&quot;); &#125; &#125; &#125; &#125; 英文原文：https://smarterco.de/java-load-file-from-classpath-in-spring-boot/?from=timeline","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://blog.fenxiangz.com/categories/Spring-Boot/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://blog.fenxiangz.com/tags/Spring/"},{"name":"文件加载","slug":"文件加载","permalink":"https://blog.fenxiangz.com/tags/%E6%96%87%E4%BB%B6%E5%8A%A0%E8%BD%BD/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://blog.fenxiangz.com/tags/Spring-Boot/"}]},{"title":"八皇后算法解析","slug":"algorithm/八皇后算法解析","date":"2019-11-26T11:29:24.000Z","updated":"2020-12-20T16:47:02.944Z","comments":true,"path":"post/algorithm/八皇后算法解析.html","link":"","permalink":"https://blog.fenxiangz.com/post/algorithm/%E5%85%AB%E7%9A%87%E5%90%8E%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90.html","excerpt":"","text":"今天研究力扣的一道题死活写不出来对应的算法，没办法自己算法基础太差。于是看了下答案，发现使用什么回溯算法，菜鸟表示平时开发期间写的最复杂的程序就是写了两层for循环，已经很牛逼了有木有？这个回溯算法什么鬼？于是乎百度了下，算是了解了回溯算法是什么玩意儿。这里分析一波八皇后算法来加深一下理解。 八皇后算法描述如下：在8×8格的国际象棋上摆放八个皇后，使其不能互相攻击，即任意两个皇后都不能处于同一行、同一列或同一斜线上，问有多少种摆法！ 下面来分析一波，假设此时我们想要在黑色方块位置放置一个皇后： 如果一列一列的放置皇后的话，图中黑色位置能放置一个皇后的合法性条件为： 1、绿色线条经过的方格没有皇后 （不处于同一斜线） 2、红色线条经过的方格没有皇后 （不处于同一行） 3、紫色线条经过的方格没有皇后 （不处于同一斜线）也就是说如果以黑色方块位置为参照原点：（0,0）坐标点，紫色和绿色两个线条分别是斜率为1和-1的两个函数，如下图： 紫色线所代表的函数是：y = -x; 绿色先所代表的函数是：y=x; （横坐标是列，纵坐标为行，注意行从上到下递增） 凡是位于这两条函数线上的位置（点）以及横坐标（说明位于同一行）都不能有皇后。所以假设某一列皇后的位置用行来记录，比如queen[column] = row,意思是第column列的皇后的位置在第row行。 同行的逻辑很好判断，那么我们想要在黑色方块位置放置一个皇后，怎么判断前面几列是否在绿色线条和紫色线条上已经有了皇后呢？思路也很简单： 假设黑色方块的位置为n列，nRow行，假设位于m列的所在的行是否有皇后位于紫色线或者绿色上，那么就符合下面条件： 1234567891011&#x2F;&#x2F;假设此时即将在n列放置一个皇后,n&gt;m]&#x2F;&#x2F;获取m列上皇后所在的行int mRow &#x3D; queen[m]int nRow &#x3D; queen[n]；&#x2F;&#x2F;行的差值int rowDiff &#x3D; nRow - mRow;&#x2F;&#x2F;列的差值int columnDiff &#x3D; n-m; 上面代码中 rowDiff的绝对值等于columnDiff的绝对值的话，说明点位于y=x或者y=-x的函数线上： 就说明此时黑色方块的位置是不能放置皇后的，因为在紫色或者绿色线上已经有了皇后。那么用代码来（currentColumn,curreentRow）是否可以放置皇后的方法如下 12345678910111213141516171819202122232425262728293031 &#x2F;** * 判断当（currentRow,currentColumn)是否可以放置皇后 * @param currentColumn * @return *&#x2F; public boolean isLegal(int currentRow,int currentColumn) &#123; &#x2F;&#x2F;遍历前面几列 for(int preColumn&#x3D;0;preColumn&lt;currentColumn;preColumn++) &#123; int row &#x3D; queen[preColumn]; &#x2F;&#x2F;说明在子preColumn的低currentRow已经有了皇后 if(row&#x3D;&#x3D;currentRow) &#123; return false; &#125; &#x2F;&#x2F;行与行的差值 int rowDiff&#x3D; Math.abs(row -currentRow); &#x2F;&#x2F;列于列的差值 int columnDiff &#x3D; Math.abs(currentColumn-preColumn); &#x2F;&#x2F;说明斜线上有皇后 if(rowDiff&#x3D;&#x3D;columnDiff )&#123; return false; &#125; &#125;&#x2F;&#x2F;end for &#x2F;&#x2F;说明（currentRow,currentColumn)可以摆放。 return true; &#125;&#125; 因为博主是按照一列一列的方式来进行放置的，所以整体思路就是：在当前列逐步尝试每一行是否可以放置皇后，如果有一个可以放置皇后，就继续查看下一列的每一行是否可以放置皇后。所以代码如下： 12345678910111213141516171819202122int queen[] &#x3D; new int[8];int count &#x3D; 0;private void eightQueen(int currentColumn) &#123; &#x2F;&#x2F;这个for循环的目的是尝试讲皇后放在当前列的每一行 for(int row&#x3D;0;row&lt;8;row++) &#123; &#x2F;&#x2F;判断当前列的row行是否能放置皇后 if(isLegal(row,currentColumn)) &#123; &#x2F;&#x2F;放置皇后 queen[currentColumn] &#x3D; row; if(currentColumn!&#x3D;7) &#123; &#x2F;&#x2F;摆放下一列的皇后 eightQueen(currentColumn+1); &#125;else &#123; &#x2F;&#x2F;递归结束，此时row要++了 count++; &#125; &#125; &#125;&#x2F;&#x2F;end for &#125; 需要注意的是当currentColumn==7的时候，说明此时已经完成了一种摆放方法，然后for循环继续执行，去尝试其他摆放方法。 测试一波，一共有92种摆放方法： 123456public static void main(String args[]) &#123; Queen queen &#x3D; new Queen(); queen.eightQueen(0); System.out.println(&quot;总共有 &quot; +queen.count+ &quot; 摆放方法&quot;); &#125; 所以结合八皇后的实现来看看到底什么是回溯算法，看百度百科解释 (rel=undefined)：回溯算法实际上一个类似枚举的搜索尝试过程，主要是&lt;font color#ff00ff&gt;在搜索尝试过程中寻找问题的解，当发现已不满足求解条件时，就“回溯”返回，尝试别的路径。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法 比如八皇后算法来说，我们根据约束条件判断某一列的某一行是否可以放置皇后，如果不可以就继续判断&lt;font color #ff00ff&gt;当前列的下一行是否可以放置皇后.如果可以放置皇后，就继续探寻下一列中可以放置皇后的那个位置。完成一次摆放后。再重新挨个尝试下一个可能的摆放方法。 下面用一个力扣的题 (rel=undefined)再次巩固下回溯算法的应用。该题描述如下： 123456789101112131415161718给定一个无重复元素的数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。candidates 中的数字可以无限制重复被选取。说明：所有数字（包括 target）都是正整数。解集不能包含重复的组合。 示例 1:输入: candidates &#x3D; [2,3,6,7], target &#x3D; 7,所求解集为:[ [7], [2,2,3]]示例 2:输入: candidates &#x3D; [2,3,5], target &#x3D; 8,所求解集为:[ [2,2,2,2], [2,3,3], [3,5]] 做该题的重要条件是无重复的数组，那么问题就很好解了。 首先对数组从大到小排序。这是解题的关键。 然后为了减少不必要的遍历，我们要对原来的数组进行截取： 12345678910111213141516171819List&lt;List&lt;Integer&gt;&gt; res &#x3D; new ArrayList&lt;&gt;(); &#x2F;&#x2F;重要的要大小排列Arrays.sort(candidates);&#x2F;&#x2F;说明原数组中就没有满足target的数if (candidates[0] &gt; target) &#123; return res; &#125;List&lt;Integer&gt; newCandidates&#x3D; new ArrayList&lt;Integer&gt;();int len &#x3D; candidates.length;&#x2F;&#x2F; 取小于target的数 组成一个临时数组for (int i &#x3D; 0; i &lt; len; i++) &#123; int num &#x3D; candidates[i]; if (num &gt; target) &#123; break; &#125; newCandidates.add(num); &#125; &#x2F;&#x2F; end for 通过上面的步骤我们拿到了一个从小到大排列的无重复数组newCandidates，数组中的元素都&lt;=target. 因为数组从小到大排列，所以我们有如下几种情况，以candidates = [2,3,5], target = 8为例： 符合条件的子数组满足条件如下 1、target循环减去一个数，如果能一直减到到差值等于0，那么这个数组成的数组就是一个解,比如[2,2,2,2]; 2、target减去一个数，然后形成了一个新的newTarget=target-num[i],让这个newTarget减去下一个数num[i+1],然后执行步骤1，则又是一个解，比如[2,3,3];（其实步骤1是步骤2的一个特例） 3、target减去一个数，然后形成了一个新的newTarget=target-num[i],让这个newTarget减去下一个数num[i+1]，如果能一直减到到差值等于0说明又是一个解.，比如[3,5]; 如此得到了一个规律，只要是相减之后得到差值=0,就说明就得到一个解。 得到一个新的解之后继续循环数组中的下一个数字，继续执行1,2,3步骤即可。 所以完成的解法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] candidates, int target) &#123; List&lt;List&lt;Integer&gt;&gt; res &#x3D; new ArrayList&lt;&gt;(); &#x2F;&#x2F;重要的要大小排列 Arrays.sort(candidates); List&lt;Integer&gt; temp &#x3D; new ArrayList&lt;Integer&gt;(); if (candidates[0] &gt; target) &#123; return res; &#125; int len &#x3D; candidates.length; &#x2F;&#x2F; 取小于target的数 足证一个临时数组 for (int i &#x3D; 0; i &lt; len; i++) &#123; int num &#x3D; candidates[i]; if (num &gt; target) &#123; break; &#125; temp.add(num); &#125; &#x2F;&#x2F; end for &#x2F;&#x2F; find(res, new ArrayList&lt;&gt;(), temp, target, 0); return res; &#125; public void find(List&lt;List&lt;Integer&gt;&gt; res, List&lt;Integer&gt; tmp, List&lt;Integer&gt; candidates, int target, int start)&#123; &#x2F;&#x2F;target&#x3D;&#x3D;0.找到一个新的解 if (target &#x3D;&#x3D; 0) &#123; res.add(new ArrayList&lt;&gt;(tmp)); &#125;else if(target&gt;0)&#123; for (int i &#x3D; start; i &lt; candidates.size(); i++) &#123; int num &#x3D; candidates.get(i); if(num&lt;&#x3D;target)&#123; tmp.add(num); &#x2F;&#x2F;查找新的target int newTarget &#x3D; target-num; find(res, tmp, candidates, newTarget, i); tmp.remove(tmp.size() - 1); &#125; &#125;&#x2F;&#x2F;end for &#125; &#125;&#125; 原文：https://blog.csdn.net/chunqiuwei/article/details/90113087#commentBox","categories":[{"name":"算法","slug":"算法","permalink":"https://blog.fenxiangz.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"八皇后","slug":"八皇后","permalink":"https://blog.fenxiangz.com/tags/%E5%85%AB%E7%9A%87%E5%90%8E/"},{"name":"算法","slug":"算法","permalink":"https://blog.fenxiangz.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"APK反编译总结","slug":"frontend/2019-11-26_APK反编译总结","date":"2019-11-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.953Z","comments":true,"path":"post/frontend/2019-11-26_APK反编译总结.html","link":"","permalink":"https://blog.fenxiangz.com/post/frontend/2019-11-26_APK%E5%8F%8D%E7%BC%96%E8%AF%91%E6%80%BB%E7%BB%93.html","excerpt":"","text":"1.准备环境 win7android-sdk_r24.0.2-windows.zipjdk7android studio1.5eclipsecharles 温馨提示：后面三个工具都需要jre|jdk环境，请首先安装jdk. 2.抓包过程通过在PC端安装charles软件，android端设置网络代理，抓取网络数据包。 2.1、PC端：在pc端创建wifi热点共享给外设-&gt;CMD命令行 netsh wlan set hostednetwork mode=allow ssid=abcd key=abcd1234 选择正在使用的网络连接，右键共享-&gt;勾选并选择刚刚创建的热点连接 netsh wlan start hostednetwork charles:proxy setting 设置代理端口，若需https抓包请设置ssl选项，并且客户端安装charles证书 2.2、客户端：WLAN设置刚创建的“abcd”共享，并指定代理IP和端口号（自己ipconfig查看即可） 3.准备反编译工具主要针对jvm的class文件和android虚拟机字节码smali，所需软件如下： apktool_2.0.0rc4.zip ---- 可以得到apk里的资源和smali文件 dex2jar-2.0.zip ---- 获得class文件 jd-gui.exe ---- 反解class文件 signapk.rar ---- 修改smali或者资源文件，重新打包签名，***DEBUG*** 4.开始吧这里以反编译土豆 app为例： 得到res和smali java -jar apktool.jar d -d ..\\..\\youku\\tudou\\tudoushipin_61.apk -o ..\\..\\youku\\tudou\\tudoushipin_61 得到class dex2jar.bat tudoushipin_61.apk 对上面的class使用jd-gui反编译，并导入eclipse 5.上演调试 &amp;&amp; android studio将smali文件导入到android studio 5.1、找到刚才apktool反解的目录找到AndroidManifest.xml，LAUNCHER对应的Activity标签上加入可被debug的配置android:debuggable=”true”，并保存。 5.2、假设我们现在把断点加载app的启动入口：找到APK的入口Activity类（搜索关键字LAUNCHER你懂得），也就是：com.tudou.ui.activity.WelcomeActivity。 到了关键性的一步，找到这个Activity对应的smali文件；定位到入口方法：onCreate；在下面加入DEBUG代码，app启动时加入断点会停在这个位置；说明一下：这段代码是smali的语法更多了解可以自行Google，OK。 a=0;// invoke-static &#123;&#125;, Landroid/os/Debug;-&gt;waitForDebugger()V 说明：根据你的需要可以把断点加到任意位置，前提是你要知道它在对应的smali文件的哪一行：方法是拿反编译后的Java文件和smali对应着去看，然后再找；后面的DEBUG也是这个思路（剧透）。 5.3、对修改后的apk重新打包 i.重新打包： java -jar apktool.jar b -d ..\\..\\youku\\tudou\\tudoushipin_61 -o debug-tudou.apk ii.重新签名： java -jar signapk\\signapk.jar signapk\\testkey.x509.pem signapk\\testkey.pk8 debug-tudou.apk debug-tudou.sign.apk iii.一切可能都不是那么顺利):( 5.4、开启android studio–&gt;基于知名的IntelliJ IDEA开发 1.导入之前反编译得到的smali文件到android studio，并在‘前面加debug代码’的地方加入断点。2.找一部android手机（模拟器就算了，又慢又总是不兼容），安装刚才的签名后的apk，通过USB数据线接入PC。 5.5、有一些必要的说明 1.默认安装完android studio，例如：C:\\dev\\android\\sdk2.对于android Dalvik虚拟机的调试监控，DDMS已经被废弃了，新的是tools下的monitor工具，将其启动3.在monitor中会看到devices中会出现小手机图标，端口号一般是8600 6.开始远程调试1.android studio中菜单栏-&gt;RUN-&gt;Edit Configuration -&gt; Remote（这根在eclipse中差不多）指定host:localhost，端口：8600，module：smali所在的位置启动app–&gt;运行debug即可 -&gt; 顺利的话光标会定位到你刚才的断点处。 2.观察Android Monitor窗口观察Debugger tag，可以查看对象和变量的值 @hell 分享","categories":[{"name":"Android","slug":"Android","permalink":"https://blog.fenxiangz.com/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://blog.fenxiangz.com/tags/Android/"},{"name":"apk","slug":"apk","permalink":"https://blog.fenxiangz.com/tags/apk/"},{"name":"反编译","slug":"反编译","permalink":"https://blog.fenxiangz.com/tags/%E5%8F%8D%E7%BC%96%E8%AF%91/"}]},{"title":"移动端跨平台方案如何选择","slug":"frontend/2019-10-27_移动端跨平台方案如何选择","date":"2019-10-27T00:00:00.000Z","updated":"2020-12-20T16:47:02.953Z","comments":true,"path":"post/frontend/2019-10-27_移动端跨平台方案如何选择.html","link":"","permalink":"https://blog.fenxiangz.com/post/frontend/2019-10-27_%E7%A7%BB%E5%8A%A8%E7%AB%AF%E8%B7%A8%E5%B9%B3%E5%8F%B0%E6%96%B9%E6%A1%88%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9.html","excerpt":"","text":"","categories":[{"name":"跨平台","slug":"跨平台","permalink":"https://blog.fenxiangz.com/categories/%E8%B7%A8%E5%B9%B3%E5%8F%B0/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://blog.fenxiangz.com/tags/JavaScript/"},{"name":"chameleon","slug":"chameleon","permalink":"https://blog.fenxiangz.com/tags/chameleon/"},{"name":"跨平台","slug":"跨平台","permalink":"https://blog.fenxiangz.com/tags/%E8%B7%A8%E5%B9%B3%E5%8F%B0/"},{"name":"小程序","slug":"小程序","permalink":"https://blog.fenxiangz.com/tags/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}]},{"title":"小程序多端框架全面测评：chameleon、Taro、uni-app、mpvue、WePY","slug":"frontend/2019-10-26_小程序多端框架全面测评","date":"2019-10-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.952Z","comments":true,"path":"post/frontend/2019-10-26_小程序多端框架全面测评.html","link":"","permalink":"https://blog.fenxiangz.com/post/frontend/2019-10-26_%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%A4%9A%E7%AB%AF%E6%A1%86%E6%9E%B6%E5%85%A8%E9%9D%A2%E6%B5%8B%E8%AF%84.html","excerpt":"","text":"最近前端届多端框架频出，相信很多有代码多端运行需求的开发者都会产生一些疑惑：这些框架都有什么优缺点？到底应该用哪个？ 作为 Taro 开发团队一员，笔者想在本文尽量站在一个客观公正的角度去评价各个框架的选型和优劣。但宥于利益相关，本文的观点很可能是带有偏向性的，大家可以带着批判的眼光去看待，权当抛砖引玉。 那么，当我们在讨论多端框架时，我们在谈论什么： 多端笔者以为，现在流行的多端框架可以大致分为三类： 1. 全包型 这类框架最大的特点就是从底层的渲染引擎、布局引擎，到中层的 DSL，再到上层的框架全部由自己开发，代表框架是 Qt 和 Flutter。这类框架优点非常明显：性能（的上限）高；各平台渲染结果一致。缺点也非常明显：需要完全重新学习 DSL（QML/Dart），以及难以适配中国特色的端：小程序。 这类框架是最原始也是最纯正的的多端开发框架，由于底层到上层每个环节都掌握在自己手里，也能最大可能地去保证开发和跨端体验一致。但它们的框架研发成本巨大，渲染引擎、布局引擎、DSL、上层框架每个部分都需要大量人力开发维护。 2. Web 技术型 这类框架把 Web 技术（JavaScript，CSS）带到移动开发中，自研布局引擎处理 CSS，使用 JavaScript 写业务逻辑，使用流行的前端框架作为 DSL，各端分别使用各自的原生组件渲染。代表框架是 React Native 和 Weex，这样做的优点有： 开发迅速 复用前端生态 易于学习上手，不管前端后端移动端，多多少少都会一点 JS、CSS 缺点有： 交互复杂时难以写出高性能的代码，这类框架的设计就必然导致 JS 和 Native 之间需要通信，类似于手势操作这样频繁地触发通信就很可能使得 UI 无法在 16ms 内及时绘制。React Native 有一些声明式的组件可以避免这个问题，但声明式的写法很难满足复杂交互的需求。 由于没有渲染引擎，使用各端的原生组件渲染，相同代码渲染的一致性没有第一种高。 3. JavaScript 编译型 这类框架就是我们这篇文章的主角们：Taro、WePY 、uni-app 、 mpvue 、 chameleon，它们的原理也都大同小异：先以 JavaScript 作为基础选定一个 DSL 框架，以这个 DSL 框架为标准在各端分别编译为不同的代码，各端分别有一个运行时框架或兼容组件库保证代码正确运行。 这类框架最大优点和创造的最大原因就是小程序，因为第一第二种框架其实除了可以跨系统平台之外，也都能编译运行在浏览器中。(Qt 有 Qt for WebAssembly, Flutter 有 Hummingbird，React Native 有 react-native-web, Weex 原生支持) 另外一个优点是在移动端一般会编译到 React Native/Weex，所以它们也都拥有 Web 技术型框架的优点。这看起来很美好，但实际上 React Native/Weex 的缺点编译型框架也无法避免。除此之外，编译型框架的抽象也不是免费的：当 bug 出现时，问题的根源可能出在运行时、编译时、组件库以及三者依赖的库等等各个方面。在 Taro 开源的过程中，我们就遇到过 Babel 的 bug，React Native 的 bug，JavaScript 引擎的 bug，当然也少不了 Taro 本身的 bug。相信其它原理相同的框架也无法避免这一问题。 但这并不意味着这类为了小程序而设计的多端框架就都不堪大用。首先现在各巨头超级 App 的小程序百花齐放，框架会为了抹平小程序做了许多工作，这些工作在大部分情况下是不需要开发者关心的。其次是许多业务类型并不需要复杂的逻辑和交互，没那么容易触发到框架底层依赖的 bug。 那么当你的业务适合选择编译型框架时，在笔者看来首先要考虑的就是选择 DSL 的起点。因为有多端需求业务通常都希望能快速开发，一个能够快速适应团队开发节奏的 DSL 就至关重要。不管是 React 还是 Vue（或者类 Vue）都有它们的优缺点，大家可以根据团队技术栈和偏好自行选择。 如果不管什么 DSL 都能接受，那就可以进入下一个环节： 生态以下内容均以各框架现在（2019 年 3 月 11 日）已发布稳定版为标准进行讨论。 1. 开发工具 就开发工具而言 uni-app 应该是一骑绝尘，它的文档内容最为翔实丰富，还自带了 IDE 图形化开发工具，鼠标点点点就能编译测试发布。 其它的框架都是使用 CLI 命令行工具，但值得注意的是 chameleon 有独立的语法检查工具，Taro 则单独写了 ESLint 规则和规则集。 在语法支持方面，mpvue、uni-app、Taro 、WePY 均支持 TypeScript，四者也都能通过 typing 实现编辑器自动补全。除了 API 补全之外，得益于 TypeScript 对于 JSX 的良好支持，Taro 也能对组件进行自动补全。 CSS 方面，所有框架均支持 SASS、LESS、Stylus，Taro 则多一个 CSS Modules 的支持。 所以这一轮比拼的结果应该是： uni-app &gt; Taro &gt; chameleon &gt; WePY、mpvue 2. 多端支持度 只从支持端的数量来看，Taro 和 uni-app 以六端略微领先（移动端、H5、微信小程序、百度小程序、支付宝小程序、头条小程序），chameleon 少了头条小程序紧随其后。 但值得一提的是 chameleon 有一套自研多态协议，编写多端代码的体验会好许多，可以说是一个能戳到多端开发痛点的功能。uni-app 则有一套独立的条件编译语法，这套语法能同时作用于 js、样式和模板文件。Taro 可以在业务逻辑中根据环境变量使用条件编译，也可以直接使用条件编译文件（类似 React Native 的方式）。 在移动端方面，uni-app 基于 weex 定制了一套 nvue 方案 弥补 weex API 的不足；Taro则是暂时基于 expo 达到同样的效果；chameleon 在移动端则有一套 SDK 配合多端协议与原生语言通信。 H5 方面，chameleon 同样是由多态协议实现支持，uni-app 和 Taro 则是都在 H5 实现了一套兼容的组件库和 API。 mpvue 和 WePY 都提供了转换各端小程序的功能，但都没有 h5 和移动端的支持。 所以最后一轮对比的结果是： chameleon &gt; Taro、uni-app &gt; mpvue &gt; WePY 3. 组件库/工具库/demo 作为开源时间最长的框架，WePY 不管从 Demo，组件库数量 ，工具库来看都占有一定优势。 uni-app 则有自己的插件市场和 UI 库，如果算上收费的框架和插件比起 WePy 也是完全不遑多让的。 Taro 也有官方维护的跨端 UI 库 taro-ui ，另外在状态管理工具上也有非常丰富的选择（Redux、MobX、dva），但 demo 的数量不如前两个。但 Taro 有一个转换微信小程序代码为 Taro 代码的工具，可以弥补这一问题。 而 mpvue 没有官方维护的 UI 库，chameleon 第三方的 demo 和工具库也还基本没有。 所以这轮的排序是： WePY &gt; uni-app 、taro &gt; mpvue &gt; chameleon 4. 接入成本 接入成本有两个方面： 第一是框架接入原有微信小程序生态。由于目前微信小程序已呈一家独大之势，开源的组件和库（例如 wxparse、echart、zan-ui 等）多是基于原生微信小程序框架语法写成的。目前看来 uni-app 、Taro、mpvue 均有文档或 demo 在框架中直接使用原生小程序组件/库，WePY 由于运行机制的问题，很多情况需要小改一下目标库的源码，chameleon 则是提供了一个按步骤大改目标库源码的迁移方式。 第二是原有微信小程序项目部分接入框架重构。在这个方面 Taro 在京东购物小程序上进行了大胆的实践，具体可以查看文章《Taro 在京东购物小程序上的实践》。其它框架则没有提到相关内容。 而对于两种接入方式 Taro 都提供了 taro convert 功能，既可以将原有微信小程序项目转换为 Taro 多端代码，也可以将微信小程序生态的组件转换为 Taro 组件。 所以这轮的排序是： Taro &gt; mpvue 、 uni-app &gt; WePY &gt; chameleon 流行度 从 GitHub 的 star 来看，mpvue 、Taro、WePY 的差距非常小。从 NPM 和 CNPM 的 CLI 工具下载量来看，是 Taro（3k/week）&gt; mpvue (2k/w) &gt; WePY (1k/w)。但发布时间也刚好反过来。笔者估计三家的流行程度和案例都差不太多。 uni-app 则号称有上万案例，但不像其它框架一样有一些大厂应用案例。另外从开发者的数量来看也是 uni-app 领先，它拥有 20+ 个 QQ 交流群（最大人数 2000）。 所以从流行程度来看应该是： uni-app &gt; Taro、WePY、mpvue &gt; chameleon 5. 开源建设 一个开源作品能走多远是由框架维护团队和第三方开发者共同决定的。虽然开源建设不能具体地量化，但依然是衡量一个框架/库生命力的非常重要的标准。 从第三方贡献者数量来看，Taro 在这一方面领先，并且 Taro 的一些核心包/功能（MobX、CSS Modules、alias）也是由第三方开发者贡献的。除此之外，腾讯开源的 omi 框架小程序部分也是基于 Taro 完成的。 WePY 在腾讯开源计划的加持下在这一方面也有不错的表现；mpvue 由于停滞开发了很久就比较落后了；可能是产品策略的原因，uni-app 在开源建设上并不热心，甚至有些部分代码都没有开源；chameleon 刚刚开源不久，但它的代码和测试用例都非常规范，以后或许会有不错的表现。 那么这一轮的对比结果是： Taro &gt; WePY &gt; mpvue &gt; chameleon &gt; uni-app 最后补一个总的生态对比图表： 未来从各框架已经公布的规划来看： WePY 已经发布了 v2.0.alpha 版本，虽然没有公开的文档可以查阅到 2.0 版本有什么新功能/特性，但据其作者介绍，WePY 2.0 会放大招，是一个「对得起开发者」的版本。笔者也非常期待 2.0 正式发布后 WePY 的表现。 mpvue 已经发布了 2.0 的版本，主要是更新了其它端小程序的支持。但从代码提交， issue 的回复/解决率来看，mpvue 要想在未来有作为首先要打消社区对于 mpvue不管不顾不更新的质疑。 uni-app 已经在生态上建设得很好了，应该会在此基础之上继续稳步发展。如果 uni-app 能加强开源开放，再加强与大厂的合作，相信未来还能更上一层楼。 chameleon 的规划比较宏大，虽然是最后发的框架，但已经在规划或正在实现的功能有： 快应用和端拓展协议 通用组件库和垂直类组件库 面向研发的图形化开发工具 面向非研发的图形化页面搭建工具 如果 chameleon 把这些功能都做出来的话，再继续完善生态，争取更多第三方开发者，那么在未来 chameleon 将大有可为。 Taro 的未来也一样值得憧憬。Taro 即将要发布的 1.3 版本就会支持以下功能： 快应用支持 Taro Doctor，自动化检查项目配置和代码合法性 更多的 JSX 语法支持，1.3 之后限制生产力的语法只有 只能用 map 创造循环组件 一条 H5 打包体积大幅精简 同时 Taro 也正在对移动端进行大规模重构；开发图形化开发工具；开发组件/物料平台以及图形化页面搭建工具。 结语那说了那么多，到底用哪个呢？ 如果不介意尝鲜和学习 DSL 的话，完全可以尝试 WePY 2.0 和 chameleon ，一个是酝酿了很久的 2.0 全新升级，一个有专门针对多端开发的多态协议。 uni-app 和 Taro 相比起来就更像是「水桶型」框架，从工具、UI 库，开发体验、多端支持等各方面来看都没有明显的短板。而 mpvue 由于开发一度停滞，现在看来各个方面都不如在小程序端基于它的 uni-app 。 当然，Talk is cheap。如果对这个话题有更多兴趣的同学可以去 GitHub 另行研究，有空看代码，没空看提交。 原文：https://blog.csdn.net/zhichaosong/article/details/88980582","categories":[{"name":"跨平台","slug":"跨平台","permalink":"https://blog.fenxiangz.com/categories/%E8%B7%A8%E5%B9%B3%E5%8F%B0/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://blog.fenxiangz.com/tags/JavaScript/"},{"name":"chameleon","slug":"chameleon","permalink":"https://blog.fenxiangz.com/tags/chameleon/"},{"name":"跨平台","slug":"跨平台","permalink":"https://blog.fenxiangz.com/tags/%E8%B7%A8%E5%B9%B3%E5%8F%B0/"},{"name":"小程序","slug":"小程序","permalink":"https://blog.fenxiangz.com/tags/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}]},{"title":"vue自定义组件中的v-model简单解释（vue单页面组件传值）","slug":"frontend/vue/2017-10-26_vue自定义组件中的v-model简单解释","date":"2019-10-25T00:00:00.000Z","updated":"2020-12-20T16:47:02.954Z","comments":true,"path":"post/frontend/vue/2017-10-26_vue自定义组件中的v-model简单解释.html","link":"","permalink":"https://blog.fenxiangz.com/post/frontend/vue/2017-10-26_vue%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BB%84%E4%BB%B6%E4%B8%AD%E7%9A%84v-model%E7%AE%80%E5%8D%95%E8%A7%A3%E9%87%8A.html","excerpt":"","text":"在使用iview框架的时候，经常会看到组件用v-model双向绑定数据，与传统步骤父组件通过props传值子组件，子组件发送$emit来修改值相比，这种方式避免操作子组件的同时再操作父组件，显得子组件的封装效果更好。所以，个人认为，我们自己封装组件也应该有这样的思维，父组件通过slot或者props传值，由子组件完成一些效果，再抛出必要的事件让父组件接受，这样组件的可复用性就很强，有利于多次使用。 v-model指令是什么?刚刚提到，iview通过v-model双向绑定数据，所以首先我们要明白v-model在这个过程中做了什么。有vue基础的同学应该知道，v-model本质是一个语法糖，在v-bind和v-on的整合。表单元素比如input，v-bind绑定一个值，就把data数据传给了value，同时再通过v-on监听input事件，当表单数据改变的时候，也会把值传给data数据，代码如下 123456&lt;input type&#x3D;&#39;text&#39; v-model&#x3D;&#39;msg&#39;&gt;&#x2F;&#x2F; 相当于&lt;input type&#x3D;&#39;text&#39; :value&#x3D;msg @input&#x3D;&#39;msg &#x3D;$event.target.value&#39;&gt; vue2.2新增model API虽说新增，实际上vue3.0都已经发布了，这其实算个比较旧特性，官网是这么写的 12允许一个自定义组件在使用 v-model 时定制 prop 和 event。默认情况下，一个组件上的 v-model 会把 value 用作 prop 且把 input 用作 event，但是一些输入类型比如单选框和复选框按钮可能想使用 value prop 来达到不同的目的。使用 model 选项可以回避这些情况产生的冲突。 这句话比较长，咱们来一步步理解，首先是第一句 允许一个自定义组件在使用v-model时定制prop和event 一般说来，v-model用在表单元素上进行数据的双向绑定，自定义组件通常通过父子组件传值绑定数据 默认情况下，一个组件上的v-model会把value用作prop且把input用作event 前边说了，v-model是v-bind和v-on的语法糖，那么这里v-model就完成两个步骤 举一个栗子 比如 1234567891011121314&#x2F;&#x2F; 父组件&lt;Child v-model&#x3D;&#39;iptValue&#39;&gt;&lt;&#x2F;Child&gt;&#x2F;&#x2F; 子组件Vue.components(&#39;Child&#39;,&#123; model: &#123; prop: ipt, evnet: change &#125; props: &#123; ipt: Number &#125; template: &#96;&lt;input type&#x3D;&#39;number&#39; :value&#x3D;&#39;ipt&#39; @change&#x3D;&#39;$emit(&quot;change&quot;,parseInt($event.target.value))&#39;&gt;&#96;&#125;) 这里父组件中的v-model相当于 1&lt;Child:value&#x3D;&#39;iptValue&#39; @change&#x3D;&#39;value &#x3D;&gt; iptValue &#x3D; value&#39;&gt;&lt;&#x2F;Child&gt; 用文字解释下上面的代码 前面说了，父子组件传值通过prop和$emit，第一步父组件把iptValue通过prop传给了子组件，但要注意，我这里的子组件给prop取了一个别名叫做ipt作为区分,所以子组件的prop对象中的键为我取的别名ipt。第二步，当子组件input值改变的时候，子组件监听了一个onchange方法，注意我这里也给$emit中的事件取了一个别名，只不过这个别名和原来的名字一样change，input值改变emit提交change事件并把新值传给父组件，又又又要注意，$emit的荷载都是字符串…. 然后就跟上面代码一样 父组件执行value =&gt; iptValue = value语句，这样就完成了父子组件数据的双向绑定 个人觉得v-model用在自定义组件最大的好处是提高了组件的封装性，父组件不必要另外写一个接受子组件发送给来的$emit方法 最后是第三句话，但是一些输入类型比如单选框和复选框按钮可能想使用 value prop 来达到不同的目的 其实这很容易理解，因为value字符串在input中是有意义的，取别名有利于区分，不太重要啦这一小部分… 结束那么以上就是个人对于自定义组件v-model的一点心得，文章有疑问或错误的地方还请指出，感谢阅读 原文：https://www.cnblogs.com/youma/p/11386428.html","categories":[{"name":"VUE","slug":"VUE","permalink":"https://blog.fenxiangz.com/categories/VUE/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://blog.fenxiangz.com/tags/JavaScript/"},{"name":"Vue","slug":"Vue","permalink":"https://blog.fenxiangz.com/tags/Vue/"}]},{"title":"vue自定义组件中的v-model简单解释（vue单页面组件传值）","slug":"frontend/2016-10-26_深入理解jQuery插件开发","date":"2019-10-25T00:00:00.000Z","updated":"2020-12-20T16:47:02.951Z","comments":true,"path":"post/frontend/2016-10-26_深入理解jQuery插件开发.html","link":"","permalink":"https://blog.fenxiangz.com/post/frontend/2016-10-26_%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jQuery%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91.html","excerpt":"","text":"如果你看到这篇文章，我确信你毫无疑问会认为jQuery是一个使用简便的库。jQuery可能使用起来很简单，但是它仍然有一些奇怪的地方，对它基本功能和概念不熟悉的人可能会难以掌握。但是不用担心，我下面已经把代码划分成小部分，做了一个简单的指导。那些语法看起来可能过于复杂，但是如果进入到它的思想和模式中，它是非常简单易懂的。下面，我们有了一个插件的基本层次： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&#x2F;&#x2F; Shawn Khameneh&#x2F;&#x2F; ExtraordinaryThoughts.com (function($) &#123; var privateFunction &#x3D; function() &#123; &#x2F;&#x2F; 代码在这里运行 &#125; var methods &#x3D; &#123; init: function(options) &#123; return this.each(function() &#123; var $this &#x3D; $(this); var settings &#x3D; $this.data(&#39;pluginName&#39;); if(typeof(settings) &#x3D;&#x3D; &#39;undefined&#39;) &#123; var defaults &#x3D; &#123; propertyName: &#39;value&#39;, onSomeEvent: function() &#123;&#125; &#125; settings &#x3D; $.extend(&#123;&#125;, defaults, options); $this.data(&#39;pluginName&#39;, settings); &#125; else &#123; settings &#x3D; $.extend(&#123;&#125;, settings, options); &#125; &#x2F;&#x2F; 代码在这里运行 &#125;); &#125;, destroy: function(options) &#123; return $(this).each(function() &#123; var $this &#x3D; $(this); $this.removeData(&#39;pluginName&#39;); &#125;); &#125;, val: function(options) &#123; var someValue &#x3D; this.eq(0).html(); return someValue; &#125; &#125;; $.fn.pluginName &#x3D; function() &#123; var method &#x3D; arguments[0]; if(methods[method]) &#123; method &#x3D; methods[method]; arguments &#x3D; Array.prototype.slice.call(arguments, 1); &#125; else if( typeof(method) &#x3D;&#x3D; &#39;object&#39; || !method ) &#123; method &#x3D; methods.init; &#125; else &#123; $.error( &#39;Method &#39; + method + &#39; does not exist on jQuery.pluginName&#39; ); return this; &#125; return method.apply(this, arguments); &#125; &#125;)(jQuery); 你可能会注意到，我所提到代码的结构和其他插件代码有很大的不同。根据你的使用和需求的不同，插件的开发方式也可能会呈现多样化。我的目的是澄清代码中的一些概念，足够让你找到适合自己的方法去理解和开发一个jQuery插件。 现在，来解剖我们的代码吧！ 容器：一个即时执行函数根本上来说，每个插件的代码是被包含在一个即时执行的函数当中，如下： 123(function(arg1, arg2) &#123; &#x2F;&#x2F; 代码&#125;)(arg1, arg2); 即时执行函数，顾名思义，是一个函数。让它与众不同的是，它被包含在一对小括号里面，这让所有的代码都在匿名函数的局部作用域中运行。这并不是说DOM（全局变量）在函数内是被屏蔽的，而是外部无法访问到函数内部的公共变量和对象命名空间。这是一个很好的开始，这样你声明你的变量和对象的时候，就不用担心着变量名和已经存在的代码有冲突。 现在，因为函数内部所有的所有公共变量是无法访问的，这样要把jQuery本身作为一个内部的公共变量来使用就会成为问题。就像普通的函数一样，即时函数也根据引用传入对象参数。我们可以将jQuery对象传入函数，如下： 1234(function($) &#123; &#x2F;&#x2F; 局部作用域中使用$来引用jQuery&#125;)(jQuery); 我们传入了一个把公共变量“jQuery”传入了一个即时执行的函数里面，在函数局部（容器）中我们可以通过“$”来引用它。也就是说，我们把容器当做一个函数来调用，而这个函数的参数就是jQuery。因为我们引用的“jQuery”作为公共变量传入，而不是它的简写“$”，这样我们就可以兼容Prototype库。如果你不用Prototype或者其它用“$”做简写的库的话，你不这样做也不会造成什么影响，但是知道这种用法仍是一件好事。 插件：一个函数一个jQuery插件本质上是我们塞进jQuery命名空间中一个庞大的函数，当然，我们可以很轻易地用“jQuery.pluginName=function”，来达到我们的目的，但是如果我们这样做的话我们的插件的代码是处于没有被保护的暴露状态的。“jQuery.fn”是“jQuery.prototype”的简写，意味当我们通过jQuery命名空间去获取我们的插件的时候，它仅可写（不可修改）。它事实上可以为你干点什么事呢？它让你恰当地组织自己的代码，和理解如何保护你的代码不受运行时候不需要的修改。最好的说法就是，这是一个很好的实践！ 通过一个插件，我们获得一个基本的jQuery函数： 12345678910(function($) &#123; $.fn.pluginName &#x3D; function(options) &#123; &#x2F;&#x2F; 代码在此处运行 return this; &#125; &#125;)(jQuery); 上面的代码中的函数可以像其他的jQuery函数那样通过“$(‘#element’).pluginName()”来调用。注意，我是如何把“return this”语句加进去的；这小片的代码通过返回一个原来元素的集合（包含在this当中）的引用来产生链式调用的效果，而这些元素是被一个jQuery对象所包裹的。你也应该注意，“this”在这个特定的作用域中是一个jQuery对象，相当于“$(‘#element’)”。 根据返回的对象，我们可以总结出，在上面的代码中，使用“$(‘#element’).pluginName()”的效果和使用“$(‘#element’)”的效果是一样的。在你的即时执行函数作用域中，没必要用“$(this)”的方式来把this包裹到一个jQuery对象中，因为this本身已经是被包装好的jQuery对象。 多个元素：理解SizzlejQuery使用的选择器引擎叫Sizzle，Sizzle可以为你的函数提供多元素操作（例如对所有类名相同的元素）。这是jQuery几个优秀的特性之一，但这也是你在开发插件过程中需要考虑的事情。即使你不准备为你的插件提供多元素支持，但为这做准备仍然是一个很好的实践。 这里我添加了一小段代码，它让你的插件代码为多元素集合中每个元素单独地起作用： 1234567891011121314151617function($) &#123; &#x2F;&#x2F; 向jQuery中被保护的“fn”命名空间中添加你的插件代码，用“pluginName”作为插件的函数名称 $.fn.pluginName &#x3D; function(options) &#123; &#x2F;&#x2F; 返回“this”（函数each（）的返回值也是this），以便进行链式调用。 return this.each(function() &#123; &#x2F;&#x2F; 此处运行代码，可以通过“this”来获得每个单独的元素 &#x2F;&#x2F; 例如： $(this).show()； var $this &#x3D; $(this); &#125;); &#125; &#125;)(jQuery); 在以上示例代码中，我并不是用 each（）在我的选择器中每个元素上运行代码。在那个被 each（）调用的函数的局部作用域中，你可以通过this来引用每个被单独处理的元素，也就是说你可以通过$(this)来引用它的jQuery对象。在局部作用域中，我用$this变量存储起jQuery对象，而不是每次调用函数的时候都使用$(this)，这会是个很好的实践。当然，这样做并不总是必要的；但我已经额外把它包含在我的代码中。还有要注意的是，我们将会对每个单独方法都使用 each（），这样到时我们就可以返回我们需要的值，而不是一个jQuery对象。 下面是一个例子，假如我们的插件支持一个 val 的方法，它可以返回我们需要的值： 12$(&#39;#element&#39;).pluginName(&#39;val&#39;);&#x2F;&#x2F; 会返回我们需要的值，而不是一个jQuery对象 功能：公有方法和私有方法一个基本的函数可能在某些情况下可以良好地工作，但是一个稍微复杂一点的插件就需要提供各种各样的方法和私有函数。你可能会使用不同的命名空间去为你的插件提供各种方法，但是最好不要让你的源代码因为多余的命名空间而变得混乱。 下面的代码定义了一个存储公有方法的JSON对象，以及展示了如何使用插件中的主函数中去判断哪些方法被调用，和如何在让方法作用到选择器每个元素上。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859(function($) &#123; &#x2F;&#x2F; 在我们插件容器内，创造一个公共变量来构建一个私有方法 var privateFunction &#x3D; function() &#123; &#x2F;&#x2F; code here &#125; &#x2F;&#x2F; 通过字面量创造一个对象，存储我们需要的共有方法 var methods &#x3D; &#123; &#x2F;&#x2F; 在字面量对象中定义每个单独的方法 init: function() &#123; &#x2F;&#x2F; 为了更好的灵活性，对来自主函数，并进入每个方法中的选择器其中的每个单独的元素都执行代码 return this.each(function() &#123; &#x2F;&#x2F; 为每个独立的元素创建一个jQuery对象 var $this &#x3D; $(this); &#x2F;&#x2F; 执行代码 &#x2F;&#x2F; 例如： privateFunction(); &#125;); &#125;, destroy: function() &#123; &#x2F;&#x2F; 对选择器每个元素都执行方法 return this.each(function() &#123; &#x2F;&#x2F; 执行代码 &#125;); &#125; &#125;; $.fn.pluginName &#x3D; function() &#123; &#x2F;&#x2F; 获取我们的方法，遗憾的是，如果我们用function(method)&#123;&#125;来实现，这样会毁掉一切的 var method &#x3D; arguments[0]; &#x2F;&#x2F; 检验方法是否存在 if(methods[method]) &#123; &#x2F;&#x2F; 如果方法存在，存储起来以便使用 &#x2F;&#x2F; 注意：我这样做是为了等下更方便地使用each（） method &#x3D; methods[method]; &#x2F;&#x2F; 如果方法不存在，检验对象是否为一个对象（JSON对象）或者method方法没有被传入 &#125; else if( typeof(method) &#x3D;&#x3D; &#39;object&#39; || !method ) &#123; &#x2F;&#x2F; 如果我们传入的是一个对象参数，或者根本没有参数，init方法会被调用 method &#x3D; methods.init; &#125; else &#123; &#x2F;&#x2F; 如果方法不存在或者参数没传入，则报出错误。需要调用的方法没有被正确调用 $.error( &#39;Method &#39; + method + &#39; does not exist on jQuery.pluginName&#39; ); return this; &#125; &#x2F;&#x2F; 调用我们选中的方法 &#x2F;&#x2F; 再一次注意我们是如何将each（）从这里转移到每个单独的方法上的 return method.call(this); &#125; &#125;)(jQuery); 注意我把 privateFunction 当做了一个函数内部的全局变量。考虑到所有的代码的运行都是在插件容器内进行的，所以这种做法是可以被接受的，因为它只在插件的作用域中可用。在插件中的主函数中，我检验了传入参数所指向的方法是否存在。如果方法不存在或者传入的是参数为对象， init 方法会被运行。最后，如果传入的参数不是一个对象而是一个不存在的方法，我们会报出一个错误信息。 同样要注意的是，我是如何在每个方法中都使用 this.each() 的。当我们在主函数中调用 method.call(this) 的时候，这里的 this 事实上就是一个jQuery对象，作为 this 传入每个方法中。所以在我们方法的即时作用域中，它已经是一个jQuery对象。只有在被 each（）所调用的函数中，我们才有必要将this包装在一个jQuery对象中。 下面是一些用法的例子： 12345678910111213141516171819202122&#x2F;* 注意这些例子可以在目前的插件代码中正确运行，并不是所有的插件都使用同样的代码结构 *&#x2F;&#x2F;&#x2F; 为每个类名为 &quot;.className&quot; 的元素执行init方法$(&#39;.className&#39;).pluginName();$(&#39;.className&#39;).pluginName(&#39;init&#39;);$(&#39;.className&#39;).pluginName(&#39;init&#39;, &#123;&#125;); &#x2F;&#x2F; 向init方法传入“&#123;&#125;”对象作为函数参数$(&#39;.className&#39;).pluginName(&#123;&#125;); &#x2F;&#x2F; 向init方法传入“&#123;&#125;”对象作为函数参数 &#x2F;&#x2F; 为每个类名为 “.className” 的元素执行destroy方法$(&#39;.className&#39;).pluginName(&#39;destroy&#39;);$(&#39;.className&#39;).pluginName(&#39;destroy&#39;, &#123;&#125;); &#x2F;&#x2F; 向destroy方法传入“&#123;&#125;”对象作为函数参数 &#x2F;&#x2F; 所有代码都可以正常运行$(&#39;.className&#39;).pluginName(&#39;init&#39;, &#39;argument1&#39;, &#39;argument2&#39;); &#x2F;&#x2F; 把 &quot;argument 1&quot; 和 &quot;argument 2&quot; 传入 &quot;init&quot; &#x2F;&#x2F; 不正确的使用$(&#39;.className&#39;).pluginName(&#39;nonexistantMethod&#39;);$(&#39;.className&#39;).pluginName(&#39;nonexistantMethod&#39;, &#123;&#125;);$(&#39;.className&#39;).pluginName(&#39;argument 1&#39;); &#x2F;&#x2F; 会尝试调用 &quot;argument 1&quot; 方法$(&#39;.className&#39;).pluginName(&#39;argument 1&#39;, &#39;argument 2&#39;); &#x2F;&#x2F; 会尝试调用 &quot;argument 1&quot; ，“argument 2”方法$(&#39;.className&#39;).pluginName(&#39;privateFunction&#39;); &#x2F;&#x2F; &#39;privateFunction&#39; 不是一个方法 在上面的例子中多次出现了 {} ，表示的是传入方法中的参数。在这小节中，上面代码可以可以正常运行，但是参数不会被传入方法中。继续阅读下一小节，你会知道如何向方法传入参数。 设置插件：传入参数许多插件都支持参数传入，如配置参数和回调函数。你可以通过传入JS键值对对象或者函数参数，为方法提供信息。如果你的方法支持多于一个或两个参数，那么没有比传入对象参数更恰当的方式。 1234567891011121314151617181920212223242526272829303132333435363738394041424344(function($) &#123; var methods &#x3D; &#123; init: function(options) &#123; &#x2F;&#x2F; 在每个元素上执行方法 return this.each(function() &#123; var $this &#x3D; $(this); &#x2F;&#x2F; 创建一个默认设置对象 var defaults &#x3D; &#123; propertyName: &#39;value&#39;, onSomeEvent: function() &#123;&#125; &#125; &#x2F;&#x2F; 使用extend方法从options和defaults对象中构造出一个settings对象 var settings &#x3D; $.extend(&#123;&#125;, defaults, options); &#x2F;&#x2F; 执行代码 &#125;); &#125; &#125;; $.fn.pluginName &#x3D; function() &#123; var method &#x3D; arguments[0]; if(methods[method]) &#123; method &#x3D; methods[method]; &#x2F;&#x2F; 我们的方法是作为参数传入的，把它从参数列表中删除，因为调用方法时并不需要它 arguments &#x3D; Array.prototype.slice.call(arguments, 1); &#125; else if( typeof(method) &#x3D;&#x3D; &#39;object&#39; || !method ) &#123; method &#x3D; methods.init; &#125; else &#123; $.error( &#39;Method &#39; + method + &#39; does not exist on jQuery.pluginName&#39; ); return this; &#125; &#x2F;&#x2F; 用apply方法来调用我们的方法并传入参数 return method.apply(this, arguments); &#125; &#125;)(jQuery); 正如上面所示，一个“options”参数被添加到方法当中，和“arguments”也被添加到了主函数中。如果一个方法已经被声明，在参数传入方法之前，调用那个方法的参数会从参数列表中删除掉。我用了“apply（）”来代替了“call（）”，“apply（）”本质上是和“call（）”做着同样的工作的，但不同的是它允许参数的传入。这种结构也允许多个参数的传入，如果你愿意这样做，你也可以为你的方法修改参数列表，例如：“init:function(arg1, arg2){}”。 如果你是使用JS对象作为参数传入，你可能需要定义一个默认对象。一旦默认对象被声明，你可以使用“$.extend”来合并参数对象和默认对象中的值，以形成一个新的参数对象来使用（在我们的例子中就是“settings”）； 这里有一些例子，用来演示以上的逻辑： 123456789101112131415161718var options &#x3D; &#123; customParameter: &#39;Test 1&#39;, propertyName: &#39;Test 2&#39;&#125; var defaults &#x3D; &#123; propertyName: &#39;Test 3&#39;, onSomeEvent: &#39;Test 4&#39;&#125; var settings &#x3D; $.extend(&#123;&#125;, defaults, options);&#x2F;*settings &#x3D;&#x3D; &#123; propertyName: &#39;Test 2&#39;, onSomeEvent: &#39;Test 4&#39;, customParameter: &#39;Test 1&#39;&#125;*&#x2F; 保存设置：添加持久性数据有时你会想在你的插件中保存设置和信息，这时jQuery中的“data（）”函数就可以派上用场了。它在使用上是非常简单的，它会尝试获取和元素相关的数据，如果数据不存在，它就会创造相应的数据并添加到元素上。一旦你使用了“data（）”来为元素添加信息，请确认你已经记住，当不再需要数据的时候，用“removeData（）”来删除相应的数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&#x2F;&#x2F; Shawn Khameneh&#x2F;&#x2F; ExtraordinaryThoughts.com (function($) &#123; var privateFunction &#x3D; function() &#123; &#x2F;&#x2F; 执行代码 &#125; var methods &#x3D; &#123; init: function(options) &#123; &#x2F;&#x2F; 在每个元素上执行方法 return this.each(function() &#123; var $this &#x3D; $(this); &#x2F;&#x2F; 尝试去获取settings，如果不存在，则返回“undefined” var settings &#x3D; $this.data(&#39;pluginName&#39;); &#x2F;&#x2F; 如果获取settings失败，则根据options和default创建它 if(typeof(settings) &#x3D;&#x3D; &#39;undefined&#39;) &#123; var defaults &#x3D; &#123; propertyName: &#39;value&#39;, onSomeEvent: function() &#123;&#125; &#125; settings &#x3D; $.extend(&#123;&#125;, defaults, options); &#x2F;&#x2F; 保存我们新创建的settings $this.data(&#39;pluginName&#39;, settings); &#125; else &#123; &#x2F; 如果我们获取了settings，则将它和options进行合并（这不是必须的，你可以选择不这样做） settings &#x3D; $.extend(&#123;&#125;, settings, options); &#x2F;&#x2F; 如果你想每次都保存options，可以添加下面代码： &#x2F;&#x2F; $this.data(&#39;pluginName&#39;, settings); &#125; &#x2F;&#x2F; 执行代码 &#125;); &#125;, destroy: function(options) &#123; &#x2F;&#x2F; 在每个元素中执行代码 return $(this).each(function() &#123; var $this &#x3D; $(this); &#x2F;&#x2F; 执行代码 &#x2F;&#x2F; 删除元素对应的数据 $this.removeData(&#39;pluginName&#39;); &#125;); &#125;, val: function(options) &#123; &#x2F;&#x2F; 这里的代码通过.eq(0)来获取选择器中的第一个元素的，我们或获取它的HTML内容作为我们的返回值 var someValue &#x3D; this.eq(0).html(); &#x2F;&#x2F; 返回值 return someValue; &#125; &#125;; $.fn.pluginName &#x3D; function() &#123; var method &#x3D; arguments[0]; if(methods[method]) &#123; method &#x3D; methods[method]; arguments &#x3D; Array.prototype.slice.call(arguments, 1); &#125; else if( typeof(method) &#x3D;&#x3D; &#39;object&#39; || !method ) &#123; method &#x3D; methods.init; &#125; else &#123; $.error( &#39;Method &#39; + method + &#39; does not exist on jQuery.pluginName&#39; ); return this; &#125; return method.apply(this, arguments); &#125; &#125;)(jQuery); 在上面的代码中，我检验了元素的数据是否存在。如果数据不存在，“options”和“default”会被合并，构建成一个新的settings，然后用“data（）”保存在元素中。 英文原文：Extraordinary Thougths ， 编译：伯乐在线——戴嘉华","categories":[{"name":"jQuery","slug":"jQuery","permalink":"https://blog.fenxiangz.com/categories/jQuery/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://blog.fenxiangz.com/tags/JavaScript/"},{"name":"Vue","slug":"Vue","permalink":"https://blog.fenxiangz.com/tags/Vue/"}]},{"title":"大型分布式网站术语分析","slug":"distribution/2019-08-14_大型分布式网站术语分析","date":"2019-08-13T00:00:00.000Z","updated":"2020-12-20T16:47:02.951Z","comments":true,"path":"post/distribution/2019-08-14_大型分布式网站术语分析.html","link":"","permalink":"https://blog.fenxiangz.com/post/distribution/2019-08-14_%E5%A4%A7%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BD%91%E7%AB%99%E6%9C%AF%E8%AF%AD%E5%88%86%E6%9E%90.html","excerpt":"","text":"1. I/O优化 增加缓存，减少磁盘的访问次数。 优化磁盘的管理系统，设计最优的磁盘方式策略，以及磁盘的寻址策略，这是在底层操作系统层面考虑的。 设计合理的磁盘存储数据块，以及访问这些数据库的策略，这是在应用层面考虑的。例如，我们可以给存放的数据设计索引，通过寻址索引来加快和减少磁盘的访问量，还可以采用异步和非阻塞的方式加快磁盘的访问速度。 应用合理的RAID策略提升磁盘I/O。 2. Web前端调优 减少网络交互的次数（多次请求合并） 减少网络传输数据量的大小(压缩) 尽量减少编码（尽量提前将字符转化为字节，或者减少从字符到字节的转化过程。） 使用浏览器缓存 减少Cookie传输 合理布局页面 使用页面压缩 延迟加载页面 CSS在最上面，JS在最下面 CDN 反向代理 页面静态化 异地部署 3.服务降级（自动优雅降级）拒绝服务和关闭服务 4.幂等性设计有些服务天然具有幂等性，比如讲用户性别设置为男性，不管设置多少次，结果都一样。但是对转账交易等操作，问题就会比较复杂，需要通过交易编号等信息进行服务调用有效性校验，只有有效的操作才能继续执行。 （注：幂等性是系统的接口对外一种承诺(而不是实现), 承诺只要调用接口成功, 外部多次调用对系统的影响是一致的. 声明为幂等的接口会认为外部调用失败是常态, 并且失败之后必然会有重试.） 5.失效转移若数据服务器集群中任何一台服务器宕机，那么应用程序针对这台服务器的所有读写操作都需要重新路由到其他服务器，保证数据访问不会失败，这个过程叫失效转移。失效转移包括：失效确认（心跳检测和应用程序访问失败报告）、访问转移、数据恢复。失效转移保证当一个数据副本不可访问时，可以快速切换访问数据的其他副本，保证系统可用。 6.性能优化根据网站分层架构,性能优化可分为：web前端性能优化、应用服务器性能优化、存储服务器性能优化。 web前端性能优化 浏览器访问优化：减少http请求;使用浏览器缓存;启用压缩;css放在页面最上面、javaScript放在页面最下面;减少Cookie传输 CDN加速 反向代理 应用服务器性能优化 分布式缓存（Redis等） 异步操作（消息队列） 使用集群（负载均衡） 代码优化 存储性能优化 机械硬盘vs固态硬盘 B+树 vs LSM树 RAID vs HDFS 7. 代码优化 多线程（Q:怎么确保线程安全？无锁机制有哪些？） 资源复用（单例模式，连接池，线程池） 数据结构 垃圾回收 8. 负载均衡 HTTP重定向负载均衡 当用户发来请求的时候，Web服务器通过修改HTTP响应头中的Location标记来返回一个新的url，然后浏览器再继续请求这个新url，实际上就是页面重定向。通过重定向，来达到“负载均衡”的目标。例如，我们在下载PHP源码包的时候，点击下载链接时，为了解决不同国家和地域下载速度的问题，它会返回一个离我们近的下载地址。重定向的HTTP返回码是302。 优点：比较简单。 缺点：浏览器需要两次请求服务器才能完成一次访问，性能较差。重定向服务自身的处理能力有可能成为瓶颈，整个集群的伸缩性国模有限；使用HTTP302响应码重定向，有可能使搜索引擎判断为SEO作弊，降低搜索排名。 DNS域名解析负载均衡 DNS（Domain Name System）负责域名解析的服务，域名url实际上是服务器的别名，实际映射是一个IP地址，解析过程，就是DNS完成域名到IP的映射。而一个域名是可以配置成对应多个IP的。因此，DNS也就可以作为负载均衡服务。 事实上，大型网站总是部分使用DNS域名解析，利用域名解析作为第一级负载均衡手段，即域名解析得到的一组服务器并不是实际提供Web服务的物理服务器，而是同样提供负载均衡服务的内部服务器，这组内部负载均衡服务器再进行负载均衡，将请求分发到真是的Web服务器上。 优点：将负载均衡的工作转交给DNS，省掉了网站管理维护负载均衡服务器的麻烦，同时许多DNS还支持基于地理位置的域名解析，即会将域名解析成举例用户地理最近的一个服务器地址，这样可以加快用户访问速度，改善性能。 缺点：不能自由定义规则，而且变更被映射的IP或者机器故障时很麻烦，还存在DNS生效延迟的问题。而且DNS负载均衡的控制权在域名服务商那里，网站无法对其做更多改善和更强大的管理。 反向代理负载均衡 反向代理服务可以缓存资源以改善网站性能。实际上，在部署位置上，反向代理服务器处于Web服务器前面（这样才可能缓存Web相应，加速访问），这个位置也正好是负载均衡服务器的位置，所以大多数反向代理服务器同时提供负载均衡的功能，管理一组Web服务器，将请求根据负载均衡算法转发到不同的Web服务器上。Web服务器处理完成的响应也需要通过反向代理服务器返回给用户。由于web服务器不直接对外提供访问，因此Web服务器不需要使用外部ip地址，而反向代理服务器则需要配置双网卡和内部外部两套IP地址。 优点：和反向代理服务器功能集成在一起，部署简单。 缺点：反向代理服务器是所有请求和响应的中转站，其性能可能会成为瓶颈。 LVS-NAT:修改IP地址 LVS-TUN: 一个IP报文封装在另一个IP报文的技术。 LVS-DR:将数据帧的MAC地址改为选出服务器的MAC地址，再将修改后的数据帧在与服务器组的局域网上发送。 9.缓存缓存就是将数据存放在距离计算最近的位置以加快处理速度。缓存是改善软件性能的第一手段，现在CPU越来越快的一个重要因素就是使用了更多的缓存，在复杂的软件设计中，缓存几乎无处不在。大型网站架构设计在很多方面都使用了缓存设计。 CDN: 及内容分发网络，部署在距离终端用户最近的网络服务商，用户的网络请求总是先到达他的网络服务商哪里，在这里缓存网站的一些静态资源（较少变化的数据），可以就近以最快速度返回给用户，如视频网站和门户网站会将用户访问量大的热点内容缓存在CDN中。 反向代理：反向代理属于网站前端架构的一部分，部署在网站的前端，当用户请求到达网站的数据中心时，最先访问到的就是反向代理服务器，这里缓存网站的静态资源，无需将请求继续转发给应用服务器就能返回给用户。 本地缓存：在应用服务器本地缓存着热点数据，应用程序可以在本机内存中直接访问数据，而无需访问数据库。 分布式缓存：大型网站的数据量非常庞大，即使只缓存一小部分，需要的内存空间也不是单机能承受的，所以除了本地缓存，还需要分布式缓存，将数据缓存在一个专门的分布式缓存集群中，应用程序通过网络通信访问缓存数据。 使用缓存有两个前提条件，一是数据访问热点不均衡，某些数据会被更频繁的访问，这些数据应该放在缓存中；二是数据在某个时间段内有效，不会很快过期，否则缓存的数据就会因已经失效而产生脏读，影响结果的正确性。网站应用中，缓存处理可以加快数据访问速度，还可以减轻后端应用和数据存储的负载压力，这一点对网站数据库架构至关重要，网站数据库几乎都是按照有缓存的前提进行负载能力设计的。 10. 负载均衡算法轮询 Round Robin加强轮询 Weight Round Robin随机 Random加强随机 Weight Random最少连接 Least Connections加强最少连接源地址散列 Hash其他算法 最快算法(Fastest)：传递连接给那些响应最快的服务器。当其中某个服务器发生第二到第7 层的故障，BIG-IP 就把其从服务器队列中拿出，不参加下一次的用户请求的分配，直到其恢复正常。 观察算法(Observed)：连接数目和响应时间以这两项的最佳平衡为依据为新的请求选择服务器。当其中某个服务器发生第二到第7 层的故障，BIG-IP就把其从服务器队列中拿出，不参加下一次的用户请求的分配，直到其恢复正常。 预测算法(Predictive)：BIG-IP利用收集到的服务器当前的性能指标，进行预测分析，选择一台服务器在下一个时间片内，其性能将达到最佳的服务器相应用户的请求。(被BIG-IP 进行检测) 动态性能分配算法(Dynamic Ratio-APM):BIG-IP 收集到的应用程序和应用服务器的各项性能参数，动态调整流量分配。 动态服务器补充算法(Dynamic Server Act.):当主服务器群中因故障导致数量减少时，动态地将备份服务器补充至主服务器群。 服务质量算法(QoS):按不同的优先级对数据流进行分配。 服务类型算法(ToS): 按不同的服务类型(在Type of Field中标识)负载均衡对数据流进行分配。 规则模式算法：针对不同的数据流设置导向规则，用户可自行 11. 扩展性和伸缩性的区别扩展性：指对现有系统影响最小的情况下，系统功能可持续扩展或替身的能力。表现在系统基础设施稳定不需要经常变更，应用之间较少依赖和耦合，对需求变更可以敏捷响应。它是系统架构设计层面的开闭原则（对扩展开放，对修改关闭），架构设计考虑未来功能扩展，当系统增加新功能时，不需要对现有系统的结构和代码进行修改。 衡量网站架构扩展性好坏的主要标准就是在网站增加新的业务产品时，是否可以实现对现有产品透明无影响，不需要任何改动或者很少改动既有业务功能就可以上线新产品。不同产品之间是否很少耦合，一个产品改动对其他产品无影响，其他产品和功能不需要受牵连进行改动。 伸缩性：所谓网站的伸缩性指是不需要改变网站的软硬件设计，仅仅通过改变部署的服务器数量就可以扩大或者缩小网站的服务处理能力。 指系统能够增加（减少）自身资源规模的方式增强（减少）自己计算处理事务的能力。如果这种增减是成比例的，就被称作线性伸缩性。在网站架构中，通常指利用集群的方式增加服务器数量、提高系统的整体事务吞吐能力。 衡量架构伸缩性的主要标准就是可以用多台服务器构建集群，是否容易向集群中添加新的服务器。加入新的服务器后是否可以提供和原来服务无差别的服务、集群中的可容纳的总的服务器数量是否有限制。 12.分布式缓存的一致性hash具体算法过程：先构造一个长度为2^32的整数环（这个环被称作一致性Hash环）根据节点名称的Hash值（其分布范围为[0,2^32 – 1]）将缓存服务器阶段设置在这个Hash环上。然后根据需要缓存的数据的Key值计算得到Hash值（其分布范围也同样为[0,2^32 – 1]），然后在Hash环上顺时针查找举例这个KEY的hash值最近的缓存服务器节点，完成KEY到服务器的Hash映射查找。 优化策略：将每台物理服务器虚拟为一组虚拟缓存服务器，将虚拟服务器的Hash值放置在Hash环上，key在换上先找到虚拟服务器节点，再得到物理服务器的信息。 一台物理服务器设置多少个虚拟服务器节点合适呢？经验值：150。 13. 网络安全 XSS攻击 跨站点脚本攻击(Cross Site Script)，指黑客通过篡改网页，注入恶意的HTML脚本，在用户浏览网页时，控制用户浏览器进行恶意操作的一种攻击方式。 防范手段：消毒（XSS攻击者一般都是通过在请求中嵌入恶意脚本大道攻击的目的，这些脚本是一般用户输入中不使用的，如果进行过滤和消毒处理，即对某些html危险字符转移，如“&gt;”转译为“&amp; gt;”）;HttpOnly(防止XSS攻击者窃取Cookie). 注入攻击：SQL注入和OS注入 SQL防范：预编译语句PreparedStatement; ORM；避免密码明文存放；处理好相应的异常。 CSRF（Cross Site Request Forgery，跨站点请求伪造）。听起来与XSS有点相似，事实上两者区别很大，XSS利用的是站内的信任用户，而CSRF则是通过伪装来自受信任用户的请求来利用受信任的网站。 防范：httpOnly;增加token;通过Referer识别。 文件上传漏洞 DDos攻击 14. 加密技术 摘要加密：MD5, SHA 对称加密：DES算法，RC算法， AES 非对称加密：RSA 非对称加密技术通常用在信息安全传输，数字签名等场合。 HTTPS传输中浏览器使用的数字证书实质上是经过权威机构认证的非对称加密的公钥。 15. 流控（流量控制） 流量丢弃 通过单机内存队列来进行有限的等待，直接丢弃用户请求的处理方式显得简单而粗暴，并且如果是I/O密集型应用（包括网络I/O和磁盘I/O），瓶颈一般不再CPU和内存。因此，适当的等待，既能够替身用户体验，又能够提高资源利用率。 通过分布式消息队列来将用户的请求异步化。 参考资料1. LVS：三种负载均衡方式比较+另三种负载均衡方式2. 《大型网站技术架构——核心原理与技术分析》李智慧 著。3. 亿级Web系统搭建：单机到分布式集群4. 《大型分布式网站架构设计与实现》陈康贤 著。 from:http://www.importnew.com/24198.html","categories":[{"name":"概念","slug":"概念","permalink":"https://blog.fenxiangz.com/categories/%E6%A6%82%E5%BF%B5/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://blog.fenxiangz.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"术语","slug":"术语","permalink":"https://blog.fenxiangz.com/tags/%E6%9C%AF%E8%AF%AD/"},{"name":"理论","slug":"理论","permalink":"https://blog.fenxiangz.com/tags/%E7%90%86%E8%AE%BA/"}]},{"title":"【转载-阮一峰】CAP 定理的含义","slug":"distribution/2019-08-12_CAP定理的含义","date":"2019-08-12T00:00:00.000Z","updated":"2020-12-20T16:47:02.950Z","comments":true,"path":"post/distribution/2019-08-12_CAP定理的含义.html","link":"","permalink":"https://blog.fenxiangz.com/post/distribution/2019-08-12_CAP%E5%AE%9A%E7%90%86%E7%9A%84%E5%90%AB%E4%B9%89.html","excerpt":"","text":"分布式系统（distributed system）正变得越来越重要，大型网站几乎都是分布式的。 分布式系统的最大难点，就是各个节点的状态如何同步。CAP 定理是这方面的基本定理，也是理解分布式系统的起点。 本文介绍该定理。它其实很好懂，而且是显而易见的。下面的内容主要参考了 Michael Whittaker 的文章。 一、分布式系统的三个指标 1998年，加州大学的计算机科学家 Eric Brewer 提出，分布式系统有三个指标。 Consistency Availability Partition tolerance 它们的第一个字母分别是 C、A、P。 Eric Brewer 说，这三个指标不可能同时做到。这个结论就叫做 CAP 定理。 二、Partition tolerance先看 Partition tolerance，中文叫做”分区容错”。 大多数分布式系统都分布在多个子网络。每个子网络就叫做一个区（partition）。分区容错的意思是，区间通信可能失败。比如，一台服务器放在中国，另一台服务器放在美国，这就是两个区，它们之间可能无法通信。 上图中，G1 和 G2 是两台跨区的服务器。G1 向 G2 发送一条消息，G2 可能无法收到。系统设计的时候，必须考虑到这种情况。 一般来说，分区容错无法避免，因此可以认为 CAP 的 P 总是成立。CAP 定理告诉我们，剩下的 C 和 A 无法同时做到。 三、ConsistencyConsistency 中文叫做”一致性”。意思是，写操作之后的读操作，必须返回该值。举例来说，某条记录是 v0，用户向 G1 发起一个写操作，将其改为 v1。 接下来，用户的读操作就会得到 v1。这就叫一致性。 问题是，用户有可能向 G2 发起读操作，由于 G2 的值没有发生变化，因此返回的是 v0。G1 和 G2 读操作的结果不一致，这就不满足一致性了。 为了让 G2 也能变为 v1，就要在 G1 写操作的时候，让 G1 向 G2 发送一条消息，要求 G2 也改成 v1。 这样的话，用户向 G2 发起读操作，也能得到 v1。 四、AvailabilityAvailability 中文叫做”可用性”，意思是只要收到用户的请求，服务器就必须给出回应。 用户可以选择向 G1 或 G2 发起读操作。不管是哪台服务器，只要收到请求，就必须告诉用户，到底是 v0 还是 v1，否则就不满足可用性。 五、Consistency 和 Availability 的矛盾一致性和可用性，为什么不可能同时成立？答案很简单，因为可能通信失败（即出现分区容错）。 如果保证 G2 的一致性，那么 G1 必须在写操作时，锁定 G2 的读操作和写操作。只有数据同步后，才能重新开放读写。锁定期间，G2 不能读写，没有可用性不。 如果保证 G2 的可用性，那么势必不能锁定 G2，所以一致性不成立。 综上所述，G2 无法同时做到一致性和可用性。系统设计时只能选择一个目标。如果追求一致性，那么无法保证所有节点的可用性；如果追求所有节点的可用性，那就没法做到一致性。 [更新 2018.7.17] 读者问，在什么场合，可用性高于一致性？ 举例来说，发布一张网页到 CDN，多个服务器有这张网页的副本。后来发现一个错误，需要更新网页，这时只能每个服务器都更新一遍。 一般来说，网页的更新不是特别强调一致性。短时期内，一些用户拿到老版本，另一些用户拿到新版本，问题不会特别大。当然，所有人最终都会看到新版本。所以，这个场合就是可用性高于一致性。 （完） 【转载】http://www.ruanyifeng.com/blog/2018/07/cap.html","categories":[{"name":"概念","slug":"概念","permalink":"https://blog.fenxiangz.com/categories/%E6%A6%82%E5%BF%B5/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://blog.fenxiangz.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"CAP","slug":"CAP","permalink":"https://blog.fenxiangz.com/tags/CAP/"},{"name":"概念","slug":"概念","permalink":"https://blog.fenxiangz.com/tags/%E6%A6%82%E5%BF%B5/"}]},{"title":"Zookeeper是如何解决脑裂问题","slug":"distribution/2019-08-13_Zookeeper是如何解决脑裂问题","date":"2019-08-12T00:00:00.000Z","updated":"2020-12-20T16:47:02.950Z","comments":true,"path":"post/distribution/2019-08-13_Zookeeper是如何解决脑裂问题.html","link":"","permalink":"https://blog.fenxiangz.com/post/distribution/2019-08-13_Zookeeper%E6%98%AF%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98.html","excerpt":"","text":"前言这是分布式系统中一个很实际的问题，书上说的不是很详细，整理总结一下。 1、脑裂和假死1.1 脑裂官方定义：当一个集群的不同部分在同一时间都认为自己是活动的时候，我们就可以将这个现象称为脑裂症状。通俗的说，就是比如当你的 cluster 里面有两个结点，它们都知道在这个 cluster 里需要选举出一个 master。那么当它们两之间的通信完全没有问题的时候，就会达成共识，选出其中一个作为 master。但是如果它们之间的通信出了问题，那么两个结点都会觉得现在没有 master，所以每个都把自己选举成 master。于是 cluster 里面就会有两个 master。举例： UserA和UserB分别将自己的信息注册在RouterA和RouterB中。RouterA和RouterB使用数据同步（2PC），来同步信息。那么当UserA想要向UserB发送一个消息的时候，需要现在RouterA中查询出UserA到UserB的消息路由路径，然后再交付给相应的路径进行路由。 当脑裂发生的时候，相当RouterA和RouterB直接的联系丢失了，RouterA认为整个系统中只有它一个Router，RouterB也是这样认为的。那么相当于RouterA中没有UserB的信息，RouterB中没有UserA的信息了，此时UserA再发送消息给UserB的时候，RouterA会认为UserB已经离线了，然后将该信息进行离线持久化，这样整个网络的路由是不是就乱掉了。 对于Zookeeper来说有一个很重要的问题，就是到底是根据一个什么样的情况来判断一个节点死亡down掉了。 在分布式系统中这些都是有监控者来判断的，但是监控者也很难判定其他的节点的状态，唯一一个可靠的途径就是心跳，Zookeeper也是使用心跳来判断客户端是否仍然活着，但是使用心跳机制来判断节点的存活状态也带来了假死问题。 1.2 假死ZooKeeper每个节点都尝试注册一个象征master的临时节点，其他没有注册成功的则成为slaver，并且通过watch机制监控着master所创建的临时节点，Zookeeper通过内部心跳机制来确定master的状态，一旦master出现意外Zookeeper能很快获悉并且通知其他的slaver，其他slaver在之后作出相关反应。这样就完成了一个切换。 这种模式也是比较通用的模式，基本大部分都是这样实现的，但是这里面有个很严重的问题，如果注意不到会导致短暂的时间内系统出现脑裂，因为心跳出现超时可能是master挂了，但是也可能是master，zookeeper之间网络出现了问题，也同样可能导致。这种情况就是假死，master并未死掉，但是与ZooKeeper之间的网络出现问题导致Zookeeper认为其挂掉了然后通知其他节点进行切换，这样slaver中就有一个成为了master，但是原本的master并未死掉，这时候client也获得master切换的消息，但是仍然会有一些延时，zookeeper需要通讯需要一个一个通知，这时候整个系统就很混乱可能有一部分client已经通知到了连接到新的master上去了，有的client仍然连接在老的master上如果同时有两个client需要对master的同一个数据更新并且刚好这两个client此刻分别连接在新老的master上，就会出现很严重问题。 1.3 总结假死：由于心跳超时（网络原因导致的）认为master死了，但其实master还存活着。 脑裂：由于假死会发起新的master选举，选举出一个新的master，但旧的master网络又通了，导致出现了两个master ，有的客户端连接到老的master 有的客户端链接到新的master。 2、Zookeeper的解决方案要解决Split-Brain的问题，一般有3种方式: Quorums（ˈkwôrəm 法定人数） ：比如3个节点的集群，Quorums = 2, 也就是说集群可以容忍1个节点失效，这时候还能选举出1个lead，集群还可用。比如4个节点的集群，它的Quorums = 3，Quorums要超过3，相当于集群的容忍度还是1，如果2个节点失效，那么整个集群还是无效的 Redundant communications：冗余通信的方式，集群中采用多种通信方式，防止一种通信方式失效导致集群中的节点无法通信。 Fencing, 共享资源的方式：比如能看到共享资源就表示在集群中，能够获得共享资源的锁的就是Leader，看不到共享资源的，就不在集群中。 ZooKeeper默认采用了Quorums这种方式，即只有集群中超过半数节点投票才能选举出Leader。这样的方式可以确保leader的唯一性,要么选出唯一的一个leader,要么选举失败。在ZooKeeper中Quorums有2个作用： 集群中最少的节点数用来选举Leader保证集群可用：通知客户端数据已经安全保存前集群中最少数量的节点数已经保存了该数据。一旦这些节点保存了该数据，客户端将被通知已经安全保存了，可以继续其他任务。而集群中剩余的节点将会最终也保存了该数据。 假设某个leader假死，其余的followers选举出了一个新的leader。这时，旧的leader复活并且仍然认为自己是leader，这个时候它向其他followers发出写请求也是会被拒绝的。因为每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。那有没有follower不知道新的leader存在呢，有可能，但肯定不是大多数，否则新leader无法产生。Zookeeper的写也遵循quorum机制，因此，得不到大多数支持的写是无效的，旧leader即使各种认为自己是leader，依然没有什么作用。 3、总结总结一下就是，通过Quorums机制来防止脑裂和假死，当leader挂掉之后，可以重新选举出新的leader节点使整个集群达成一致；当出现假死现象时，通过epoch大小来拒绝旧的leader发起的请求，在前面也已经讲到过，这个时候，重新恢复通信的老的leader节点会进入恢复模式，与新的leader节点做数据同步，perfect。 原文: https://blog.csdn.net/u013374645/article/details/93140148","categories":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://blog.fenxiangz.com/categories/zookeeper/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://blog.fenxiangz.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"概念","slug":"概念","permalink":"https://blog.fenxiangz.com/tags/%E6%A6%82%E5%BF%B5/"},{"name":"脑裂","slug":"脑裂","permalink":"https://blog.fenxiangz.com/tags/%E8%84%91%E8%A3%82/"}]},{"title":"JVM内存结构：堆、栈、方法区","slug":"java/advance/2019-07-22_java_stack","date":"2019-07-22T00:00:00.000Z","updated":"2020-12-20T16:47:02.961Z","comments":true,"path":"post/java/advance/2019-07-22_java_stack.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2019-07-22_java_stack.html","excerpt":"","text":"一、定义 1、堆：FIFO队列优先，先进先出。jvm只有一个堆区被所有线程所共享！堆存放在二级缓存中，调用对象的速度相对慢一些，生命周期由虚拟机的垃圾回收机制定。 2、栈：FILO先进后出，暂存数据的地方。每个线程都包含一个栈区！栈存放在一级缓存中，存取速度较快，“栈是限定仅在表头进行插入和删除操作的线性表”。 3、方法区：用来存放方法和static变量。 二、存储的数据类型 1、堆用来存储new出来的对象和数组 2、栈用来存储基本类型变量和对象的引用变量的地址 3、方法区存储方法和static变量 三、优缺点 1、堆的优点-可以动态的分配内存大小，生命周期不确定。缺点-速度略慢 2、栈的优点-速度快，缺点-存在栈中的数据大小和生命周期必须是明确的，缺少灵活性。 四、直接内存 直接内存并不是虚拟机运行时数据区的一部分，也不是Java 虚拟机规范中农定义的内存区域。在JDK1.4 中新加入了NIO(New Input/Output)类，引入了一种基于通道(Channel)与缓冲区（Buffer）的I/O 方式，它可以使用native 函数库直接分配堆外内存，然后通脱一个存储在Java堆中的DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。 本机直接内存的分配不会受到Java 堆大小的限制，受到本机总内存大小限制 配置虚拟机参数时，不要忽略直接内存 防止出现OutOfMemoryError异常 直接内存（堆外内存）与堆内存比较 直接内存申请空间耗费更高的性能，当频繁申请到一定量时尤为明显 直接内存IO读写的性能要优于普通的堆内存，在多次读写操作的情况下差异明显 代码验证： package com.youyuan.web.controller.user; import java.nio.ByteBuffer; /** * 直接内存 与 堆内存的比较 */ public class ByteBufferCompare &#123; public static void main(String[] args) &#123; allocateCompare(); //分配比较 operateCompare(); //读写比较 &#125; /** * 直接内存 和 堆内存的 分配空间比较 * &lt;p&gt; * 结论： 在数据量提升时，直接内存相比非直接内的申请，有很严重的性能问题 */ public static void allocateCompare() &#123; int time = 10000000; //操作次数 long st = System.currentTimeMillis(); for (int i = 0; i &lt; time; i++) &#123; //ByteBuffer.allocate(int capacity) 分配一个新的字节缓冲区。 ByteBuffer buffer = ByteBuffer.allocate(2); //非直接内存分配申请 &#125; long et = System.currentTimeMillis(); System.out.println(&quot;在进行&quot; + time + &quot;次分配操作时，堆内存 分配耗时:&quot; + (et - st) + &quot;ms&quot;); long st_heap = System.currentTimeMillis(); for (int i = 0; i &lt; time; i++) &#123; //ByteBuffer.allocateDirect(int capacity) 分配新的直接字节缓冲区。 ByteBuffer buffer = ByteBuffer.allocateDirect(2); //直接内存分配申请 &#125; long et_direct = System.currentTimeMillis(); System.out.println(&quot;在进行&quot; + time + &quot;次分配操作时，直接内存 分配耗时:&quot; + (et_direct - st_heap) + &quot;ms&quot;); &#125; /** * 直接内存 和 堆内存的 读写性能比较 * &lt;p&gt; * 结论：直接内存在直接的IO 操作上，在频繁的读写时 会有显著的性能提升 */ public static void operateCompare() &#123; int time = 1000000000; ByteBuffer buffer = ByteBuffer.allocate(2 * time); long st = System.currentTimeMillis(); for (int i = 0; i &lt; time; i++) &#123; // putChar(char value) 用来写入 char 值的相对 put 方法 buffer.putChar(&#39;a&#39;); &#125; buffer.flip(); for (int i = 0; i &lt; time; i++) &#123; buffer.getChar(); &#125; long et = System.currentTimeMillis(); System.out.println(&quot;在进行&quot; + time + &quot;次读写操作时，非直接内存读写耗时：&quot; + (et - st) + &quot;ms&quot;); ByteBuffer buffer_d = ByteBuffer.allocateDirect(2 * time); long st_direct = System.currentTimeMillis(); for (int i = 0; i &lt; time; i++) &#123; // putChar(char value) 用来写入 char 值的相对 put 方法 buffer_d.putChar(&#39;a&#39;); &#125; buffer_d.flip(); for (int i = 0; i &lt; time; i++) &#123; buffer_d.getChar(); &#125; long et_direct = System.currentTimeMillis(); System.out.println(&quot;在进行&quot; + time + &quot;次读写操作时，直接内存读写耗时:&quot; + (et_direct - st_direct) + &quot;ms&quot;); &#125; &#125; 输出：在进行10000000次分配操作时，堆内存 分配耗时:12ms在进行10000000次分配操作时，直接内存 分配耗时:8233ms在进行1000000000次读写操作时，非直接内存读写耗时：4055ms在进行1000000000次读写操作时，直接内存读写耗时:745ms 可以自己设置不同的time 值进行比较 分析 从数据流的角度，来看 非直接内存作用链:本地IO –&gt;直接内存–&gt;非直接内存–&gt;直接内存–&gt;本地IO直接内存作用链:本地IO–&gt;直接内存–&gt;本地IO 直接内存使用场景 有很大的数据需要存储，它的生命周期很长 适合频繁的IO操作，例如网络并发场景 参考 《深入理解Java虚拟机》 –周志明 博文：https://www.cnblogs.com/xing901022/p/5243657.html (rel=undefined)","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"内存模型","slug":"内存模型","permalink":"https://blog.fenxiangz.com/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"},{"name":"jvm","slug":"jvm","permalink":"https://blog.fenxiangz.com/tags/jvm/"}]},{"title":"Java直接内存与非直接内存性能测试","slug":"java/nio/2019-07-22_java_nio_Java直接内存与非直接内存性能测试","date":"2019-07-22T00:00:00.000Z","updated":"2020-12-20T16:47:02.970Z","comments":true,"path":"post/java/nio/2019-07-22_java_nio_Java直接内存与非直接内存性能测试.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/nio/2019-07-22_java_nio_Java%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98%E4%B8%8E%E9%9D%9E%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95.html","excerpt":"","text":"什么是直接内存与非直接内存根据官方文档的描述： A byte bufferis either direct or non-direct. Given a direct byte buffer, the Java virtual machine will make a best effort to perform native I/O operations directly upon it. That is, it will attempt to avoid copying the buffer’s content to (or from) an intermediate buffer before (orafter) each invocation of one of the underlying operating system’s native I/O operations. byte byffer可以是两种类型，一种是基于直接内存（也就是非堆内存）；另一种是非直接内存（也就是堆内存）。 对于直接内存来说，JVM将会在IO操作上具有更高的性能，因为它直接作用于本地系统的IO操作。而非直接内存，也就是堆内存中的数据，如果要作IO操作，会先复制到直接内存，再利用本地IO处理。 从数据流的角度，非直接内存是下面这样的作用链： 1本地IO--&gt;直接内存--&gt;非直接内存--&gt;直接内存--&gt;本地IO 而直接内存是： 1本地IO--&gt;直接内存--&gt;本地IO 很明显，再做IO处理时，比如网络发送大量数据时，直接内存会具有更高的效率。 A direct byte buffer may be created by invoking the allocateDirect factory method of this class. The buffers returned by this method typically have somewhat higher allocation and deallocation costs than non-direct buffers. The contents of direct buffers may reside outside of the normal garbage-collected heap, and so their impact upon the memory footprint of an application might not be obvious. It is therefore recommended that direct buffers be allocated primarily for large, long-lived buffers that are subject to the underlying system’s native I/O operations. In general it is best to allocate direct buffers only when they yield a measureable gain in program performance. 但是，不要高兴的太早。文档中也说了，直接内存使用allocateDirect创建，但是它比申请普通的堆内存需要耗费更高的性能。不过，这部分的数据是在JVM之外的，因此它不会占用应用的内存。 所以呢，当你有很大的数据要缓存，并且它的生命周期又很长，那么就比较适合使用直接内存。只是一般来说，如果不是能带来很明显的性能提升，还是推荐直接使用堆内存。 关于直接内存需要注意的，就是上面两点了，其他的关于视图啊、作用链啊，都是使用上的问题了。如果有兴趣，可以参考官方API ( 进去后搜索ByteBuffer，就能看到！)，里面有少量的描述！重要的一些用法，还得自己摸索。 使用场景通过上面的官方文档，与一些资料的搜索。可以总结下，直接内存的使用场景： 1 有很大的数据需要存储，它的生命周期又很长 2 适合频繁的IO操作，比如网络并发场景 申请分配地址速度比较下面用一段简单的代码，测试下申请内存空间的速度： 12345678910111213inttime &#x3D; 10000000;Date begin &#x3D; newDate();for(int i&#x3D;0;i&lt;time;i++)&#123; ByteBuffer buffer &#x3D; ByteBuffer.allocate(2);&#125;Dateend &#x3D; newDate();System.out.println(end.getTime()-begin.getTime());begin &#x3D; newDate();for(int i&#x3D;0;i&lt;time;i++)&#123; ByteBuffer buffer &#x3D; ByteBuffer.allocateDirect(2);&#125;end &#x3D; newDate();System.out.println(end.getTime()-begin.getTime()); 得到的测试结果如下： 在数据量提升时，直接内存相比于非直接内存的申请 有十分十分十分明显的性能问题！ 读写速度比较然后在写段代码，测试下读写的速度： 1234567891011121314151617181920212223inttime &#x3D; 1000;Date begin &#x3D; newDate();ByteBuffer buffer &#x3D; ByteBuffer.allocate(2*time);for(int i&#x3D;0;i&lt;time;i++)&#123; buffer.putChar(&#39;a&#39;);&#125;buffer.flip();for(int i&#x3D;0;i&lt;time;i++)&#123; buffer.getChar();&#125;Dateend &#x3D; newDate();System.out.println(end.getTime()-begin.getTime());begin &#x3D; newDate();ByteBuffer buffer2 &#x3D; ByteBuffer.allocateDirect(2*time);for(int i&#x3D;0;i&lt;time;i++)&#123; buffer2.putChar(&#39;a&#39;);&#125;buffer2.flip();for(int i&#x3D;0;i&lt;time;i++)&#123; buffer2.getChar();&#125;end &#x3D; newDate();System.out.println(end.getTime()-begin.getTime()); 测试的结果如下： 可以看到直接内存在直接的IO操作上，还是有明显的差异的！ 作者：xingoo 出处：http://www.cnblogs.com/xing901022","categories":[{"name":"Java NIO","slug":"Java-NIO","permalink":"https://blog.fenxiangz.com/categories/Java-NIO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"NIO","slug":"NIO","permalink":"https://blog.fenxiangz.com/tags/NIO/"},{"name":"直接内存","slug":"直接内存","permalink":"https://blog.fenxiangz.com/tags/%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98/"},{"name":"非直接内存","slug":"非直接内存","permalink":"https://blog.fenxiangz.com/tags/%E9%9D%9E%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98/"},{"name":"堆内存","slug":"堆内存","permalink":"https://blog.fenxiangz.com/tags/%E5%A0%86%E5%86%85%E5%AD%98/"}]},{"title":"Java8新特性学习-函数式编程","slug":"java/advance/2019-05-04_java_labmda","date":"2019-05-04T00:00:00.000Z","updated":"2020-12-20T16:47:02.961Z","comments":true,"path":"post/java/advance/2019-05-04_java_labmda.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2019-05-04_java_labmda.html","excerpt":"","text":"Java8 新引入函数式编程方式，大大的提高了编码效率。本文将对涉及的对象等进行统一的学习及记录。内容包括：Stream/Function/Optional/Consumer。 首先需要清楚一个概念：函数式接口；它指的是有且只有一个未实现的方法的接口，一般通过 FunctionalInterface 这个注解来表明某个接口是一个函数式接口。函数式接口是 Java 支持函数式编程的基础。 本文目录： 1 Java8 函数式编程语法入门 2 Java 函数式接口 2.1 Consumer 2.2 Function 2.3 Predicate 3 函数式编程接口的使用 3.1 Stream 3.1.1 Stream 对象的创建 3.1.2 Stream 对象的使用 3.1.2.1 filter 3.1.2.2 map 3.1.2.3 flatMap 3.1.2.4 takeWhile 3.1.2.5 dropWhile 3.1.2.6 reduce 与 collect 3.2 Optional 3.2.1 Optional 对象创建 3.2.1.1 empty 3.2.1.2 of 3.2.1.3 ofNullable 3.2.2 方法 3.2.3 使用场景 3.2.3.1 判断结果不为空后使用 3.2.3.2 变量为空时提供默认值 3.2.3.3 变量为空时抛出异常,否则使用 1 Java8 函数式编程语法入门Java8 中函数式编程语法能够精简代码。 使用 Consumer 作为示例，它是一个函数式接口，包含一个抽象方法 accept，这个方法只有输入而无输出。 现在我们要定义一个 Consumer 对象，传统的方式是这样定义的： Consumer c = new Consumer() &#123; @Overridepublicvoidaccept(Object o) &#123; System.out.println(o); &#125; &#125;; 而在 Java8 中，针对函数式编程接口，可以这样定义： Consumer c = (o) -&gt; &#123; System.out.println(o); &#125;; 上面已说明，函数式编程接口都只有一个抽象方法，因此在采用这种写法时，编译器会将这段函数编译后当作该抽象方法的实现。 如果接口有多个抽象方法，编译器就不知道这段函数应该是实现哪个方法的了。 因此，= 后面的函数体我们就可以看成是 accept 函数的实现。 输入：-&gt;前面的部分，即被 () 包围的部分。此处只有一个输入参数，实际上输入是可以有多个的，如两个参数时写法：(a, b); 当然也可以没有输入，此时直接就可以是()。 函数体：-&gt;后面的部分，即被 {} 包围的部分；可以是一段代码。 输出：函数式编程可以没有返回值，也可以有返回值。如果有返回值时，需要代码段的最后一句通过 return 的方式返回对应的值。 当函数体中只有一个语句时，可以去掉 {} 进一步简化： Consumer c = (o) -&gt; System.out.println(o); 然而这还不是最简的，由于此处只是进行打印，调用了 System.out 中的 println 静态方法对输入参数直接进行打印，因此可以简化成以下写法： Consumer c = System.out::println; 它表示的意思就是针对输入的参数将其调用 System.out 中的静态方法 println 进行打印。 到这一步就可以感受到函数式编程的强大能力。 通过最后一段代码，我们可以简单的理解函数式编程，Consumer 接口直接就可以当成一个函数了，这个函数接收一个输入参数，然后针对这个输入进行处理；当然其本质上仍旧是一个对象，但我们已经省去了诸如老方式中的对象定义过程，直接使用一段代码来给函数式接口对象赋值。 而且最为关键的是，这个函数式对象因为本质上仍旧是一个对象，因此可以做为其它方法的参数或者返回值，可以与原有的代码实现无缝集成！ 下面对 Java 中的几个预先定义的函数式接口及其经常使用的类进行分析学习。 2 Java 函数式接口2.1 ConsumerConsumer 是一个函数式编程接口； 顾名思义，Consumer 的意思就是消费，即针对某个东西我们来使用它，因此它包含有一个有输入而无输出的 accept 接口方法； 除 accept 方法，它还包含有 andThen 这个方法； 其定义如下： default Consumer&lt;T&gt; andThen(Consumer&lt;? super T&gt; after) &#123; Objects.requireNonNull(after); return (T t) -&gt; &#123; accept(t); after.accept(t); &#125;; &#125; 可见这个方法就是指定在调用当前 Consumer 后是否还要调用其它的 Consumer； 使用示例： publicstaticvoidconsumerTest() &#123; Consumer f = System.out::println; Consumer f2 = n -&gt; System.out.println(n + &quot;-F2&quot;); //执行完F后再执行F2的Accept方法 f.andThen(f2).accept(&quot;test&quot;); //连续执行F的Accept方法 f.andThen(f).andThen(f).andThen(f).accept(&quot;test1&quot;); &#125; 2.2 FunctionFunction 也是一个函数式编程接口；它代表的含义是 “函数”，而函数经常是有输入输出的，因此它含有一个 apply 方法，包含一个输入与一个输出； 除 apply 方法外，它还有 compose 与 andThen 及 indentity 三个方法，其使用见下述示例； /** * Function测试 */publicstaticvoidfunctionTest() &#123; Function&lt;Integer, Integer&gt; f = s -&gt; s++; Function&lt;Integer, Integer&gt; g = s -&gt; s * 2; /** * 下面表示在执行F时，先执行G，并且执行F时使用G的输出当作输入。 * 相当于以下代码： * Integer a = g.apply(1); * System.out.println(f.apply(a)); */ System.out.println(f.compose(g).apply(1)); /** * 表示执行F的Apply后使用其返回的值当作输入再执行G的Apply； * 相当于以下代码 * Integer a = f.apply(1); * System.out.println(g.apply(a)); */ System.out.println(f.andThen(g).apply(1)); /** * identity方法会返回一个不进行任何处理的Function，即输出与输入值相等； */ System.out.println(Function.identity().apply(&quot;a&quot;)); &#125; 2.3 PredicatePredicate 为函数式接口，predicate 的中文意思是 “断定”，即判断的意思，判断某个东西是否满足某种条件； 因此它包含 test 方法，根据输入值来做逻辑判断，其结果为 True 或者 False。 它的使用方法示例如下： /** * Predicate测试 */privatestaticvoidpredicateTest() &#123; Predicate&lt;String&gt; p = o -&gt; o.equals(&quot;test&quot;); Predicate&lt;String&gt; g = o -&gt; o.startsWith(&quot;t&quot;); /** * negate: 用于对原来的Predicate做取反处理； * 如当调用p.test(&quot;test&quot;)为True时，调用p.negate().test(&quot;test&quot;)就会是False； */ Assert.assertFalse(p.negate().test(&quot;test&quot;)); /** * and: 针对同一输入值，多个Predicate均返回True时返回True，否则返回False； */ Assert.assertTrue(p.and(g).test(&quot;test&quot;)); /** * or: 针对同一输入值，多个Predicate只要有一个返回True则返回True，否则返回False */ Assert.assertTrue(p.or(g).test(&quot;ta&quot;)); &#125; 3 函数式编程接口的使用通过 Stream 以及 Optional 两个类，可以进一步利用函数式接口来简化代码。 3.1 StreamStream 可以对多个元素进行一系列的操作，也可以支持对某些操作进行并发处理。 3.1.1 Stream 对象的创建Stream 对象的创建途径有以下几种 a. 创建空的 Stream 对象 Stream stream = Stream.empty(); b. 通过集合类中的 stream 或者 parallelStream 方法创建； List&lt;String&gt; list = Arrays.asList(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;); Stream listStream = list.stream(); //获取串行的Stream对象 Stream parallelListStream = list.parallelStream(); //获取并行的Stream对象 c. 通过 Stream 中的 of 方法创建： Stream s = Stream.of(&quot;test&quot;); Stream s1 = Stream.of(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;); d. 通过 Stream 中的 iterate 方法创建： iterate 方法有两个不同参数的方法： publicstatic&lt;T&gt; Stream&lt;T&gt; iterate(final T seed, final UnaryOperator&lt;T&gt; f); publicstatic&lt;T&gt; Stream&lt;T&gt; iterate(T seed, Predicate&lt;? super T&gt; hasNext, UnaryOperator&lt;T&gt; next) 其中第一个方法将会返回一个无限有序值的 Stream 对象：它的第一个元素是 seed，第二个元素是 f.apply(seed); 第 N 个元素是 f.apply(n-1 个元素的值)；生成无限值的方法实际上与 Stream 的中间方法类似，在遇到中止方法前一般是不真正的执行的。因此无限值的这个方法一般与 limit 等方法一起使用，来获取前多少个元素。 当然获取前多少个元素也可以使用第二个方法。 第二个方法与第一个方法生成元素的方式类似，不同的是它返回的是一个有限值的 Stream；中止条件是由 hasNext 来断定的。 第二种方法的使用示例如下： /** * 本示例表示从1开始组装一个序列，第一个是1，第二个是1+1即2，第三个是2+1即3..，直接10时中止； * 也可简化成以下形式： * Stream.iterate(1, * n -&gt; n &lt;= 10, * n -&gt; n+1).forEach(System.out::println); * 写成以下方式是为简化理解 */ Stream.iterate(1, new Predicate&lt;Integer&gt;() &#123; @Overridepublicbooleantest(Integer integer) &#123; return integer &lt;= 10; &#125; &#125;, new UnaryOperator&lt;Integer&gt;() &#123; @Overridepublic Integer apply(Integer integer) &#123; return integer+1; &#125; &#125;).forEach(System.out::println); e. 通过 Stream 中的 generate 方法创建 与 iterate 中创建无限元素的 Stream 类似，不过它的每个元素与前一元素无关，且生成的是一个无序的队列。也就是说每一个元素都可以随机生成。因此一般用来创建常量的 Stream 以及随机的 Stream 等。 示例如下： /** * 随机生成10个Double元素的Stream并将其打印 */ Stream.generate(new Supplier&lt;Double&gt;() &#123; @Overridepublic Double get() &#123; return Math.random(); &#125; &#125;).limit(10).forEach(System.out::println); //上述写法可以简化成以下写法： Stream.generate(() -&gt; Math.random()).limit(10).forEach(System.out::println); f. 通过 Stream 中的 concat 方法连接两个 Stream 对象生成新的 Stream 对象 这个比较好理解不再赘述。 3.1.2 Stream 对象的使用Stream 对象提供多个非常有用的方法，这些方法可以分成两类： 中间操作：将原始的 Stream 转换成另外一个 Stream；如 filter 返回的是过滤后的 Stream。 终端操作：产生的是一个结果或者其它的复合操作；如 count 或者 forEach 操作。 其清单如下所示，方法的具体说明及使用示例见后文。 所有中间操作 方法说明sequential返回一个相等的串行的 Stream 对象，如果原 Stream 对象已经是串行就可能会返回原对象parallel返回一个相等的并行的 Stream 对象，如果原 Stream 对象已经是并行的就会返回原对象unordered返回一个不关心顺序的 Stream 对象，如果原对象已经是这类型的对象就会返回原对象onClose返回一个相等的 Steam 对象，同时新的 Stream 对象在执行 Close 方法时会调用传入的 Runnable 对象close关闭 Stream 对象filter元素过滤：对 Stream 对象按指定的 Predicate 进行过滤，返回的 Stream 对象中仅包含未被过滤的元素map元素一对一转换：使用传入的 Function 对象对 Stream 中的所有元素进行处理，返回的 Stream 对象中的元素为原元素处理后的结果mapToInt元素一对一转换：将原 Stream 中的使用传入的 IntFunction 加工后返回一个 IntStream 对象flatMap元素一对多转换：对原 Stream 中的所有元素进行操作，每个元素会有一个或者多个结果，然后将返回的所有元素组合成一个统一的 Stream 并返回；distinct去重：返回一个去重后的 Stream 对象sorted排序：返回排序后的 Stream 对象peek使用传入的 Consumer 对象对所有元素进行消费后，返回一个新的包含所有原来元素的 Stream 对象limit获取有限个元素组成新的 Stream 对象返回skip抛弃前指定个元素后使用剩下的元素组成新的 Stream 返回takeWhile如果 Stream 是有序的（Ordered），那么返回最长命中序列（符合传入的 Predicate 的最长命中序列）组成的 Stream；如果是无序的，那么返回的是所有符合传入的 Predicate 的元素序列组成的 Stream。dropWhile与 takeWhile 相反，如果是有序的，返回除最长命中序列外的所有元素组成的 Stream；如果是无序的，返回所有未命中的元素组成的 Stream。 所有终端操作 方法说明iterator返回 Stream 中所有对象的迭代器;spliterator返回对所有对象进行的 spliterator 对象forEach对所有元素进行迭代处理，无返回值forEachOrdered按 Stream 的 Encounter 所决定的序列进行迭代处理，无返回值toArray返回所有元素的数组reduce使用一个初始化的值，与 Stream 中的元素一一做传入的二合运算后返回最终的值。每与一个元素做运算后的结果，再与下一个元素做运算。它不保证会按序列执行整个过程。collect根据传入参数做相关汇聚计算min返回所有元素中最小值的 Optional 对象；如果 Stream 中无任何元素，那么返回的 Optional 对象为 Emptymax与 Min 相反count所有元素个数anyMatch只要其中有一个元素满足传入的 Predicate 时返回 True，否则返回 FalseallMatch所有元素均满足传入的 Predicate 时返回 True，否则 FalsenoneMatch所有元素均不满足传入的 Predicate 时返回 True，否则 FalsefindFirst返回第一个元素的 Optioanl 对象；如果无元素返回的是空的 Optional； 如果 Stream 是无序的，那么任何元素都可能被返回。findAny返回任意一个元素的 Optional 对象，如果无元素返回的是空的 Optioanl。isParallel判断是否当前 Stream 对象是并行的 下面就几个比较常用的方法举例说明其用法： 3.1.2.1 filter用于对 Stream 中的元素进行过滤，返回一个过滤后的 Stream 其方法定义如下： Stream&lt;T&gt; filter(Predicate&lt;? super T&gt; predicate); 使用示例： Stream&lt;String&gt; s = Stream.of(&quot;test&quot;, &quot;t1&quot;, &quot;t2&quot;, &quot;teeeee&quot;, &quot;aaaa&quot;); //查找所有包含t的元素并进行打印 s.filter(n -&gt; n.contains(&quot;t&quot;)).forEach(System.out::println); 3.1.2.2 map元素一对一转换。 它接收一个 Funcation 参数，用其对 Stream 中的所有元素进行处理，返回的 Stream 对象中的元素为 Function 对原元素处理后的结果 其方法定义如下： &lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super T, ? extends R&gt; mapper); 示例，假设我们要将一个 String 类型的 Stream 对象中的每个元素添加相同的后缀. txt，如 a 变成 a.txt，其写法如下： Stream&lt;String&gt; s = Stream.of(&quot;test&quot;, &quot;t1&quot;, &quot;t2&quot;, &quot;teeeee&quot;, &quot;aaaa&quot;); s.map(n -&gt; n.concat(&quot;.txt&quot;)).forEach(System.out::println); 3.1.2.3 flatMap元素一对多转换：对原 Stream 中的所有元素使用传入的 Function 进行处理，每个元素经过处理后生成一个多个元素的 Stream 对象，然后将返回的所有 Stream 对象中的所有元素组合成一个统一的 Stream 并返回； 方法定义如下： &lt;R&gt; Stream&lt;R&gt; flatMap(Function&lt;? super T, ? extends Stream&lt;? extends R&gt;&gt; mapper); 示例，假设要对一个 String 类型的 Stream 进行处理，将每一个元素的拆分成单个字母，并打印： Stream&lt;String&gt; s = Stream.of(&quot;test&quot;, &quot;t1&quot;, &quot;t2&quot;, &quot;teeeee&quot;, &quot;aaaa&quot;); s.flatMap(n -&gt; Stream.of(n.split(&quot;&quot;))).forEach(System.out::println); 3.1.2.4 takeWhile方法定义如下： default Stream&lt;T&gt; takeWhile(Predicate&lt;? super T&gt; predicate) 如果 Stream 是有序的（Ordered），那么返回最长命中序列（符合传入的 Predicate 的最长命中序列）组成的 Stream；如果是无序的，那么返回的是所有符合传入的 Predicate 的元素序列组成的 Stream。 与 Filter 有点类似，不同的地方就在当 Stream 是有序时，返回的只是最长命中序列。 如以下示例，通过 takeWhile 查找”test”, “t1”, “t2”, “teeeee”, “aaaa”, “taaa” 这几个元素中包含 t 的最长命中序列： Stream&lt;String&gt; s = Stream.of(&quot;test&quot;, &quot;t1&quot;, &quot;t2&quot;, &quot;teeeee&quot;, &quot;aaaa&quot;, &quot;taaa&quot;); //以下结果将打印： &quot;test&quot;, &quot;t1&quot;, &quot;t2&quot;, &quot;teeeee&quot;，最后的那个taaa不会进行打印 s.takeWhile(n -&gt; n.contains(&quot;t&quot;)).forEach(System.out::println); 3.1.2.5 dropWhile与 takeWhile 相反，如果是有序的，返回除最长命中序列外的所有元素组成的 Stream；如果是无序的，返回所有未命中的元素组成的 Stream; 其定义如下： default Stream&lt;T&gt; dropWhile(Predicate&lt;? super T&gt; predicate) 如以下示例，通过 dropWhile 删除”test”, “t1”, “t2”, “teeeee”, “aaaa”, “taaa” 这几个元素中包含 t 的最长命中序列： Stream&lt;String&gt; s = Stream.of(&quot;test&quot;, &quot;t1&quot;, &quot;t2&quot;, &quot;teeeee&quot;, &quot;aaaa&quot;, &quot;taaa&quot;); //以下结果将打印：&quot;aaaa&quot;, &quot;taaa&quot; s.dropWhile(n -&gt; n.contains(&quot;t&quot;)).forEach(System.out::println); 3.1.2.6 reduce 与 collect关于 reduce 与 collect 由于功能较为复杂，在后续将进行单独分析与学习，此处暂不涉及。 3.2 Optional用于简化 Java 中对空值的判断处理，以防止出现各种空指针异常。 Optional 实际上是对一个变量进行封装，它包含有一个属性 value，实际上就是这个变量的值。 3.2.1 Optional 对象创建它的构造函数都是 private 类型的，因此要初始化一个 Optional 的对象无法通过其构造函数进行创建。它提供了一系列的静态方法用于构建 Optional 对象: 3.2.1.1 empty用于创建一个空的 Optional 对象；其 value 属性为 Null。 如： Optional o = Optional.empty(); 3.2.1.2 of根据传入的值构建一个 Optional 对象; 传入的值必须是非空值，否则如果传入的值为空值，则会抛出空指针异常。 使用： o = Optional.of(&quot;test&quot;); 3.2.1.3 ofNullable根据传入值构建一个 Optional 对象 传入的值可以是空值，如果传入的值是空值，则与 empty 返回的结果是一样的。 3.2.2 方法Optional 包含以下方法： 方法名说明get获取 Value 的值，如果 Value 值是空值，则会抛出 NoSuchElementException 异常；因此返回的 Value 值无需再做空值判断，只要没有抛出异常，都会是非空值。isPresentValue 是否为空值的判断；ifPresent当 Value 不为空时，执行传入的 Consumer；ifPresentOrElseValue 不为空时，执行传入的 Consumer；否则执行传入的 Runnable 对象；filter当 Value 为空或者传入的 Predicate 对象调用 test(value) 返回 False 时，返回 Empty 对象；否则返回当前的 Optional 对象map一对一转换：当 Value 为空时返回 Empty 对象，否则返回传入的 Function 执行 apply(value) 后的结果组装的 Optional 对象；flatMap一对多转换：当 Value 为空时返回 Empty 对象，否则传入的 Function 执行 apply(value) 后返回的结果（其返回结果直接是 Optional 对象）or如果 Value 不为空，则返回当前的 Optional 对象；否则，返回传入的 Supplier 生成的 Optional 对象；stream如果 Value 为空，返回 Stream 对象的 Empty 值；否则返回 Stream.of(value) 的 Stream 对象；orElseValue 不为空则返回 Value，否则返回传入的值；orElseGetValue 不为空则返回 Value，否则返回传入的 Supplier 生成的值；orElseThrowValue 不为空则返回 Value，否则抛出 Supplier 中生成的异常对象； 3.2.3 使用场景常用的使用场景如下： 3.2.3.1 判断结果不为空后使用如某个函数可能会返回空值，以往的做法： String s = test(); if (null != s) &#123; System.out.println(s); &#125; 现在的写法就可以是： Optional&lt;String&gt; s = Optional.ofNullable(test()); s.ifPresent(System.out::println); 乍一看代码复杂度上差不多甚至是略有提升；那为什么要这么做呢？ 一般情况下，我们在使用某一个函数返回值时，要做的第一步就是去分析这个函数是否会返回空值；如果没有进行分析或者分析的结果出现偏差，导致函数会抛出空值而没有做检测，那么就会相应的抛出空指针异常！ 而有了 Optional 后，在我们不确定时就可以不用去做这个检测了，所有的检测 Optional 对象都帮忙我们完成，我们要做的就是按上述方式去处理。 3.2.3.2 变量为空时提供默认值如要判断某个变量为空时使用提供的值，然后再针对这个变量做某种运算； 以往做法： if (null == s) &#123; s = &quot;test&quot;; &#125; System.out.println(s); 现在的做法： Optional&lt;String&gt; o = Optional.ofNullable(s); System.out.println(o.orElse(&quot;test&quot;)); 3.2.3.3 变量为空时抛出异常，否则使用以往写法： if (null == s) &#123; thrownew Exception(&quot;test&quot;); &#125; System.out.println(s); 现在写法： Optional&lt;String&gt; o = Optional.ofNullable(s); System.out.println(o.orElseThrow(()-&gt;new Exception(&quot;test&quot;))); 其它场景待补充。 原文：https://blog.csdn.net/icarusliu/article/details/79495534","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"函数式编程","slug":"函数式编程","permalink":"https://blog.fenxiangz.com/tags/%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B/"},{"name":"lambda","slug":"lambda","permalink":"https://blog.fenxiangz.com/tags/lambda/"}]},{"title":"Java NIO客户端主动关闭连接，导致服务器空轮询","slug":"java/nio/2019-04-07_java_nio_debug","date":"2019-04-07T00:00:00.000Z","updated":"2020-12-20T16:47:02.970Z","comments":true,"path":"post/java/nio/2019-04-07_java_nio_debug.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/nio/2019-04-07_java_nio_debug.html","excerpt":"","text":"当客户端连接关闭时，服务器select()不会阻塞，然后一直分发读就绪操作，且读到的字节长度都是0，这是什么情况。 服务器代码: try &#123; ServerSocketChannel serverChannel = ServerSocketChannel.open(); serverChannel.bind(new InetSocketAddress(666)); serverChannel.configureBlocking(false); Selector selector = Selector.open(); serverChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) &#123; int count = selector.select(); //阻塞 if (count &gt; 0) &#123; Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = keys.iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); if (key.isAcceptable()) &#123; System.out.println(&quot;client connect&quot;); ServerSocketChannel serverSocketChannel = (ServerSocketChannel) key.channel(); SocketChannel sc = serverSocketChannel.accept(); sc.configureBlocking(false); sc.register(selector, SelectionKey.OP_READ); &#125; if (key.isReadable()) &#123; SocketChannel socketChannel = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(512); socketChannel.read(buffer); buffer.flip(); System.out.println(&quot;on read size:&quot; + buffer.remaining()); &#125; &#125; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; 客户端代码： public class NIOClientTest&#123; public static void main(String[] args) throws UnknownHostException, IOException&#123; try &#123; Socket socket = new Socket(&quot;127.0.0.1&quot;,666); try(OutputStreamWriter output = new OutputStreamWriter(socket.getOutputStream());)&#123; output.write(1); output.flush(); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; socket.close(); &#125; catch (Exception e1) &#123; e1.printStackTrace(); &#125; &#125; &#125; 解决： 当客户端主动切断连接的时候，服务端 Socket 的读事件（FD_READ）仍然起作用，也就是说，服务端 Socket 的状态仍然是有东西可读，当然此时读出来的字节肯定是 0。 socketChannel.read(buffer) 是有返回值的，这种情况下返回值是 -1，所以如果 read 方法返回的是 -1，就可以关闭和这个客户端的连接了。 SocketChannel.read 的返回值： 这种情况也有可能会抛出 IOException，需要捕获异常并判断。 nio的客户端如果关闭了，服务端还是会收到该channel的读事件，但是数目为0，而且会读到-1，其实-1在网络io中就是socket关闭的含义，在文件时末尾的含义，所以为了避免客户端关闭服务端一直收到读事件，必须检测上一次的读是不是-1，如果是-1，就关闭这个channel。 ByteBuffer buffer = ByteBuffer.allocate(100); SocketChannel sc = (SocketChannel) key.channel(); StringBuffer buf = new StringBuffer(); int c = 0; while ((c = sc.read(buffer)) &gt; 0) &#123; buf.append(new String(buffer.array())); &#125; if (c == -1) &#123; System.out.println(&quot;断开&quot;); sc.close(); &#125; String msg = buf.toString(); System.out.println(msg);","categories":[{"name":"Java NIO","slug":"Java-NIO","permalink":"https://blog.fenxiangz.com/categories/Java-NIO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"NIO","slug":"NIO","permalink":"https://blog.fenxiangz.com/tags/NIO/"},{"name":"问题","slug":"问题","permalink":"https://blog.fenxiangz.com/tags/%E9%97%AE%E9%A2%98/"}]},{"title":"NIO 各种使用注意点","slug":"java/nio/2019-04-07_java_nio_使用注意点","date":"2019-04-07T00:00:00.000Z","updated":"2020-12-20T16:47:02.970Z","comments":true,"path":"post/java/nio/2019-04-07_java_nio_使用注意点.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/nio/2019-04-07_java_nio_%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E7%82%B9.html","excerpt":"","text":"找问题的时后发现了这篇文章，惊为天人，几乎涵盖了我所有碰到的坑，非常不错！ 不得不说，NIO的API设计的够难用的，坑还巨多….这也是为什么大家都不直接使用nio的原因吧，一般会用mina或者netty啥的（这是个记录的博客，所以会不断更新） 关于通道本身的一些注意点，请参考我之前的：nio通道(2)—几个注意点 其他一些参考nio summary 1 坑爹的事件SocetChannel和ServerSocketChannel各自支持的事件，在前面已经提到： http://www.360doc.com/content/12/0902/17/495229_233773276.shtml 1.1 什么时候可以register客户端：linux:SocketChannel: 注册了op_read,op_write,op_connect的SocketChannel在connect之前，open之后，都可以select到，只不过不能够read和writewindows:SocketChannel:只有注册了op_connect的SocketChannel在connect之后，才被select到。是一个正确的符合逻辑的理解。 服务器端： (2)如果只注册了读的操作，则select时，会发生阻塞，因为是没有为读准备好的socket(3)如果没有可写的socket，则select时，不会发生阻塞，直接返回0。如果阻塞写，只能是发送区满。 另外：iterator到selectedKey之后，需要将该key移除出selectedKey。如果不移出，例如OP_ACCEPT，则再下次accept之后，会产生空的SocketChannel。 1.2 connect事件（连接–成功or失败？）在之前的Socket通道中，已经看到，非阻塞模式下，connect操作会返回false，后面会发出CONNECT事件来表示连接，但是这里其实没有区分成功还是失败。。 connect事件：表示连接通道连接就绪或者发生了错误，会被加到ready 集合中（下面面是API说明） If the selector detects that the corresponding socket channel is ready to complete its connection sequence, or has an error pending, then it will add OP_CONNECT to the key’s ready set and add the key to its selected-key set. 所以这个事件发生的时候不能简单呢的认为连接成功，要使用finishConnect判断下，如果连接失败，会抛出异常 NIO就绪处理之OP_CONNECT if (key.isValid() &amp;&amp; key.isConnectable()) &#123; SocketChannel ch = (SocketChannel) key.channel(); if (ch.finishConnect()) &#123; // Connect successfully // key.interestOps(SelectionKey.OP_READ); &#125; else &#123; // Connect failed &#125; &#125; 1.3 read&amp;关闭一直很奇怪，为啥没有close事件，终于在一次实验的时候发现： 1.启动一个客户端和服务端 2.关闭客户端，服务端会发生一个read事件，并且在read的时候抛出异常，来表示关闭 另外，这个事件会不断发生，就算从已准备好的集合移除也没有，必须将该channel关闭或者调用哪个该key的cancel方法，因为SelectionKey代表的是Selector和Channel之间的联系，所以在Channel关闭了之后，对于Selector来说，这个Channel永远都会发出关闭这个事件，表明自己关闭了，直到从该Selector移除去 3.服务端关闭，client端在write的时候会抛出异常 java.io.IOException: 远程主机强迫关闭了一个现有的连接。 at sun.nio.ch.SocketDispatcher.write0(Native Method) at sun.nio.ch.SocketDispatcher.write(Unknown Source) 1.4 还是关闭(TIME_WAIT)NIO的SelectableChannel关闭的一个问题 如果在取消SelectionKey（这时候只是加入取消的键集合，下一次select才会执行）后没有调用到selector的select方法（因为Client一般在取消key后，我们都会终止调用select的循环，当然，server关闭一个注册的channel我们是不会终止select循环的），那么本地socket将进入CLOSE-WAIT状态（等待本地Socket关闭）。简单的解决办法是在 SelectableChannel.close方法之后调用Selector.selectNow方法 Netty在超过256连接关闭的时候主动调用一次selectNow 1.5 writeNIO就绪处理之OP_WRITE 一开始很多人以为write事件，表示在调用channel的write方法之后，就会发生这个事件，然后channel再会把数据真正写出，但是实际上，写操作的就绪条件为底层缓冲区有空闲空间，而写缓冲区绝大部分时间都是有空闲空间的，所以当你注册写事件后，写操作一直是就绪的，选择处理线程全占用整个CPU资源。所以，只有当你确实有数据要写时再注册写操作，并在写完以后马上取消注册，一般的，Client端需要注册OP_CONNECT,OP_READ;Server端需要注册OP_ACCEPT并且连接之后注册OP_READ 当有数据在写时，将数据写到缓冲区中，并注册写事件。 [java]view plaincopy public void write(byte\\[\\] data) throws IOException &#123; writeBuffer.put(data); key.interestOps(SelectionKey.OP_WRITE); &#125; 注册写事件后，写操作就绪，这时将之前写入缓冲区的数据写入通道，并取消注册。[java]view plaincopy 大部分情况下，其实直接用write方法写就好了，没必要用写事件。 另外，关于write还可以参考下面的4.2的一些注意点 2 Channel的bind方法Socket/ServerSocket 两者都有bind方法，表示绑定到某个端口，在绑定之后，前者调用connect方法，表示连接到某个服务端；后者要在后面调用accept方法，监听到来的连接请求（一个Socket句柄包含了两个地址对，本地ip:port—-远程ip:port） 3 Selector的select和wakeup机制Java NIO类库Selector机制解析（上）Java NIO类库Selector机制解析（下）Java NIO 类库Selector机制解析（续） 在windows平台下，调用Selector.open()方法，会自己和自己建立两条TCP连接，消耗了两个TCP连接和端口，也消耗了文件描述符 在linux平台下，会自己和自己建立两条管道，消耗了两个系统的文件描述符 一个阻塞seelct上的线程想要被唤醒，有3种方式： 1.有数据可读/.可写，或者出现异常 2.阻塞时间到，time out 3.收到一个non-block信号，由kill或者pthread_kill发出 这两个方法完全是来模仿Linux中的的kill和pthread_kill给阻塞在select上的线程发信号的。但因为发信号这个东西并不是一个跨平台的标准（pthread_kill这个系统调用也不是所有Unix/Linux都支持的），而pipe是所有的Unix/Linux所支持的，但Windows又不支持，所以，Windows用了TCP连接来实现这个事。(在Linux下使用pipe管道) 4 一些特殊情况4.1 读不满因为我们的数据都是偏业务性的，比如使用开头一个字节来表示后面数据的长度，接着就会等待读取到那么多数据，但是TCP是流式的协议，100字节的数据可能是一段段发送过来的，所以在没有读到完整的数据前需要等待。 这时候可以将buffer attach到key上，下次read发生的时候再继续读取，但是也有另外一种说法，在网络条件比较好的情况下，直接使用一个临时selector会减少上下文切换。。这个不太明白 4.2 写不出去在发送缓冲区空间不够的情况下，write方法可能会返回能够写出去的字节数，比如只剩50字节，你写入100字节，这时候write会返回50，即往缓冲区写入了50字节 在网络较好的情况下，这应该是不太可能发生的，一般都是网络有问题，重传率很高 详细的情况可以参考：java nio对OP_WRITE的处理解决网速慢的连接 while (bb.hasRemaining()) &#123; int len = socketChannel.write(bb); if (len &lt; 0) &#123; throw new EOFException(); &#125; &#125; 由于缓冲区一直蛮，下面的代码会一直执行，占用CPU100%，因此推荐的方式如下 while (bb.hasRemaining()) &#123; int len = socketChannel.write(bb); if (len &lt; 0) &#123; throw new EOFException(); &#125; if (len == 0) &#123; selectionKey.interestOps( selectionKey.interestOps() | SelectionKey.OP_WRITE); mainSelector.wakeup(); break; &#125; &#125; 如果返回0，表示缓冲区满，那么注册WRITE事件，缓冲区不满的情况下，就会触发WRITE事件，在那时候再写入，可以避免不要的消耗。（另外Grizzly还是用了另一种方式，也可以从上面的参考链接得到） 原文：https://blog.csdn.net/asdasdasd123123123/article/details/88253912","categories":[{"name":"Java NIO","slug":"Java-NIO","permalink":"https://blog.fenxiangz.com/categories/Java-NIO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"NIO","slug":"NIO","permalink":"https://blog.fenxiangz.com/tags/NIO/"},{"name":"问题","slug":"问题","permalink":"https://blog.fenxiangz.com/tags/%E9%97%AE%E9%A2%98/"}]},{"title":"基于Netty自己动手实现RPC框架","slug":"java/netty/2019-03-26_基于Netty自己动手实现RPC框架","date":"2019-03-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.968Z","comments":true,"path":"post/java/netty/2019-03-26_基于Netty自己动手实现RPC框架.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/netty/2019-03-26_%E5%9F%BA%E4%BA%8ENetty%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E5%AE%9E%E7%8E%B0RPC%E6%A1%86%E6%9E%B6.html","excerpt":"","text":"今天我们要来做一道小菜，这道菜就是RPC通讯框架。它使用netty作为原料，fastjson序列化工具作为调料，来实现一个极简的多线程RPC服务框架。 我们暂且命名该RPC框架为rpckids。 食用指南在告诉读者完整的制作菜谱之前，我们先来试试这个小菜怎么个吃法，好不好吃，是不是吃起来很方便。如果读者觉得很难吃，那后面的菜谱就没有多大意义了，何必花心思去学习制作一门谁也不爱吃的大烂菜呢？ 例子中我会使用rpckids提供的远程RPC服务，用于计算斐波那契数和指数，客户端通过rpckids提供的RPC客户端向远程服务传送参数，并接受返回结果，然后呈现出来。你可以使用rpckids定制任意的业务rpc服务。 斐波那契数输入输出比较简单，一个Integer，一个Long。 指数输入有两个值，输出除了计算结果外还包含计算耗时，以纳秒计算。之所以包含耗时，只是为了呈现一个完整的自定义的输入和输出类。 指数服务自定义输入输出类// 指数RPC的输入 public class ExpRequest &#123; private int base; private int exp; // constructor &amp; getter &amp; setter &#125; // 指数RPC的输出 public class ExpResponse &#123; private long value; private long costInNanos; // constructor &amp; getter &amp; setter &#125; 斐波那契和指数计算处理public class FibRequestHandler implements IMessageHandler&lt;Integer&gt; &#123; private List&lt;Long&gt; fibs = new ArrayList&lt;&gt;(); &#123; fibs.add(1L); // fib(0) = 1 fibs.add(1L); // fib(1) = 1 &#125; @Override public void handle(ChannelHandlerContext ctx, String requestId, Integer n) &#123; for (int i = fibs.size(); i &lt; n + 1; i++) &#123; long value = fibs.get(i - 2) + fibs.get(i - 1); fibs.add(value); &#125; // 输出响应 ctx.writeAndFlush(new MessageOutput(requestId, &quot;fib_res&quot;, fibs.get(n))); &#125; &#125; public class ExpRequestHandler implements IMessageHandler&lt;ExpRequest&gt; &#123; @Override public void handle(ChannelHandlerContext ctx, String requestId, ExpRequest message) &#123; int base = message.getBase(); int exp = message.getExp(); long start = System.nanoTime(); long res = 1; for (int i = 0; i &lt; exp; i++) &#123; res *= base; &#125; long cost = System.nanoTime() - start; // 输出响应 ctx.writeAndFlush(new MessageOutput(requestId, &quot;exp_res&quot;, new ExpResponse(res, cost))); &#125; &#125; 构建RPC服务器RPC服务类要监听指定IP端口，设定io线程数和业务计算线程数，然后注册斐波那契服务输入类和指数服务输入类，还有相应的计算处理器。 public class DemoServer &#123; public static void main(String[] args) &#123; RPCServer server = new RPCServer(&quot;localhost&quot;, 8888, 2, 16); server.service(&quot;fib&quot;, Integer.class, new FibRequestHandler()) .service(&quot;exp&quot;, ExpRequest.class, new ExpRequestHandler()); server.start(); &#125; &#125; 构建RPC客户端RPC客户端要链接远程IP端口，并注册服务输出类(RPC响应类)，然后分别调用20次斐波那契服务和指数服务，输出结果 public class DemoClient &#123; private RPCClient client; public DemoClient(RPCClient client) &#123; this.client = client; // 注册服务返回类型 this.client.rpc(&quot;fib_res&quot;, Long.class).rpc(&quot;exp_res&quot;, ExpResponse.class); &#125; public long fib(int n) &#123; return (Long) client.send(&quot;fib&quot;, n); &#125; public ExpResponse exp(int base, int exp) &#123; return (ExpResponse) client.send(&quot;exp&quot;, new ExpRequest(base, exp)); &#125; public static void main(String[] args) &#123; RPCClient client = new RPCClient(&quot;localhost&quot;, 8888); DemoClient demo = new DemoClient(client); for (int i = 0; i &lt; 20; i++) &#123; System.out.printf(&quot;fib(%d) = %d\\n&quot;, i, demo.fib(i)); &#125; for (int i = 0; i &lt; 20; i++) &#123; ExpResponse res = demo.exp(2, i); System.out.printf(&quot;exp2(%d) = %d cost=%dns\\n&quot;, i, res.getValue(), res.getCostInNanos()); &#125; &#125; &#125; 运行先运行服务器，服务器输出如下，从日志中可以看到客户端链接过来了，然后发送了一系列消息，最后关闭链接走了。 server started @ localhost:8888 connection comes read a message read a message ... connection leaves 再运行客户端，可以看到一些列的计算结果都成功完成了输出。 fib(0) = 1 fib(1) = 1 fib(2) = 2 fib(3) = 3 fib(4) = 5 ... exp2(0) = 1 cost=559ns exp2(1) = 2 cost=495ns exp2(2) = 4 cost=524ns exp2(3) = 8 cost=640ns exp2(4) = 16 cost=711ns ... 牢骚本以为是小菜一碟，但是编写完整的代码和文章却将近花费了一天的时间，深感写码要比做菜耗时太多了。因为只是为了教学目的，所以在实现细节上还有好多没有仔细去雕琢的地方。如果是要做一个开源项目，力求非常完美的话。至少还要考虑一下几点。 客户端连接池 多服务进程负载均衡 日志输出 参数校验，异常处理 客户端流量攻击 服务器压力极限 如果要参考grpc的话，还得实现流式响应处理。如果还要为了节省网络流量的话，又需要在协议上下功夫。这一大堆的问题还是抛给读者自己思考去吧。 关注公众号「码洞」，发送「RPC」即可获取以上完整菜谱的GitHub开源代码链接。读者有什么不明白的地方，洞主也会一一解答。 下面我们接着讲RPC服务器和客户端精细的制作过程 服务器菜谱定义消息输入输出格式，消息类型、消息唯一ID和消息的json序列化字符串内容。消息唯一ID是用来客户端验证服务器请求和响应是否匹配。 public class MessageInput &#123; private String type; private String requestId; private String payload; public MessageInput(String type, String requestId, String payload) &#123; this.type = type; this.requestId = requestId; this.payload = payload; &#125; public String getType() &#123; return type; &#125; public String getRequestId() &#123; return requestId; &#125; // 因为我们想直接拿到对象，所以要提供对象的类型参数 public &lt;T&gt; T getPayload(Class&lt;T&gt; clazz) &#123; if (payload == null) &#123; return null; &#125; return JSON.parseObject(payload, clazz); &#125; &#125; public class MessageOutput &#123; private String requestId; private String type; private Object payload; public MessageOutput(String requestId, String type, Object payload) &#123; this.requestId = requestId; this.type = type; this.payload = payload; &#125; public String getType() &#123; return this.type; &#125; public String getRequestId() &#123; return requestId; &#125; public Object getPayload() &#123; return payload; &#125; &#125; 消息解码器，使用Netty的ReplayingDecoder实现。简单起见，这里没有使用checkpoint去优化性能了，感兴趣的话读者可以参考一下我之前在公众号里发表的相关文章，将checkpoint相关的逻辑自己添加进去。 public class MessageDecoder extends ReplayingDecoder&lt;MessageInput&gt; &#123; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; String requestId = readStr(in); String type = readStr(in); String content = readStr(in); out.add(new MessageInput(type, requestId, content)); &#125; private String readStr(ByteBuf in) &#123; // 字符串先长度后字节数组，统一UTF8编码 int len = in.readInt(); if (len &lt; 0 || len &gt; (1 &lt;&lt; 20)) &#123; throw new DecoderException(&quot;string too long len=&quot; + len); &#125; byte[] bytes = new byte[len]; in.readBytes(bytes); return new String(bytes, Charsets.UTF8); &#125; &#125; 消息处理器接口，每个自定义服务必须实现handle方法 public interface IMessageHandler&lt;&gt; &#123; Tvoid handle(ChannelHandlerContext ctx, String requestId, T message); &#125; // 找不到类型的消息统一使用默认处理器处理 public class DefaultHandler implements IMessageHandler&lt;MessageInput&gt; &#123; @Override public void handle(ChannelHandlerContext ctx, String requesetId, MessageInput input) &#123; System.out.println(&quot;unrecognized message type=&quot; + input.getType() + &quot; comes&quot;); &#125; &#125; 消息类型注册中心和消息处理器注册中心，都是用静态字段和方法，其实也是为了图方便，写成非静态的可能会优雅一些。 public class MessageRegistry &#123; private static Map&lt;String, Class&lt;?&gt;&gt; clazzes = new HashMap&lt;&gt;(); public static void register(String type, Class&lt;?&gt; clazz) &#123; clazzes.put(type, clazz); &#125; public static Class&lt;?&gt; get(String type) &#123; return clazzes.get(type); &#125; &#125; public class MessageHandlers &#123; private static Map&lt;String, IMessageHandler&lt;?&gt;&gt; handlers = new HashMap&lt;&gt;(); public static DefaultHandler defaultHandler = new DefaultHandler(); public static void register(String type, IMessageHandler&lt;?&gt; handler) &#123; handlers.put(type, handler); &#125; public static IMessageHandler&lt;?&gt; get(String type) &#123; IMessageHandler&lt;?&gt; handler = handlers.get(type); return handler; &#125; &#125; 响应消息的编码器比较简单 @Sharable public class MessageEncoder extends MessageToMessageEncoder&lt;MessageOutput&gt; &#123; @Override protected void encode(ChannelHandlerContext ctx, MessageOutput msg, List&lt;Object&gt; out) throws Exception &#123; ByteBuf buf = PooledByteBufAllocator.DEFAULT.directBuffer(); writeStr(buf, msg.getRequestId()); writeStr(buf, msg.getType()); writeStr(buf, JSON.toJSONString(msg.getPayload())); out.add(buf); &#125; private void writeStr(ByteBuf buf, String s) &#123; buf.writeInt(s.length()); buf.writeBytes(s.getBytes(Charsets.UTF8)); &#125; &#125; 好，接下来进入关键环节，将上面的小模小块凑在一起，构建一个完整的RPC服务器框架，这里就需要读者有必须的Netty基础知识了，需要编写Netty的事件回调类和服务构建类。 @Sharable public class MessageCollector extends ChannelInboundHandlerAdapter &#123; // 业务线程池 private ThreadPoolExecutor executor; public MessageCollector(int workerThreads) &#123; // 业务队列最大1000，避免堆积 // 如果子线程处理不过来，io线程也会加入处理业务逻辑(callerRunsPolicy) BlockingQueue&lt;Runnable&gt; queue = new ArrayBlockingQueue&lt;&gt;(1000); // 给业务线程命名 ThreadFactory factory = new ThreadFactory() &#123; AtomicInteger seq = new AtomicInteger(); @Override public Thread newThread(Runnable r) &#123; Thread t = new Thread(r); t.setName(&quot;rpc-&quot; + seq.getAndIncrement()); return t; &#125; &#125;; // 闲置时间超过30秒的线程自动销毁 this.executor = new ThreadPoolExecutor(1, workerThreads, 30, TimeUnit.SECONDS, queue, factory, new CallerRunsPolicy()); &#125; public void closeGracefully() &#123; // 优雅一点关闭，先通知，再等待，最后强制关闭 this.executor.shutdown(); try &#123; this.executor.awaitTermination(10, TimeUnit.SECONDS); &#125; catch (InterruptedException e) &#123; &#125; this.executor.shutdownNow(); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; // 客户端来了一个新链接 System.out.println(&quot;connection comes&quot;); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; // 客户端走了一个 System.out.println(&quot;connection leaves&quot;); ctx.close(); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof MessageInput) &#123; System.out.println(&quot;read a message&quot;); // 用业务线程池处理消息 this.executor.execute(() -&gt; &#123; this.handleMessage(ctx, (MessageInput) msg); &#125;); &#125; &#125; private void handleMessage(ChannelHandlerContext ctx, MessageInput input) &#123; // 业务逻辑在这里 Class&lt;?&gt; clazz = MessageRegistry.get(input.getType()); if (clazz == null) &#123; // 没注册的消息用默认的处理器处理 MessageHandlers.defaultHandler.handle(ctx, input.getRequestId(), input); return; &#125; Object o = input.getPayload(clazz); // 这里是小鲜的瑕疵，代码外观上比较难看，但是大厨表示才艺不够，很无奈 // 读者如果感兴趣可以自己想办法解决 @SuppressWarnings(&quot;unchecked&quot;) IMessageHandler&lt;Object&gt; handler = (IMessageHandler&lt;Object&gt;) MessageHandlers.get(input.getType()); if (handler != null) &#123; handler.handle(ctx, input.getRequestId(), o); &#125; else &#123; // 用默认的处理器处理吧 MessageHandlers.defaultHandler.handle(ctx, input.getRequestId(), input); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; // 此处可能因为客户端机器突发重启 // 也可能是客户端链接闲置时间超时，后面的ReadTimeoutHandler抛出来的异常 // 也可能是消息协议错误，序列化异常 // etc. // 不管它，链接统统关闭，反正客户端具备重连机制 System.out.println(&quot;connection error&quot;); cause.printStackTrace(); ctx.close(); &#125; &#125; public class RPCServer &#123; private String ip; private int port; private int ioThreads; // 用来处理网络流的读写线程 private int workerThreads; // 用于业务处理的计算线程 public RPCServer(String ip, int port, int ioThreads, int workerThreads) &#123; this.ip = ip; this.port = port; this.ioThreads = ioThreads; this.workerThreads = workerThreads; &#125; private ServerBootstrap bootstrap; private EventLoopGroup group; private MessageCollector collector; private Channel serverChannel; // 注册服务的快捷方式 public RPCServer service(String type, Class&lt;?&gt; reqClass, IMessageHandler&lt;?&gt; handler) &#123; MessageRegistry.register(type, reqClass); MessageHandlers.register(type, handler); return this; &#125; // 启动RPC服务 public void start() &#123; bootstrap = new ServerBootstrap(); group = new NioEventLoopGroup(ioThreads); bootstrap.group(group); collector = new MessageCollector(workerThreads); MessageEncoder encoder = new MessageEncoder(); bootstrap.channel(NioServerSocketChannel.class).childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipe = ch.pipeline(); // 如果客户端60秒没有任何请求，就关闭客户端链接 pipe.addLast(new ReadTimeoutHandler(60)); // 挂上解码器 pipe.addLast(new MessageDecoder()); // 挂上编码器 pipe.addLast(encoder); // 将业务处理器放在最后 pipe.addLast(collector); &#125; &#125;); bootstrap.option(ChannelOption.SO_BACKLOG, 100) // 客户端套件字接受队列大小 .option(ChannelOption.SO_REUSEADDR, true) // reuse addr，避免端口冲突 .option(ChannelOption.TCP_NODELAY, true) // 关闭小流合并，保证消息的及时性 .childOption(ChannelOption.SO_KEEPALIVE, true); // 长时间没动静的链接自动关闭 serverChannel = bootstrap.bind(this.ip, this.port).channel(); System.out.printf(&quot;server started @ %s:%d\\n&quot;, ip, port); &#125; public void stop() &#123; // 先关闭服务端套件字 serverChannel.close(); // 再斩断消息来源，停止io线程池 group.shutdownGracefully(); // 最后停止业务线程 collector.closeGracefully(); &#125; &#125; 上面就是完整的服务器菜谱，代码较多，读者如果没有Netty基础的话，可能会看得眼花缭乱。如果你不常使用JDK的Executors框架，阅读起来估计也够呛。如果读者需要相关学习资料，可以找我索取。 客户端菜谱服务器使用NIO实现，客户端也可以使用NIO实现，不过必要性不大，用同步的socket实现也是没有问题的。更重要的是，同步的代码比较简短，便于理解。所以简单起见，这里使用了同步IO。 定义RPC请求对象和响应对象，和服务器一一对应。 public class RPCRequest &#123; private String requestId; private String type; private Object payload; public RPCRequest(String requestId, String type, Object payload) &#123; this.requestId = requestId; this.type = type; this.payload = payload; &#125; public String getRequestId() &#123; return requestId; &#125; public String getType() &#123; return type; &#125; public Object getPayload() &#123; return payload; &#125; &#125; public class RPCResponse &#123; private String requestId; private String type; private Object payload; public RPCResponse(String requestId, String type, Object payload) &#123; this.requestId = requestId; this.type = type; this.payload = payload; &#125; public String getRequestId() &#123; return requestId; &#125; public void setRequestId(String requestId) &#123; this.requestId = requestId; &#125; public String getType() &#123; return type; &#125; public void setType(String type) &#123; this.type = type; &#125; public Object getPayload() &#123; return payload; &#125; public void setPayload(Object payload) &#123; this.payload = payload; &#125; &#125; 定义客户端异常，用于统一抛出RPC错误 public class RPCException extends RuntimeException &#123; private static final long serialVersionUID = 1L; public RPCException(String message, Throwable cause) &#123; super(message, cause); &#125; public RPCException(String message) &#123; super(message); &#125; public RPCException(Throwable cause) &#123; super(cause); &#125; &#125; 请求ID生成器，简单的UUID64 public class RequestId &#123; public static String next() &#123; return UUID.randomUUID().toString(); &#125; &#125; 响应类型注册中心，和服务器对应 public class ResponseRegistry &#123; private static Map&lt;String, Class&lt;?&gt;&gt; clazzes = new HashMap&lt;&gt;(); public static void register(String type, Class&lt;?&gt; clazz) &#123; clazzes.put(type, clazz); &#125; public static Class&lt;?&gt; get(String type) &#123; return clazzes.get(type); &#125; &#125; 好，接下来进入客户端的关键环节，链接管理、读写消息、链接重连都在这里 public class RPCClient &#123; private String ip; private int port; private Socket sock; private DataInputStream input; private OutputStream output; public RPCClient(String ip, int port) &#123; this.ip = ip; this.port = port; &#125; public void connect() throws IOException &#123; SocketAddress addr = new InetSocketAddress(ip, port); sock = new Socket(); sock.connect(addr, 5000); // 5s超时 input = new DataInputStream(sock.getInputStream()); output = sock.getOutputStream(); &#125; public void close() &#123; // 关闭链接 try &#123; sock.close(); sock = null; input = null; output = null; &#125; catch (IOException e) &#123; &#125; &#125; public Object send(String type, Object payload) &#123; // 普通rpc请求，正常获取响应 try &#123; return this.sendInternal(type, payload, false); &#125; catch (IOException e) &#123; throw new RPCException(e); &#125; &#125; public RPCClient rpc(String type, Class&lt;?&gt; clazz) &#123; // rpc响应类型注册快捷入口 ResponseRegistry.register(type, clazz); return this; &#125; public void cast(String type, Object payload) &#123; // 单向消息，服务器不得返回结果 try &#123; this.sendInternal(type, payload, true); &#125; catch (IOException e) &#123; throw new RPCException(e); &#125; &#125; private Object sendInternal(String type, Object payload, boolean cast) throws IOException &#123; if (output == null) &#123; connect(); &#125; String requestId = RequestId.next(); ByteArrayOutputStream bytes = new ByteArrayOutputStream(); DataOutputStream buf = new DataOutputStream(bytes); writeStr(buf, requestId); writeStr(buf, type); writeStr(buf, JSON.toJSONString(payload)); buf.flush(); byte[] fullLoad = bytes.toByteArray(); try &#123; // 发送请求 output.write(fullLoad); &#125; catch (IOException e) &#123; // 网络异常要重连 close(); connect(); output.write(fullLoad); &#125; if (!cast) &#123; // RPC普通请求，要立即获取响应 String reqId = readStr(); // 校验请求ID是否匹配 if (!requestId.equals(reqId)) &#123; close(); throw new RPCException(&quot;request id mismatch&quot;); &#125; String typ = readStr(); Class&lt;?&gt; clazz = ResponseRegistry.get(typ); // 响应类型必须提前注册 if (clazz == null) &#123; throw new RPCException(&quot;unrecognized rpc response type=&quot; + typ); &#125; // 反序列化json串 String payld = readStr(); Object res = JSON.parseObject(payld, clazz); return res; &#125; return null; &#125; private String readStr() throws IOException &#123; int len = input.readInt(); byte[] bytes = new byte[len]; input.readFully(bytes); return new String(bytes, Charsets.UTF8); &#125; private void writeStr(DataOutputStream out, String s) throws IOException &#123; out.writeInt(s.length()); out.write(s.getBytes(Charsets.UTF8)); &#125; &#125; 牢骚重提本以为是小菜一碟，但是编写完整的代码和文章却将近花费了一天的时间，深感写码要比做菜耗时太多了。因为只是为了教学目的，所以在实现细节上还有好多没有仔细去雕琢的地方。如果是要做一个开源项目，力求非常完美的话。至少还要考虑一下几点。 客户端连接池 多服务进程负载均衡 日志输出 参数校验，异常处理 客户端流量攻击 服务器压力极限 如果要参考grpc的话，还得实现流式响应处理。如果还要为了节省网络流量的话，又需要在协议上下功夫。这一大堆的问题还是抛给读者自己思考去吧。 关注公众号「码洞」，发送「RPC」即可获取以上完整菜谱的GitHub开源代码链接。读者有什么不明白的地方，洞主也会一一解答。原文：https://juejin.im/post/5ad2a99ff265da238d51264d","categories":[{"name":"Netty","slug":"Netty","permalink":"https://blog.fenxiangz.com/categories/Netty/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"Netty","slug":"Netty","permalink":"https://blog.fenxiangz.com/tags/Netty/"},{"name":"RPC","slug":"RPC","permalink":"https://blog.fenxiangz.com/tags/RPC/"}]},{"title":"Java 和操作系统交互细节","slug":"java/advance/2019-03-06_java_os","date":"2019-03-06T00:00:00.000Z","updated":"2020-12-20T16:47:02.960Z","comments":true,"path":"post/java/advance/2019-03-06_java_os.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2019-03-06_java_os.html","excerpt":"","text":"结合 CPU 理解一行 Java 代码是怎么执行的根据冯·诺依曼思想，计算机采用二进制作为数制基础，必须包含:运算器、控制器、存储设备，以及输入输出设备，如下图所示。 (该图来源于百度) 我们先来分析 CPU 的工作原理，现代 CPU 芯片中大都集成了，控制单元，运算单元，存储单元。控制单元是 CPU 的控制中心， CPU 需要通过它才知道下一步做什么，也就是执行什么指令，控制单元又包含:指令寄存器( IR )，指令译码器( ID )和操作控制器( OC ).当程序被加载进内存后，指令就在内存中了，这个时候说的内存是独立于 CPU 外的主存设备，也就是 PC 机中的内存条，指令指针寄存器IP 指向内存中下一条待执行指令的地址，控制单元根据 IP寄存器的指向，将主存中的指令装载到指令寄存器，这个指令寄存器也是一个存储设备，不过他集成在 CPU 内部，指令从主存到达 CPU 后只是一串 010101 的二进制串，还需要通过译码器解码，分析出操作码是什么，操作数在哪，之后就是具体的运算单元进行算术运算(加减乘除)，逻辑运算(比较，位移).而 CPU 指令执行过程大致为:取址(去主存获取指令放到寄存器)，译码(从主存获取操作数放入高速缓存 L1 )，执行(运算). 这里解释下上图中 CPU 内部集成的存储单元 SRAM ，正好和主存中的 DRAM 对应， RAM 是随机访问内存，就是给一个地址就能访问到数据，而磁盘这种存储媒介必须顺序访问，而 RAM 又分为动态和静态两种，静态 RAM 由于集成度较低，一般容量小，速度快，而动态 RAM 集成度较高，主要通过给电容充电和放电实现，速度没有静态 RAM 快，所以一般将动态 RAM 做为主存，而静态 RAM 作为 CPU 和主存之间的高速缓存(cache)，用来屏蔽 CPU 和主存速度上的差异，也就是我们经常看到的 L1 ， L2 缓存。每一级别缓存速度变低，容量变大。下图展示了存储器的层次化架构，以及 CPU 访问主存的过程，这里有两个知识点，一个是多级缓存之间为保证数据的一致性，而推出的缓存一致性协议，具体可以参考这篇文章，另外一个知识点是， cache 和主存的映射，首先要明确的是 cahce 缓存的单位是缓存行，对应主存中的一个内存块，并不是一个变量，这个主要是因为 ** CPU 访问的空间局限性:被访问的某个存储单元，在一个较短时间内，很有可能再次被访问到，以及空间局限性:被访问的某个存储单元，在较短时间内，他的相邻存储单元也会被访问到。**而映射方式有很多种，类似于 cache 行号 = 主存块号 mod cache总行数 ，这样每次获取到一个主存地址，根据这个地址计算出在主存中的块号就可以计算出在 cache 中的行号。 下面我们接着聊 CPU 的指令执行。取址，译码，执行，这是一个指令的执行过程，所有指令都会严格按照这个顺序执行，但是多个指令之间其实是可以并行的，对于单核 CPU 来说，同一时刻只能有一条指令能够占有执行单元运行，这里说的执行是 CPU 指令处理(取指，译码，执行)三步骤中的第三步，也就是运算单元的计算任务，所以为了提升 CPU 的指令处理速度，所以需要保证运算单元在执行前的准备工作都完成，这样运算单元就可以一直处于运算中，而刚刚的串行流程中，取指，解码的时候运算单元是空闲的，而且取指和解码如果没有命中高速缓存还需要从主存取，而主存的速度和 CPU 不在一个级别上，所以指令流水线 可以大大提高 CPU 的处理速度，下图是一个3级流水线的示例图，而现在的奔腾 CPU 都是32级流水线，具体做法就是将上面三个流程拆分的更细。 除了指令流水线， CPU 还有分支预测，乱序执行等优化速度的手段。好了，我们回到正题，一行 Java 代码是怎么执行的。 一行代码能够执行，必须要有可以执行的上下文环境，包括，指令寄存器，数据寄存器，栈空间等内存资源，然后这行代码必须作为一个执行流能够被操作系统的任务调度器识别，并给他分配 CPU 资源，当然这行代码所代表的指令必须是 CPU 可以解码识别的，所以一行 Java 代码必须被解释成对应的 CPU 指令才能执行。下面我们看下System.out.println(“Hello world”)这行代码的转译过程。 Java 是一门高级语言，这类语言不能直接运行在硬件上，必须运行在能够识别 Java 语言特性的虚拟机上，而 Java 代码必须通过 Java 编译器将其转换成虚拟机所能识别的指令序列，也称为 Java 字节码，之所以称为字节码是因为 Java 字节码的操作指令(OpCode)被固定为一个字节，以下为 System.out.println(“Hello world”) 编译后的字节码 0x00: b2 00 02 getstatic Java .lang.System.out 0x03: 12 03 ldc &quot;Hello, World!&quot; 0x05: b6 00 04 invokevirtual Java .io.PrintStream.println 0x08: b1 return 最左列是偏移；中间列是给虚拟机读的字节码；最右列是高级语言的代码，下面是通过汇编语言转换成的机器指令，中间是机器码，第三列为对应的机器指令，最后一列是对应的汇编代码 0x00: 55 push rbp 0x01: 48 89 e5 mov rbp,rsp 0x04: 48 83 ec 10 sub rsp,0x10 0x08: 48 8d 3d 3b 00 00 00 lea rdi,[rip+0x3b] ; 加载 &quot;Hello, World!\\n&quot; 0x0f: c7 45 fc 00 00 00 00 mov DWORD PTR [rbp-0x4]，0x0 0x16: b0 00 mov al,0x0 0x18: e8 0d 00 00 00 call 0x12 ; 调用 printf 方法 0x1d: 31 c9 xor ecx,ecx 0x1f: 89 45 f8 mov DWORD PTR [rbp-0x8]，eax 0x22: 89 c8 mov eax,ecx 0x24: 48 83 c4 10 add rsp,0x10 0x28: 5d pop rbp 0x29: c3 ret JVM 通过类加载器加载 class 文件里的字节码后，会通过解释器解释成汇编指令，最终再转译成 CPU 可以识别的机器指令，解释器是软件来实现的，主要是为了实现同一份 Java 字节码可以在不同的硬件平台上运行，而将汇编指令转换成机器指令由硬件直接实现，这一步速度是很快的，当然 JVM 为了提高运行效率也可以将某些热点代码(一个方法内的代码)一次全部编译成机器指令后然后在执行，也就是和解释执行对应的即时编译(JIT)， JVM 启动的时候可以通过 -Xint 和 -Xcomp 来控制执行模式。 从软件层面上， class 文件被加载进虚拟机后，类信息会存放在方法区，在实际运行的时候会执行方法区中的代码，在 JVM 中所有的线程共享堆内存和方法区，而每个线程有自己独立的 Java 方法栈，本地方法栈(面向 native 方法)，PC寄存器(存放线程执行位置)，当调用一个方法的时候， Java 虚拟机会在当前线程对应的方法栈中压入一个栈帧，用来存放 Java 字节码操作数以及局部变量，这个方法执行完会弹出栈帧，一个线程会连续执行多个方法，对应不同的栈帧的压入和弹出，压入栈帧后就是 JVM 解释执行的过程了。 中断刚刚说到， CPU 只要一上电就像一个永动机， 不停的取指令，运算，周而复始，而中断便是操作系统的灵魂，故名思议，中断就是打断 CPU 的执行过程，转而去做点别的，例如系统执行期间发生了致命错误，需要结束执行，例如用户程序调用了一个系统调用的方法，例如mmp等，就会通过中断让 CPU 切换上下文，转到内核空间，例如一个等待用户输入的程序正在阻塞，而当用户通过键盘完成输入，内核数据已经准备好后，就会发一个中断信号，唤醒用户程序把数据从内核取走，不然内核可能会数据溢出，当磁盘报了一个致命异常，也会通过中断通知 CPU ，定时器完成时钟滴答也会发时钟中断通知 CPU . 中断的种类，我们这里就不做细分了，中断有点类似于我们经常说的事件驱动编程，而这个事件通知机制是怎么实现的呢，硬件中断的实现通过一个导线和 CPU 相连来传输中断信号，软件上会有特定的指令，例如执行系统调用创建线程的指令，而 CPU 每执行完一个指令，就会检查中断寄存器中是否有中断，如果有就取出然后执行该中断对应的处理程序。 陷入内核 : 我们在设计软件的时候，会考虑程序上下文切换的频率，频率太高肯定会影响程序执行性能，而陷入内核是针对 CPU 而言的， CPU 的执行从用户态转向内核态，以前是用户程序在使用 CPU ，现在是内核程序在使用 CPU ，这种切换是通过系统调用产生的，系统调用是执行操作系统底层的程序，Linux的设计者，为了保护操作系统，将进程的执行状态用内核态和用户态分开，同一个进程中，内核和用户共享同一个地址空间，一般 4G 的虚拟地址，其中 1G 给内核态， 3G 给用户态。在程序设计的时候我们要尽量减少用户态到内核态的切换，例如创建线程是一个系统调用，所以我们有了线程池的实现。 从 Linux 内存管理角度理解 JVM 内存模型进程上下文我们可以将程序理解为一段可执行的指令集合，而这个程序启动后，操作系统就会为他分配 CPU ，内存等资源，而这个正在运行的程序就是我们说的进程，进程是操作系统对处理器中运行的程序的一种抽象，而为进程分配的内存以及 CPU 资源就是这个进程的上下文，保存了当前执行的指令，以及变量值，而 JVM 启动后也是linux上的一个普通进程，进程的物理实体和支持进程运行的环境合称为上下文，而上下文切换就是将当前正在运行的进程换下，换一个新的进程到处理器运行，以此来让多个进程并发的执行，上下文切换可能来自操作系统调度，也有可能来自程序内部，例如读取IO的时候，会让用户代码和操作系统代码之间进行切换。 虚拟存储当我们同时启动多个 JVM 执行: System.out.println(new Object()); 将会打印这个对象的 hashcode ，hashcode 默认为内存地址，最后发现他们打印的都是 Java .lang.Object@4fca772d ，也就是多个进程返回的内存地址竟然是一样的。 通过上面的例子我们可以证明，linux中每个进程有单独的地址空间，在此之前，我们先了解下 CPU 是如何访问内存的? 假设我们现在还没有虚拟地址，只有物理地址，编译器在编译程序的时候，需要将高级语言转换成机器指令，那么 CPU 访问内存的时候必须指定一个地址，这个地址如果是一个绝对的物理地址，那么程序就必须放在内存中的一个固定的地方，而且这个地址需要在编译的时候就要确认，大家应该想到这样有多坑了吧， 如果我要同时运行两个 office word 程序，那么他们将操作同一块内存，那就乱套了，伟大的计算机前辈设计出，让 CPU采用 段基址 + 段内偏移地址 的方式访问内存，其中段基地址在程序启动的时候确认，尽管这个段基地址还是绝对的物理地址，但终究可以同时运行多个程序了， CPU 采用这种方式访问内存，就需要段基址寄存器和段内偏移地址寄存器来存储地址，最终将两个地址相加送上地址总线。而内存分段，相当于每个进程都会分配一个内存段，而且这个内存段需要是一块连续的空间，主存里维护着多个内存段，当某个进程需要更多内存，并且超出物理内存的时候，就需要将某个不常用的内存段换到硬盘上，等有充足内存的时候在从硬盘加载进来，也就是 swap .每次交换都需要操作整个段的数据。 首先连续的地址空间是很宝贵的，例如一个 50M 的内存，在内存段之间有空隙的情况下，将无法支持 5 个需要 10M 内存才能运行的程序，如何才能让段内地址不连续呢? 答案是内存分页。 在保护模式下，每一个进程都有自己独立的地址空间，所以段基地址是固定的，只需要给出段内偏移地址就可以了，而这个偏移地址称为线性地址，线性地址是连续的，而内存分页将连续的线性地址和分页后的物理地址相关联，这样逻辑上的连续线性地址可以对应不连续的物理地址。物理地址空间可以被多个进程共享，而这个映射关系将通过页表( page table)进行维护。 标准页的尺寸一般为 4KB ，分页后，物理内存被分成若干个 4KB 的数据页，进程申请内存的时候，可以映射为多个 4KB 大小的物理内存，而应用程序读取数据的时候会以页为最小单位，当需要和硬盘发生交换的时候也是以页为单位。 现代计算机多采用虚拟存储技术，虚拟存储让每个进程以为自己独占整个内存空间，其实这个虚拟空间是主存和磁盘的抽象，这样的好处是，每个进程拥有一致的虚拟地址空间，简化了内存管理，进程不需要和其他进程竞争内存空间，因为他是独占的，也保护了各自进程不被其他进程破坏，另外，他把主存看成磁盘的一个缓存，主存中仅保存活动的程序段和数据段，当主存中不存在数据的时候发生缺页中断，然后从磁盘加载进来，当物理内存不足的时候会发生 swap 到磁盘。页表保存了虚拟地址和物理地址的映射，页表是一个数组，每个元素为一个页的映射关系，这个映射关系可能是和主存地址，也可能和磁盘，页表存储在主存，我们将存储在高速缓冲区 cache 中的页表称为快表 TLAB 。 装入位 表示对于页是否在主存，如果地址页每页表示，数据还在磁盘 存放位置 建立虚拟页和物理页的映射，用于地址转换，如果为null表示是一个未分配页 修改位 用来存储数据是否修改过 权限位 用来控制是否有读写权限 禁止缓存位 主要用来保证 cache 主存 磁盘的数据一致性 内存映射正常情况下，我们读取文件的流程为，先通过系统调用从磁盘读取数据，存入操作系统的内核缓冲区，然后在从内核缓冲区拷贝到用户空间，而内存映射，是将磁盘文件直接映射到用户的虚拟存储空间中，通过页表维护虚拟地址到磁盘的映射，通过内存映射的方式读取文件的好处有，因为减少了从内核缓冲区到用户空间的拷贝，直接从磁盘读取数据到内存，减少了系统调用的开销，对用户而言，仿佛直接操作的磁盘上的文件，另外由于使用了虚拟存储，所以不需要连续的主存空间来存储数据。 在 Java 中，我们使用 MappedByteBuffer 来实现内存映射，这是一个堆外内存，在映射完之后，并没有立即占有物理内存，而是访问数据页的时候，先查页表，发现还没加载，发起缺页异常，然后在从磁盘将数据加载进内存，所以一些对实时性要求很高的中间件，例如rocketmq,消息存储在一个大小为1G的文件中，为了加快读写速度，会将这个文件映射到内存后，在每个页写一比特数据，这样就可以把整个1G文件都加载进内存，在实际读写的时候就不会发生缺页了，这个在rocketmq内部叫做文件预热。 下面我们贴一段 rocketmq 消息存储模块的代码，位于 MappedFile 类中，这个类是 rocketMq 消息存储的核心类感兴趣的可以自行研究，下面两个方法一个是创建文件映射，一个是预热文件，每预热 1000 个数据页，就让出 CPU 权限。 private void init(final String fileName, final int fileSize) throws IOException &#123; this.fileName = fileName; this.fileSize = fileSize; this.file = new File(fileName); this.fileFromOffset = Long.parseLong(this.file.getName()); boolean ok = false; ensureDirOK(this.file.getParent()); try &#123; this.fileChannel = new RandomAccessFile(this.file, &quot;rw&quot;).getChannel(); this.mappedByteBuffer = this.fileChannel.map(MapMode.READ_WRITE, 0, fileSize); TOTAL_MAPPED_VIRTUAL_MEMORY.addAndGet(fileSize); TOTAL_MAPPED_FILES.incrementAndGet(); ok = true; &#125; catch (FileNotFoundException e) &#123; log.error(&quot;create file channel &quot; + this.fileName + &quot; Failed. &quot;, e); throw e; &#125; catch (IOException e) &#123; log.error(&quot;map file &quot; + this.fileName + &quot; Failed. &quot;, e); throw e; &#125; finally &#123; if (!ok &amp;&amp; this.fileChannel != null) &#123; this.fileChannel.close(); &#125; &#125; &#125; //文件预热，OS_PAGE_SIZE = 4kb 相当于每 4kb 就写一个 byte 0 ，将所有的页都加载到内存，真正使用的时候就不会发生缺页异常了 public void warmMappedFile(FlushDiskType type, int pages) &#123; long beginTime = System.currentTimeMillis(); ByteBuffer byteBuffer = this.mappedByteBuffer.slice(); int flush = 0; long time = System.currentTimeMillis(); for (int i = 0, j = 0; i &lt; this.fileSize; i += MappedFile.OS_PAGE_SIZE, j++) &#123; byteBuffer.put(i, (byte) 0); // force flush when flush disk type is sync if (type == FlushDiskType.SYNC_FLUSH) &#123; if ((i / OS_PAGE_SIZE) - (flush / OS_PAGE_SIZE) &gt;= pages) &#123; flush = i; mappedByteBuffer.force(); &#125; &#125; // prevent gc if (j % 1000 == 0) &#123; log.info(&quot;j=&#123;&#125;， costTime=&#123;&#125;&quot;, j, System.currentTimeMillis() - time); time = System.currentTimeMillis(); try &#123; // 这里sleep(0)，让线程让出 CPU 权限，供其他更高优先级的线程执行，此线程从运行中转换为就绪 Thread.sleep(0); &#125; catch (InterruptedException e) &#123; log.error(&quot;Interrupted&quot;, e); &#125; &#125; &#125; // force flush when prepare load finished if (type == FlushDiskType.SYNC_FLUSH) &#123; log.info(&quot;mapped file warm-up done, force to disk, mappedFile=&#123;&#125;， costTime=&#123;&#125;&quot;, this.getFileName()， System.currentTimeMillis() - beginTime); mappedByteBuffer.force(); &#125; log.info(&quot;mapped file warm-up done. mappedFile=&#123;&#125;， costTime=&#123;&#125;&quot;, this.getFileName()， System.currentTimeMillis() - beginTime); this.mlock(); &#125; JVM 中对象的内存布局在linux中只要知道一个变量的起始地址就可以读出这个变量的值，因为从这个起始地址起前8位记录了变量的大小，也就是可以定位到结束地址，在 Java 中我们可以通过 Field.get(object) 的方式获取变量的值，也就是反射，最终是通过 UnSafe 类来实现的。我们可以分析下具体代码 Field 对象的 getInt方法 先安全检查 ，然后调用 FieldAccessor @CallerSensitive public int getInt(Object obj) throws IllegalArgumentException, IllegalAccessException &#123; if (!override) &#123; if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); checkAccess(caller, clazz, obj, modifiers); &#125; &#125; return getFieldAccessor(obj).getInt(obj); &#125; 获取field在所在对象中的地址的偏移量 fieldoffset UnsafeFieldAccessorImpl(Field var1) &#123; this.field = var1; if(Modifier.isStatic(var1.getModifiers())) &#123; this.fieldOffset = unsafe.staticFieldOffset(var1); &#125; else &#123; this.fieldOffset = unsafe.objectFieldOffset(var1); &#125; this.isFinal = Modifier.isFinal(var1.getModifiers()); &#125; UnsafeStaticIntegerFieldAccessorImpl 调用unsafe中的方法 public int getInt(Object var1) throws IllegalArgumentException &#123; return unsafe.getInt(this.base, this.fieldOffset); &#125; 通过上面的代码我们可以通过属性相对对象起始地址的偏移量，来读取和写入属性的值，这也是 Java 反射的原理，这种模式在jdk中很多场景都有用到，例如LockSupport.park中设置阻塞对象。 那么属性的偏移量具体根据什么规则来确定的呢? 下面我们借此机会分析下 Java 对象的内存布局 在 Java 虚拟机中，每个 Java 对象都有一个对象头 (object header) ，由标记字段和类型指针构成，标记字段用来存储对象的哈希码， GC 信息， 持有的锁信息，而类型指针指向该对象的类 Class ，在 64 位操作系统中，标记字段占有 64 位，而类型指针也占 64 位，也就是说一个 Java 对象在什么属性都没有的情况下要占有 16 字节的空间，当前 JVM 中默认开启了压缩指针，这样类型指针可以只占 32 位，所以对象头占 12 字节， 压缩指针可以作用于对象头，以及引用类型的字段。 JVM 为了内存对齐，会对字段进行重排序，这里的对齐主要指 Java 虚拟机堆中的对象的起始地址为 8 的倍数，如果一个对象用不到 8N 个字节，那么剩下的就会被填充，另外子类继承的属性的偏移量和父类一致，以 Long 为例，他只有一个非 static 属性 value ，而尽管对象头只占有 12 字节，而属性 value 的偏移量只能是 16, 其中 4 字节只能浪费掉，所以字段重排就是为了避免内存浪费， 所以我们很难在 Java 字节码被加载之前分析出这个 Java 对象占有的实际空间有多大，我们只能通过递归父类的所有属性来预估对象大小，而真实占用的大小可以通过 Java agent 中的 Instrumentation获取。当然内存对齐另外一个原因是为了让字段只出现在同一个 CPU 的缓存行中，如果字段不对齐，就有可能出现一个字段的一部分在缓存行 1 中，而剩下的一半在 缓存行 2 中，这样该字段的读取需要替换两个缓存行，而字段的写入会导致两个缓存行上缓存的其他数据都无效，这样会影响程序性能。 通过内存对齐可以避免一个字段同时存在两个缓存行里的情况，但还是无法完全规避缓存伪共享的问题，也就是一个缓存行中存了多个变量，而这几个变量在多核 CPU 并行的时候，会导致竞争缓存行的写权限，当其中一个 CPU 写入数据后，这个字段对应的缓存行将失效，导致这个缓存行的其他字段也失效。 在 Disruptor 中，通过填充几个无意义的字段，让对象的大小刚好在 64 字节，一个缓存行的大小为64字节，这样这个缓存行就只会给这一个变量使用，从而避免缓存行伪共享，但是在 jdk7 中，由于无效字段被清除导致该方法失效，只能通过继承父类字段来避免填充字段被优化，而 jdk8 提供了注解@Contended 来标示这个变量或对象将独享一个缓存行，使用这个注解必须在 JVM 启动的时候加上 -XX:-RestrictContended 参数，其实也是用空间换取时间。 jdk6 --- 32 位系统下 public final static class VolatileLong &#123; public volatile long value = 0L; public long p1, p2, p3, p4, p5, p6; // 填充字段 &#125; jdk7 通过继承 public class VolatileLongPadding &#123; public volatile long p1, p2, p3, p4, p5, p6; // 填充字段 &#125; public class VolatileLong extends VolatileLongPadding &#123; public volatile long value = 0L; &#125; jdk8 通过注解 @Contended public class VolatileLong &#123; public volatile long value = 0L; &#125; NPTL和 Java 的线程模型 按照教科书的定义，进程是资源管理的最小单位，而线程是 CPU 调度执行的最小单位，线程的出现是为了减少进程的上下文切换(线程的上下文切换比进程小很多)，以及更好适配多核心 CPU 环境，例如一个进程下多个线程可以分别在不同的 CPU 上执行，而多线程的支持，既可以放在Linux内核实现，也可以在核外实现，如果放在核外，只需要完成运行栈的切换，调度开销小，但是这种方式无法适应多 CPU 环境，底层的进程还是运行在一个 CPU 上，另外由于对用户编程要求高，所以目前主流的操作系统都是在内核支持线程，而在Linux中，线程是一个轻量级进程，只是优化了线程调度的开销。而在 JVM 中的线程和内核线程是一一对应的，线程的调度完全交给了内核，当调用Thread.run 的时候，就会通过系统调用 fork() 创建一个内核线程，这个方法会在用户态和内核态之间进行切换，性能没有在用户态实现线程高，当然由于直接使用内核线程，所以能够创建的最大线程数也受内核控制。目前 Linux上 的线程模型为 NPTL ( Native POSIX Thread Library)，他使用一对一模式，兼容 POSIX 标准，没有使用管理线程，可以更好地在多核 CPU 上运行。 线程的状态对进程而言，就三种状态，就绪，运行，阻塞，而在 JVM 中，阻塞有四种类型，我们可以通过 jstack 生成 dump 文件查看线程的状态。 BLOCKED (on object monitor) 通过 synchronized(obj) 同步块获取锁的时候，等待其他线程释放对象锁，dump 文件会显示 waiting to lock &lt;0x00000000e1c9f108&gt; TIMED WAITING (on object monitor) 和 WAITING (on object monitor) 在获取锁后，调用了 object.wait() 等待其他线程调用 object.notify()，两者区别是是否带超时时间 TIMED WAITING (sleeping) 程序调用了 thread.sleep()，这里如果 sleep(0) 不会进入阻塞状态，会直接从运行转换为就绪 TIMED WAITING (parking) 和 WAITING (parking) 程序调用了 Unsafe.park()，线程被挂起，等待某个条件发生，waiting on condition 而在 POSIX 标准中，thread_block 接受一个参数 stat ，这个参数也有三种类型，TASK_BLOCKED, TASK_WAITING, TASK_HANGING,而调度器只会对线程状态为 READY 的线程执行调度，另外一点是线程的阻塞是线程自己操作的，相当于是线程主动让出 CPU 时间片，所以等线程被唤醒后，他的剩余时间片不会变，该线程只能在剩下的时间片运行，如果该时间片到期后线程还没结束，该线程状态会由 RUNNING 转换为 READY ，等待调度器的下一次调度。 好了，关于线程就分析到这，关于 Java 并发包，核心都在 AQS 里，底层是通过 UnSafe类的 cas 方法，以及 park 方法实现，后面我们在找时间单独分析，现在我们在看看 Linux 的进程同步方案。 POSIX表示可移植操作系统接口（Portable Operating System Interface of UNIX,缩写为 POSIX ），POSIX标准定义了操作系统应该为应用程序提供的接口标准。 CAS 操作需要 CPU 支持，将比较 和 交换 作为一条指令来执行， CAS 一般有三个参数，内存位置，预期原值，新值 ，所以UnSafe 类中的 compareAndSwap 用属性相对对象初始地址的偏移量，来定位内存位置。 线程的同步线程同步出现的根本原因是访问公共资源需要多个操作，而这多个操作的执行过程不具备原子性，被任务调度器分开了，而其他线程会破坏共享资源，所以需要在临界区做线程的同步，这里我们先明确一个概念，就是临界区，他是指多个任务访问共享资源如内存或文件时候的指令，他是指令并不是受访问的资源。 POSIX 定义了五种同步对象，互斥锁，条件变量，自旋锁，读写锁，信号量，这些对象在 JVM 中也都有对应的实现，并没有全部使用 POSIX 定义的 api,通过 Java 实现灵活性更高，也避免了调用native方法的性能开销，当然底层最终都依赖于 pthread 的 互斥锁 mutex 来实现，这是一个系统调用，开销很大，所以 JVM 对锁做了自动升降级，基于AQS的实现以后在分析，这里主要说一下关键字 synchronized . 当声明 synchronized 的代码块时，编译而成的字节码会包含一个 monitorenter 和 多个 monitorexit (多个退出路径，正常和异常情况)，当执行 monitorenter 的时候会检查目标锁对象的计数器是否为0,如果为0则将锁对象的持有线程设置为自己，然后计数器加1,获取到锁，如果不为0则检查锁对象的持有线程是不是自己，如果是自己就将计数器加1获取锁，如果不是则阻塞等待，退出的时候计数器减1,当减为0的时候清楚锁对象的持有线程标记，可以看出 synchronized 是支持可重入的。 刚刚说到线程的阻塞是一个系统调用，开销大，所以 JVM 设计了自适应自旋锁，就是当没有获取到锁的时候， CPU 回进入自旋状态等待其他线程释放锁，自旋的时间主要看上次等待多长时间获取的锁，例如上次自旋5毫秒没有获取锁，这次就6毫秒，自旋会导致 CPU 空跑，另一个副总用就是不公平的锁机制，因为该线程自旋获取到锁，而其他正在阻塞的线程还在等待。除了自旋锁， JVM 还通过 CAS 实现了轻量级锁和偏向锁来分别针对多个线程在不同时间访问锁和锁仅会被一个线程使用的情况。后两种锁相当于并没有调用底层的信号量实现(通过信号量来控制线程A释放了锁例如调用了 wait()，而线程B就可以获取锁，这个只有内核才能实现，后面两种由于场景里没有竞争所以也就不需要通过底层信号量控制)，只是自己在用户空间维护了锁的持有关系，所以更高效。 如上图所示，如果线程进入 monitorenter 会将自己放入该 objectmonitor 的 entryset 队列，然后阻塞，如果当前持有线程调用了 wait 方法，将会释放锁，然后将自己封装成 objectwaiter 放入 objectmonitor 的 waitset 队列，这时候 entryset 队列里的某个线程将会竞争到锁，并进入 active 状态，如果这个线程调用了 notify 方法，将会把 waitset 的第一个 objectwaiter 拿出来放入 entryset (这个时候根据策略可能会先自旋)，当调用 notify 的那个线程执行 moniterexit 释放锁的时候， entryset 里的线程就开始竞争锁后进入 active 状态。 为了让应用程序免于数据竞争的干扰， Java 内存模型中定义了 happen-before 来描述两个操作的内存可见性，也就是 X 操作 happen-before 操作 Y ， 那么 X 操作结果 对 Y 可见。 JVM 中针对 volatile 以及 锁 的实现有 happen-before 规则， JVM 底层通过插入内存屏障来限制编译器的重排序，以 volatile 为例，内存屏障将不允许 在 volatile 字段写操作之前的语句被重排序到写操作后面 ， 也不允许读取 volatile 字段之后的语句被重排序带读取语句之前。插入内存屏障的指令，会根据指令类型不同有不同的效果，例如在 monitorexit 释放锁后会强制刷新缓存，而 volatile 对应的内存屏障会在每次写入后强制刷新到主存，并且由于 volatile 字段的特性，编译器无法将其分配到寄存器，所以每次都是从主存读取，所以 volatile 适用于读多写少得场景，最好只有个线程写多个线程读，如果频繁写入导致不停刷新缓存会影响性能。 关于应用程序中设置多少线程数合适的问题，我们一般的做法是设置 CPU 最大核心数 * 2 ，我们编码的时候可能不确定运行在什么样的硬件环境中，可以通过 Runtime.getRuntime().availableProcessors() 获取 CPU 核心，但是具体设置多少线程数，主要和线程内运行的任务中的阻塞时间有关系，如果任务中全部是计算密集型，那么只需要设置 CPU 核心数的线程就可以达到 CPU 利用率最高，如果设置的太大，反而因为线程上下文切换影响性能，如果任务中有阻塞操作，而在阻塞的时间就可以让 CPU 去执行其他线程里的任务，我们可以通过 线程数量=内核数量 / （1 - 阻塞率）这个公式去计算最合适的线程数，阻塞率我们可以通过计算任务总的执行时间和阻塞的时间获得，目前微服务架构下有大量的RPC调用，所以利用多线程可以大大提高执行效率，我们可以借助分布式链路监控来统计RPC调用所消耗的时间，而这部分时间就是任务中阻塞的时间，当然为了做到极致的效率最大，我们需要设置不同的值然后进行测试。 Java 中如何实现定时任务定时器已经是现代软件中不可缺少的一部分，例如每隔5秒去查询一下状态，是否有新邮件，实现一个闹钟等， Java 中已经有现成的 api 供使用，但是如果你想设计更高效，更精准的定时器任务，就需要了解底层的硬件知识，比如实现一个分布式任务调度中间件，你可能要考虑到各个应用间时钟同步的问题。 Java 中我们要实现定时任务，有两种方式，一种通过 timer 类， 另外一种是 JUC 中的 ScheduledExecutorService ，不知道大家有没有好奇 JVM 是如何实现定时任务的，难道一直轮询时间，看是否时间到了，如果到了就调用对应的处理任务，但是这种一直轮询不释放 CPU 肯定是不可取的，要么就是线程阻塞，等到时间到了在来唤醒线程，那么 JVM 怎么知道时间到了，如何唤醒呢? 首先我们翻一下 JDK ，发现和时间相关的 API 大概有3处，而且这 3 处还都对时间的精度做了区分: object.wait(long millisecond) 参数是毫秒，必须大于等于 0 ，如果等于 0 ，就一直阻塞直到其他线程来唤醒 ，timer 类就是通过 wait() 方法来实现，下面我们看一下wait的另外一个方法 public final void wait(long timeout, int nanos) throws InterruptedException &#123; if (timeout &lt; 0) &#123; throw new IllegalArgumentException(&quot;timeout value is negative&quot;); &#125; if (nanos &lt; 0 || nanos &gt; 999999) &#123; throw new IllegalArgumentException( &quot;nanosecond timeout value out of range&quot;); &#125; if (nanos &gt; 0) &#123; timeout++; &#125; wait(timeout); &#125; 这个方法是想提供一个可以支持纳秒级的超时时间，然而只是粗暴的加 1 毫秒。 Thread.sleep(long millisecond) 目前一般通过这种方式释放 CPU ，如果参数为 0 ，表示释放 CPU 给更高优先级的线程，自己从运行状态转换为可运行态等待 CPU 调度，他也提供了一个可以支持纳秒级的方法实现，跟 wait 额区别是它通过 500000 来分隔是否要加 1 毫秒。 public static void sleep(long millis, int nanos) throws InterruptedException &#123; if (millis &lt; 0) &#123; throw new IllegalArgumentException(&quot;timeout value is negative&quot;); &#125; if (nanos &lt; 0 || nanos &gt; 999999) &#123; throw new IllegalArgumentException( &quot;nanosecond timeout value out of range&quot;); &#125; if (nanos &gt;= 500000 || (nanos != 0 &amp;&amp; millis == 0)) &#123; millis++; &#125; sleep(millis); &#125; LockSupport.park(long nans) Condition.await()调用的该方法， ScheduledExecutorService 用的 condition.await() 来实现阻塞一定的超时时间，其他带超时参数的方法也都通过他来实现，目前大多定时器都是通过这个方法来实现的，该方法也提供了一个布尔值来确定时间的精度 System.currentTimeMillis() 以及 System.nanoTime() 这两种方式都依赖于底层操作系统，前者是毫秒级，经测试 windows 平台的频率可能超过 10ms ，而后者是纳秒级别，频率在 100ns 左右，所以如果要获取更精准的时间建议用后者 好了，api 了解完了，我们来看下定时器的底层是怎么实现的，现代PC机中有三种硬件时钟的实现，他们都是通过晶体振动产生的方波信号输入来完成时钟信号同步的。 实时时钟 RTC ，用于长时间存放系统时间的设备，即使关机也可以依靠主板中的电池继续计时。 Linux 启动的时候会从 RTC 中读取时间和日期作为初始值，之后在运行期间通过其他计时器去维护系统时间 可编程间隔定时器 PIT ，该计数器会有一个初始值，每过一个时钟周期，该初始值会减1,当该初始值被减到0时，就通过导线向 CPU 发送一个时钟中断， CPU 就可以执行对应的中断程序，也就是回调对应的任务 时间戳计数器 TSC ， 所有的 Intel8086 CPU 中都包含一个时间戳计数器对应的寄存器，该寄存器的值会在每次 CPU 收到一个时钟周期的中断信号后就会加 1 .他比 PIT 精度高，但是不能编程，只能读取。 时钟周期:硬件计时器在多长时间内产生时钟脉冲，而时钟周期频率为1秒内产生时钟脉冲的个数。目前通常为1193180. 时钟滴答:当PIT中的初始值减到0的时候，就会产生一次时钟中断，这个初始值由编程的时候指定。 Linux启动的时候，先通过 RTC 获取初始时间，之后内核通过 PIT 中的定时器的时钟滴答来维护日期，并且会定时将该日期写入 RTC,而应用程序的定时器主要是通过设置 PIT 的初始值设置的，当初始值减到0的时候，就表示要执行回调函数了，这里大家会不会有疑问，这样同一时刻只能有一个定时器程序了，而我们在应用程序中，以及多个应用程序之间，肯定有好多定时器任务，其实我们可以参考 ScheduledExecutorService 的实现，只需要将这些定时任务按照时间做一个排序，越靠前待执行的任务放在前面，第一个任务到了在设置第二个任务相对当前时间的值，毕竟 CPU 同一时刻也只能运行一个任务，关于时间的精度问题，我们无法在软件层面做的完全精准，毕竟 CPU 的调度不完全受用户程序控制，当然更大的依赖是硬件的时钟周期频率，目前 TSC 可以提高更高的精度。 现在我们知道了， Java 中的超时时间，是通过可编程间隔定时器设置一个初始值然后等待中断信号实现的，精度上受硬件时钟周期的影响，一般为毫秒级别，毕竟1纳秒光速也只有3米，所以 JDK 中带纳秒参数的实现都是粗暴做法，预留着等待精度更高的定时器出现，而获取当前时间 System.currentTimeMillis() 效率会更高，但他是毫秒级精度，他读取的 Linux 内核维护的日期，而 System.nanoTime() 会优先使用 TSC ，性能稍微低一点，但他是纳秒级，Random 类为了防止冲突就用nanoTime生成种子。 Java 如何和外部设备通信计算机的外部设备有鼠标、键盘、打印机、网卡等，通常我们将外部设备和和主存之间的信息传递称为 I/O 操作 ， 按操作特性可以分为，输出型设备，输入型设备，存储设备。现代设备都采用通道方式和主存进行交互，通道是一个专门用来处理IO任务的设备， CPU 在处理主程序时遇到I/O请求，启动指定通道上选址的设备，一旦启动成功，通道开始控制设备进行操作，而 CPU 可以继续执行其他任务，I/O 操作完成后，通道发出 I/O 操作结束的中断，处理器转而处理 IO 结束后的事件。其他处理 IO 的方式，例如轮询、中断、DMA,在性能上都不见通道，这里就不介绍了。当然 Java 程序和外部设备通信也是通过系统调用完成，这里也不在继续深入了。 作者简介 小强，铜板街资金端后台开发工程师，2015年6月加入铜板街。目前负责铜板街资金端清结算相关的开发。 原文：https://juejin.im/post/5c983bd46fb9a0710a1bd3e1","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"jvm","slug":"jvm","permalink":"https://blog.fenxiangz.com/tags/jvm/"},{"name":"操作系统","slug":"操作系统","permalink":"https://blog.fenxiangz.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"谈一谈 Java IO 模型","slug":"java/nio/2019-02-27_java_nio_model","date":"2019-02-27T00:00:00.000Z","updated":"2020-12-20T16:47:02.969Z","comments":true,"path":"post/java/nio/2019-02-27_java_nio_model.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/nio/2019-02-27_java_nio_model.html","excerpt":"","text":"Java IO 模型对于 Java 开发工程师来说，是日常工作中经常接触的内容，特别是随着分布式系统的兴起，IO 也显得越来越重要，Java 的 IO 模型本质上还是利用操作系统提供的接口来实现，不熟悉这一部分内容的话，可以先看一下上篇文章Unix 网络 IO 模型及 Linux 的 IO 多路复用模型，本文跟上篇的内容是紧密相连的，特别是本文的重点 —— Java NIO 部分，其底层原理就是 UNIX 的 IO 多路复用，IO 多路复用在上篇文章中讲述了很多。 这篇文章大概内容如下： Java IO 模型的简单介绍； BIO 、NIO、AIO 模型的介绍，会详细介绍 NIO； 几种 IO 模型的对比。 Java IO 模型介绍 在 JDK 推出 Java NIO 之前，基于 Java 的所有 Socket 通信都采用了同步阻塞模式（BIO），这种一对一的通信模型虽然简化了开发的难度，但在性能和可靠性方面却存在这巨大的瓶颈，特别是无法处理高并发的场景，使得 Java 在服务器端应用十分有限。 正是由于 Java 传统 BIO 的拙劣表现，使得 Java 不得不去开发新版的 IO 模型，最终，JDK1.4 提供了新的 NIO 类库，Java 可以支持非阻塞 IO；之后，JDK1.7 正式发布，不但对 NIO 进行了升级，还提供了 AIO 功能。本文就是在对 Java 这些 IO 模型学习后，总结的一篇笔记。 网络编程 网络编程的基本模型是 Client/Server 模型，也就是两个进程之间进行相互通信，其中服务端提供位置信息（绑定的 IP 地址和端口），客户端通过连接操作向服务端监听的地址发起连接请求，通过三次握手建立连接，如果连接成功，双方就可以通过网络套接字（socket）进行通信（可以参考TCP的三次握手和四次挥手），下面先看一下两种对 IO 模型常见的分类方式。 同步与异步 描述的是用户线程与内核的交互方式，与消息的通知机制有关： 同步：当一个同步调用发出后，需要等待返回消息（用户线程不断去询问），才能继续进行； 异步：当一个异步调用发出后，调用者不能立即得到返回消息，完成后会通过状态、通知和回调来通知调用者。 简单来说就是： 同步：同步等待消息通知，消息返回才能继续进行； 异步：异步等待消息通知，完成后被调系统通过回调等来通知调用者。 阻塞与非阻塞 阻塞和非阻塞指的是不能立刻得到结果之前，会不会阻塞当前线程。 阻塞：当前线程会被挂起，直到结果返回； 非阻塞：指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回（会导致线程切换的增加）。 举个栗子说明： 类别 示例 同步阻塞 在银行排队，不干别的事情 效率最低 同步非阻塞 排队时，边打电话边抬头看是否到自己了 效率低下 异步阻塞 在银行领一个号后，在银行里等，不能做别的事情 异步非阻塞 领完号后，在忙着自己的事情，直到柜台通知 效率较高 BIO BIO 模型是 Java IO 最开始提供的一种 IO 模型，BIO 又可以细分为两种模型，一是传统的同步阻塞模型，二是在对传统 BIO 模型的基本上进行的优化，又称为伪异步 IO 模型。 传统的 BIO 模型 传统 BIO 中，ServerSocket 负责绑定 IP 地址，启动监听端口；Socket 负责发起连接操作，连接成功后，双方通过输入和输出流进行同步阻塞通信。采用 BIO 通信模型的 Server，通常由一个独立的 Acceptor 线程负责监听 Client 端的连接，它接受到 Client 端连接请求后为每个 Client 创建一个新的线程进行处理，处理完之后，通过输出流返回给 Client 端，线程销毁，过程如下图所示（图来自《Netty 权威指南》）。 这个模型最大的问题是： 缺乏扩展性，不能处理高性能、高并发场景，线程是 JVM 中非常宝贵的资源，当线程数膨胀后，系统的性能就会急剧下降，随着并发访问量的继续增大，系统就会出现线程堆栈溢出、创建新线程失败等问题，导致 Server 不能对外提供服务。 示例代码参考 Java BIO 示例。 伪异步 IO 模型 为了改进这种一对一的连接模型，后来又演进出了一种通过线程池或者消息队列实现 1 个或者多个线程处理所有 Client 请求的模型，由于它底层依然是同步阻塞 IO，所以被称为【伪异步 IO 模型】。相比于传统 BIO 后端不断创建新的线程处理 Client 请求，它在后端使用一个线程池来代替，通过线程池可以灵活的调配线程资源，设置线程的最大值，防止由于海量并发接入导致线程资源耗尽，过程如下图所示（图来自《Netty 权威指南》）。 看似这个模型解决了 BIO 面对的问题，实际上，由于它是面向数据流的模型，底层依然是同步阻塞模型，在处理一个 socket 输入流，它会一直阻塞下去，除非：有数据可读、可用数据读取完毕、有异常，否则会一直一直阻塞下去。这个模型最大的问题是： 阻塞的时间取决于对应 IO 线程的处理速度和网络 IO 的传输速度，处理效率不可控。 Java NIO Java NIO 是 Java IO 模型中最重要的 IO 模型，也是本文主要讲述的内容，正式由于 NIO 的出现，Java 才能在服务端获得跟 C 和 C++ 一样的运行效率，NIO 是 New IO（或者 Non-block IO）的简称。 与 Socket 类和 ServerSocket 类相对应，NIO 也提供了 SocketChannel 和 ServerSocketChannel 两种不同套接字通道的实现，它们都支持阻塞和非阻塞两种模式。一般来说，低负载、低并发的应用程序可以选择同步阻塞 IO 以降低复杂度，但是高负载、高并发的网络应用，需要使用 NIO 的非阻塞模式进行开发。 基本概念 在 NIO 中有三种非常重要的概念： 缓冲区（buffer）：本质上是一个数组，它包含一些要读写的数据； 通道（channel）：是一个通道，通过它读写数据，类似于自来水管； 多路复用器（selector）：用于选择已经就绪的任务，selector 会轮询注册在其上的 channel，选出已经就绪的 channel。 三者之间的关系如上图所示，这里先简单概括一下： Buffer：是缓冲区，任何时候访问 NIO 数据，都是通过 Buffer 进行； Channel：通过它读写 Buffer 中的数据，可以用于读、写或同时读写； Selector：多路复用器，Selector 不断轮询注册在其上的 Channel，如果某个 Channel 有新的 TCP 链接接入、读和写事件，这个 Channel 就处于就绪状态，会被 Selector 轮询出来，然后通过SelectionKey() 可以获取就绪 Channel 的集合，进行后续的 IO 操作。 下面详细介绍一下这三个概念。 ChannelChannel 是全双工的，可以比流更好地映射底层操作系统的 API，与流也非常相似，有以下几点区别： Channel 可以读也可以写，但流（InputStream 或 OutputStream）是单向的； 通道可以异步读写； 它是基于缓冲区（Buffer）进行读写； 在 Java 中提供以下几种 Channel： FileChannel：用于文件的读写； DatagramChannel：用于 UDP 数据读写； SocketChannel：用于 Socket 数据读写； ServerSocketChannel：监听 TCP 连接请求。 这些 Channel 类之间的继承关系如下图所示 从上图中，可以看出，Channel 可以分为两大类：用于网络读写的 SelectableChannel 和用于文件操作的 FileChannel。 其中，FileChannel 只能在阻塞模式下工作，具体可以参考Java NIO FileChannel文件通道。 NIO Scatter/GatherJava NIO 发布时内置了对 scatter/gather的支持： Scattering read 指的是从通道读取的操作能把数据写入多个 Buffer，也就是 scatter 代表了数据从一个 Channel 到多个 Buffer的过程。 Gathering write 则正好相反，表示的是从多个 Buffer 把数据写入到一个 Channel中。 示例如下，具体参考 Java NIO Scatter / Gather BufferBuffer，本质上是一块内存区，可以用来读写数据，它包含一些要写入或者要读出的数据。在 NIO 中，所有数据都是通过 Buffer 处理的，读取数据时，它是直接读到缓冲区中，写入数据时，写入到缓冲区。 最常用的缓冲区是 ByteBuffer，一个 ByteBuffer 提供了一组功能用于操作 byte 数组，除了 ByteBuffer，还有其他的一些 Buffer，如：CharBuffer、IntBuffer 等，它们之间的关系如下图所示。 基本用法Buffer 基本用法（读写数据过程）： 把数据写入 Buffer； 调用 flip()，Buffer 由写模式变为读模式； Buffer 中读取数据； 调用 clear() 清空 buffer，等待下次写入。 示例如下： Buffer 位置信息Buffer 实质上就是一块内存，用于读写数据，这块内存被 NIO Buffer 管理，一个 Buffer 有三个属性是必须掌握的，分别是： capacity：容量； position：位置； limit：限制； 其中，position 和 limit 的具体含义取决于当前 buffer 的模式，capacity 在两种模式下都表示容量，Buffer 读模式和写模式如下图所示。 容量（capacity） Buffer 有一块固定的内存，其大小就是 capacity，一旦 Buffer 写满，就需要清空已读数据以便下次继续写入新的数据； 位置（Position） 写模式时，当写入数据到 Buffer 的时候从一个确定的位置开始，初始化时这个位置 position 为0，写入数据后，position 的值就会指向数据之后的单元，position 最大的值可以达到 capacity-1； 读模式时，也需要从一个确定的位置开始，Buffer 从写模式变为读模式时，position 会归零，每次读取后，position 向后移动； 上限（limit） 写模式时，limit 就是能写入的最大数据量，等同于 Buffer 的容量； 读模式时，limit 代表我们能读取的最大容量，它的值等同于写模式下 position 位置。 Buffer 常用方法 flip()：把 buffer 从模式调整为读模式，在读模式下，可以读取所有已经写入的数据； clear()：清空整个 buffer； compact()：只清空已读取的数据，未被读取的数据会被移动到 buffer 的开始位置，写入位置则紧跟着未读数据之后； rewind()：将 position 置为0，这样我们可以重复读取 Buffer 中的数据，limit 保持不变； mark()和reset()：通过mark方法可以标记当前的position，通过reset来恢复mark的位置 equals()：判断两个 Buffer 是否相等，需满足：类型相同、Buffer 中剩余字节数相同、所有剩余字节相等； compareTo()：compareTo 比较 Buffer 中的剩余元素，只不过这个方法适用于比较排序的。 SelectorSelector 是 Java NIO 核心部分，简单来说，它的作用就是：Selector 不断轮询注册在其上的 Channel，如果某个 Channel 上面有新的 TCP 连接、读和写事件，这个 Channel 就处于就绪状态，会被 Selector 轮询出来，然后通过 SelectorKey() 可以获取就绪 Channel 的集合，进行后续的 IO 操作。 一个 Selector 可以轮询多个 Channel，由于 JDK 底层使用了 epoll() 实现，它并没有最大连接句柄 1024/2048 的限制，这就意味着只需要一个线程负责 Selector 的轮询，就可以连接上千上万的 Client。 注册 Channel举一个栗子，简单介绍 Selector 的使用。 register() 的第二个参数代表的是 selector 监听的事件类型，Selector 可以监听事件类型总共有以下四种： SelectionKey.OP_CONNECT：只会注册一次，成功之后（TCP 连接建立之后），这个监听事件就取消了； SelectionKey.OP_ACCEPT：主要用于服务端，就是监听是否有新的连接请求； SelectionKey.OP_READ：注册之后不会取消，监听是否数据到来； SelectionKey.OP_WRITE：最好的使用方法是每当发送数据时，就注册一次，然后再取消，否则每次 select 轮询时，注册 OP_WRITE 事件的 Channel 都是 ready 的，除非 socket send buffer 满了（参考 Communicating between nio OP_READ and OP_WRITE operations）。 SelectionKeySet&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); 返回的是已经就绪的 Channel 集合，SelectionKey 对象的详细属性如下图所示。 NIO 原理 Java NIO 实现的关键是 IO 多路复用（具体可以参考上篇文章：Linux 的 IO 多路复用模型），在 Linux 平台，Java NIO 是基于 epoll（2.6以上，之前是 Select） 来实现的。 Linux 的 select/epoll 使用的是 Reactor 网络 IO 模式。网络编程中，有两种常用的设计模式，它们都是基于事件驱动： Reactor 模式：主动模式，应用程序不断去轮询，问操作系统 IO 是否就绪，实际的 IO 操作还是由应用实现（IO 多路复用采用的模式）； Proactor 模式：被动模式，操作系统把 IO 完成后通知应用程序，此时数据已经就绪。 这两种模式详细内容可以参考两种高性能 I/O 设计模式 Reactor 和 Proactor一文。 NIO 编程 关于 Java NIO，有两种最常见的使用方式： 使用原生的 Java NIO（如 Kafka）； 使用 Netty（Hadoop 的 RPC 框架 Avro 底层使用 Netty 做通信框架）。 在实际使用中，推荐第二种，使用 Netty 将会大大提高开发效率，后续会写篇关于 Netty 的文章，介绍一下 Netty 的具体内容，这里使用一个基于 Java 原生 NIO API 的小示例，讲述一下 NIO 的使用方法。 Client 端NIO Client 创建序列图如下图所示（图片来自《Netty 权威指南》）。 具体的代码及注释参考：NIO Client 端代码。 Server 端NIO Server 创建序列图如下图所示（图片来自《Netty 权威指南》）。 具体的代码及注释参考：NIO Server 端代码。 IO 模型对比 在对比之前，先简单介绍 Java AIO 模型，这里就不再进行相应的展开了。 AIO NIO 2.0 中引入异步通道的概念，并提供了异步文件通道和异步套接字导通的实现，它是真正的异步非阻塞I IO，底层是利用事件驱动（AIO）实现，不需要多路复用器（Selector）对注册的通道进行轮组操作即可实现异步读写。 可以参考在 Java 7 中体会 NIO.2 异步执行的快乐 几种 IO 模型功能和特性对比 传统 BIO 伪异步 IO NIO AIO client 数：IO 线程数 1：1 M：N（M 可以大于 N） M：1 M：0（不需要额外的线程，被动回调） IO 类型（阻塞） 阻塞IO 阻塞IO 非阻塞IO 非阻塞IO IO 类型（同步） 同步 IO 同步 IO 同步 IO（IO 多路复用） 异步 IO 可靠性 非常差 差 高 高 吞吐量 低 中 高 高 本文主要是对 Java IO 模型总结，特别是对 NIO 模型的总结。 参考 《Netty 权威指南》； Java NIO 英文版，中文版；","categories":[{"name":"Java NIO","slug":"Java-NIO","permalink":"https://blog.fenxiangz.com/categories/Java-NIO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"NIO","slug":"NIO","permalink":"https://blog.fenxiangz.com/tags/NIO/"},{"name":"Reactor","slug":"Reactor","permalink":"https://blog.fenxiangz.com/tags/Reactor/"},{"name":"NIO模型","slug":"NIO模型","permalink":"https://blog.fenxiangz.com/tags/NIO%E6%A8%A1%E5%9E%8B/"}]},{"title":"从源码的角度再学「Thread」","slug":"java/basic/2019-01-21_java_thread_source","date":"2019-01-21T09:46:24.000Z","updated":"2020-12-20T16:47:02.965Z","comments":true,"path":"post/java/basic/2019-01-21_java_thread_source.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/basic/2019-01-21_java_thread_source.html","excerpt":"","text":"原创： zhangshaolin 张少林同学微信号: zhangshaolin_tonxue 功能介绍 分享 前言Java中的线程是使用Thread类实现的，Thread在初学Java的时候就学过了，也在实践中用过，不过一直没从源码的角度去看过它的实现，今天从源码的角度出发，再次学习Java Thread，愿此后对Thread的实践更加得心应手。 从注释开始相信阅读过JDK源码的同学都能感受到JDK源码中有非常详尽的注释，阅读某个类的源码应当先看看注释对它的介绍，注释原文就不贴了，以下是我对它的总结： Thread是程序中执行的线程，Java虚拟机允许应用程序同时允许多个执行线程 每个线程都有优先级的概念，具有较高优先级的线程优先于优先级较低的线程执行 每个线程都可以被设置为守护线程 当在某个线程中运行的代码创建一个新的Thread对象时，新的线程优先级跟创建线程一致 当Java虚拟机启动的时候都会启动一个叫做main的线程，它没有守护线程，main线程会继续执行，直到以下情况发送 Runtime 类的退出方法exit被调用并且安全管理器允许进行退出操作 所有非守护线程均已死亡，或者run方法执行结束正常返回结果，或者run方法抛出异常 创建线程第一种方式：继承Thread类，重写run方法 1 //定义线程类 2 class PrimeThread extends Thread &#123; 3 long minPrime; 4 PrimeThread(long minPrime) &#123; 5 this.minPrime = minPrime; 6 &#125; 7 public void run() &#123; 8 // compute primes larger than minPrime 9 &amp;nbsp;.&amp;nbsp;.&amp;nbsp;. 10 &#125; 11 &#125; 12 //启动线程 13 PrimeThread p = new PrimeThread(143); 14 p.start(); 创建线程第二种方式：实现Runnable接口，重写run方法，因为Java的单继承限制，通常使用这种方式创建线程更加灵活 1 //定义线程 2 class PrimeRun implements Runnable &#123; 3 long minPrime; 4 PrimeRun(long minPrime) &#123; 5 this.minPrime = minPrime; 6 &#125; 7 public void run() &#123; 8 // compute primes larger than minPrime 9 &amp;nbsp;.&amp;nbsp;.&amp;nbsp;. 10 &#125; 11 &#125; 12 //启动线程 13 PrimeRun p = new PrimeRun(143); 14 new Thread(p).start(); 创建线程时可以给线程指定名字，如果没有指定，会自动为它生成名字 除非另有说明，否则将null参数传递给Thread类中的构造函数或方法将导致抛出 NullPointerException Thread 常用属性阅读一个Java类，先从它拥有哪些属性入手： 1 //线程名称，创建线程时可以指定线程的名称 2 private volatile String name; 3 4 //线程优先级，可以设置线程的优先级 5 private int priority; 6 7 //可以配置线程是否为守护线程，默认为false 8 private boolean daemon = false; 9 10 //最终执行线程任务的`Runnable` 11 private Runnable target; 12 13 //描述线程组的类 14 private ThreadGroup group; 15 16 //此线程的上下文ClassLoader 17 private ClassLoader contextClassLoader; 18 19 //所有初始化线程的数目，用于自动编号匿名线程，当没有指定线程名称时，会自动为其编号 20 private static int threadInitNumber; 21 22 //此线程请求的堆栈大小，如果创建者没有指定堆栈大小，则为0。, 虚拟机可以用这个数字做任何喜欢的事情。, 一些虚拟机会忽略它。 23 private long stackSize; 24 25 //线程id 26 private long tid; 27 28 //用于生成线程ID 29 private static long threadSeqNumber; 30 31 //线程状态 32 private volatile int threadStatus = 0; 33 34 //线程可以拥有的最低优先级 35 public final static int MIN_PRIORITY = 1; 36 37 //分配给线程的默认优先级。 38 public final static int NORM_PRIORITY = 5; 39 40 //线程可以拥有的最大优先级 41 public final static int MAX_PRIORITY = 10; Thread 构造方法了解了属性之后，看看Thread实例是怎么构造的？先预览下它大致有多少个构造方法： 查看每个构造方法内部源码，发现均调用的是名为init的私有方法，再看init方法有两个重载，而其核心方法如下： 1 /** 2 * Initializes a Thread. 3 * 4 * @param g 线程组 5 * @param target 最终执行任务的 `run()` 方法的对象 6 * @param name 新线程的名称 7 * @param stackSize 新线程所需的堆栈大小，或者 0 表示要忽略此参数 8 * @param acc 要继承的AccessControlContext，如果为null，则为 AccessController.getContext() 9 * @param inheritThreadLocals 如果为 true，从构造线程继承可继承的线程局部的初始值 10 */ 11 private void init(ThreadGroup g, Runnable target, String name, 12 long stackSize, AccessControlContext acc, 13 boolean inheritThreadLocals) &#123; 14 //线程名称为空，直接抛出空指针异常 15 if (name == null) &#123; 16 throw new NullPointerException(&quot;name cannot be null&quot;); 17 &#125; 18 //初始化当前线程对象的线程名称 19 this.name = name; 20 //获取当前正在执行的线程为父线程 21 Thread parent = currentThread(); 22 //获取系统安全管理器 23 SecurityManager security = System.getSecurityManager(); 24 //如果线程组为空 25 if (g == null) &#123; 26 //如果安全管理器不为空 27 if (security != null) &#123; 28 //获取SecurityManager中的线程组 29 g = security.getThreadGroup(); 30 &#125; 31 //如果获取的线程组还是为空 32 if (g == null) &#123; 33 //则使用父线程的线程组 34 g = parent.getThreadGroup(); 35 &#125; 36 &#125; 37 38 //检查安全权限 39 g.checkAccess(); 40 41 //使用安全管理器检查是否有权限 42 if (security != null) &#123; 43 if (isCCLOverridden(getClass())) &#123; 44 security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION); 45 &#125; 46 &#125; 47 48 //线程组中标记未启动的线程数+1，这里方法是同步的，防止出现线程安全问题 49 g.addUnstarted(); 50 51 //初始化当前线程对象的线程组 52 this.group = g; 53 //初始化当前线程对象的是否守护线程属性，注意到这里初始化时跟父线程一致 54 this.daemon = parent.isDaemon(); 55 //初始化当前线程对象的线程优先级属性，注意到这里初始化时跟父线程一致 56 this.priority = parent.getPriority(); 57 //这里初始化类加载器 58 if (security == null || isCCLOverridden(parent.getClass())) 59 this.contextClassLoader = parent.getContextClassLoader(); 60 else 61 this.contextClassLoader = parent.contextClassLoader; 62 this.inheritedAccessControlContext = 63 acc != null ? acc : AccessController.getContext(); 64 //初始化当前线程对象的最终执行任务对象 65 this.target = target; 66 //这里再对线程的优先级字段进行处理 67 setPriority(priority); 68 if (inheritThreadLocals &amp;&amp; parent.inheritableThreadLocals != null) 69 this.inheritableThreadLocals = 70 ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); 71 //初始化当前线程对象的堆栈大小 72 this.stackSize = stackSize; 73 74 //初始化当前线程对象的线程ID，该方法是同步的，内部实际上是threadSeqNumber++ 75 tid = nextThreadID(); 76 &#125; 另一个重载init私有方法如下，实际上内部调用的是上述init方法： 1 private void init(ThreadGroup g, Runnable target, String name, 2 long stackSize) &#123; 3 init(g, target, name, stackSize, null, true); 4 &#125; 接下来看看所有构造方法： 空构造方法 1 public Thread() &#123; 2 init(null, null, &quot;Thread-&quot; + nextThreadNum(), 0); 3 &#125; 内部调用的是init第二个重载方法，参数基本都是默认值，线程名称写死为&quot;Thread-&quot; + nextThreadNum()格式，nextThreadNum()为一个同步方法，内部维护一个静态属性表示线程的初始化数量+1： 1 private static int threadInitNumber; 2 private static synchronized int nextThreadNum() &#123; 3 return threadInitNumber++; 4 &#125; 与第一个构造方法区别在于可以自定义Runnable对象 自定义执行任务Runnable对象的构造方法 1 private static int threadInitNumber; 2 private static synchronized int nextThreadNum() &#123; 3 return threadInitNumber++; 4 &#125; 自定义执行任务Runnable对象和AccessControlContext对象的构造方法 1 Thread(Runnable target, AccessControlContext acc) &#123; 2 init(null, target, &quot;Thread-&quot; + nextThreadNum(), 0, acc, false); 3 &#125; 自定义线程组ThreadGroup和执行任务Runnable对象的构造方法 1 public Thread(ThreadGroup group, Runnable target) &#123; 2 init(group, target, &quot;Thread-&quot; + nextThreadNum(), 0); 3 &#125; 自定义线程名称name的构造方法 1 public Thread(String name) &#123; 2 init(null, null, name, 0); 3 &#125; 自定义线程组ThreadGroup和线程名称name的构造方法 1 public Thread(String name) &#123; 2 init(null, null, name, 0); 3 &#125; 自定义执行任务Runnable对象和线程名称name的构造方法 1 public Thread(Runnable target, String name) &#123; 2 init(null, target, name, 0); 3 &#125; 自定义线程组ThreadGroup和线程名称name和执行任务Runnable对象的构造方法 1 public Thread(ThreadGroup group, Runnable target, String name) &#123; 2 init(group, target, name, 0); 3 &#125; 全部属性都是自定义的构造方法 1 public Thread(ThreadGroup group, Runnable target, String name, 2 long stackSize) &#123; 3 init(group, target, name, stackSize); 4 &#125; Thread提供了非常灵活的重载构造方法，方便开发者自定义各种参数的Thread对象。 常用方法这里记录一些比较常见的方法吧，对于Thread中存在的一些本地方法，我们暂且不用管它～ 设置线程名称设置线程名称，该方法为同步方法，为了防止出现线程安全问题，可以手动调用Thread的实例方法设置名称，也可以在构造Thread时在构造方法中传入线程名称，我们通常都是在构造参数时设置 1 public final synchronized void setName(String name) &#123; 2 //检查安全权限 3 checkAccess(); 4 //如果形参为空，抛出空指针异常 5 if (name == null) &#123; 6 throw new NullPointerException(&quot;name cannot be null&quot;); 7 &#125; 8 //给当前线程对象设置名称 9 this.name = name; 10 if (threadStatus != 0) &#123; 11 setNativeName(name); 12 &#125; 13 &#125; 获取线程名称内部直接返回当前线程对象的名称属性 1 public final String getName() &#123; 2 return name; 3 &#125; 启动线程 1 public synchronized void start() &#123; 2 //如果不是刚创建的线程，抛出异常 3 if (threadStatus != 0) 4 throw new IllegalThreadStateException(); 5 6 //通知线程组，当前线程即将启动，线程组当前启动线程数+1，未启动线程数-1 7 group.add(this); 8 9 //启动标识 10 boolean started = false; 11 try &#123; 12 //直接调用本地方法启动线程 13 start0(); 14 //设置启动标识为启动成功 15 started = true; 16 &#125; finally &#123; 17 try &#123; 18 //如果启动呢失败 19 if (!started) &#123; 20 //线程组内部移除当前启动的线程数量-1，同时启动失败的线程数量+1 21 group.threadStartFailed(this); 22 &#125; 23 &#125; catch (Throwable ignore) &#123; 24 /* do nothing. If start0 threw a Throwable then 25 it will be passed up the call stack */ 26 &#125; 27 &#125; 28 &#125; 我们正常的启动线程都是调用Thread的start()方法，然后Java虚拟机内部会去调用Thred的run方法，可以看到Thread类也是实现Runnable接口，重写了run方法的： 1 @Override 2 public void run() &#123; 3 //当前执行任务的Runnable对象不为空，则调用其run方法 4 if (target != null) &#123; 5 target.run(); 6 &#125; 7 &#125; Thread的两种使用方式： 继承Thread类，重写run方法，那么此时是直接执行run方法的逻辑，不会使用target.run(); 实现Runnable接口，重写run方法，因为Java的单继承限制，通常使用这种方式创建线程更加灵活，这里真正的执行逻辑就会交给自定义Runnable去实现 设置守护线程本质操作是设置daemon属性 1 public final void setDaemon(boolean on) &#123; 2 //检查是否有安全权限 3 checkAccess(); 4 //本地方法，测试此线程是否存活。, 如果一个线程已经启动并且尚未死亡，则该线程处于活动状态 5 if (isAlive()) &#123; 6 //如果线程先启动后再设置守护线程，将抛出异常 7 throw new IllegalThreadStateException(); 8 &#125; 9 //设置当前守护线程属性 10 daemon = on; 11 &#125; 判断线程是否为守护线程 1 public final boolean isDaemon() &#123; 2 //直接返回当前对象的守护线程属性 3 return daemon; 4 &#125; 线程状态先来个线程状态图： 获取线程状态： 1 public State getState() &#123; 2 //由虚拟机实现，获取当前线程的状态 3 return sun.misc.VM.toThreadState(threadStatus); 4 &#125; 线程状态主要由内部枚举类State组成： 1 public enum State &#123; 2 3 NEW, 4 5 6 RUNNABLE, 7 8 9 BLOCKED, 10 11 12 WAITING, 13 14 15 TIMED_WAITING, 16 17 18 TERMINATED; 19 &#125; NEW：刚刚创建，尚未启动的线程处于此状态 RUNNABLE：在Java虚拟机中执行的线程处于此状态 BLOCKED：被阻塞等待监视器锁的线程处于此状态，比如线程在执行过程中遇到synchronized同步块，就会进入此状态，此时线程暂停执行，直到获得请求的锁 WAITING：无限期等待另一个线程执行特定操作的线程处于此状态 通过 wait() 方法等待的线程在等待 notify() 方法 通过 join() 方法等待的线程则会等待目标线程的终止 TIMED_WAITING：正在等待另一个线程执行动作，直到指定等待时间的线程处于此状态 通过 wait() 方法，携带超时时间，等待的线程在等待 notify() 方法 通过 join() 方法，携带超时时间，等待的线程则会等待目标线程的终止 TERMINATED：已退出的线程处于此状态，此时线程无法再回到 RUNNABLE 状态 线程休眠这是一个静态的本地方法，使当前执行的线程休眠暂停执行 millis 毫秒，当休眠被中断时会抛出InterruptedException中断异常 1 /** 2 * Causes the currently executing thread to sleep (temporarily cease 3 * execution) for the specified number of milliseconds, subject to 4 * the precision and accuracy of system timers and schedulers. The thread 5 * does not lose ownership of any monitors. 6 * 7 * @param millis 8 * the length of time to sleep in milliseconds 9 * 10 * @throws IllegalArgumentException 11 * if the value of &#123;@code millis&#125; is negative 12 * 13 * @throws InterruptedException 14 * if any thread has interrupted the current thread. The 15 * &lt;i&gt;interrupted status&lt;/i&gt; of the current thread is 16 * cleared when this exception is thrown. 17 */ 18 public static native void sleep(long millis) throws InterruptedException; 检查线程是否存活本地方法，测试此线程是否存活。 如果一个线程已经启动并且尚未死亡，则该线程处于活动状态。 1 /** 2 * Tests if this thread is alive. A thread is alive if it has 3 * been started and has not yet died. 4 * 5 * @return &lt;code&gt;true&lt;/code&gt; if this thread is alive; 6 * &lt;code&gt;false&lt;/code&gt; otherwise. 7 */ 8 public final native boolean isAlive(); 线程优先级 设置线程优先级 1 /** 2 * Changes the priority of this thread. 3 * &lt;p&gt; 4 * First the &lt;code&gt;checkAccess&lt;/code&gt; method of this thread is called 5 * with no arguments. This may result in throwing a 6 * &lt;code&gt;SecurityException&lt;/code&gt;. 7 * &lt;p&gt; 8 * Otherwise, the priority of this thread is set to the smaller of 9 * the specified &lt;code&gt;newPriority&lt;/code&gt; and the maximum permitted 10 * priority of the thread&#39;s thread group. 11 * 12 * @param newPriority priority to set this thread to 13 * @exception IllegalArgumentException If the priority is not in the 14 * range &lt;code&gt;MIN_PRIORITY&lt;/code&gt; to 15 * &lt;code&gt;MAX_PRIORITY&lt;/code&gt;. 16 * @exception SecurityException if the current thread cannot modify 17 * this thread. 18 * @see #getPriority 19 * @see #checkAccess() 20 * @see #getThreadGroup() 21 * @see #MAX_PRIORITY 22 * @see #MIN_PRIORITY 23 * @see ThreadGroup#getMaxPriority() 24 */ 25 public final void setPriority(int newPriority) &#123; 26 //线程组 27 ThreadGroup g; 28 //检查安全权限 29 checkAccess(); 30 //检查优先级形参范围 31 if (newPriority &gt; MAX_PRIORITY || newPriority &lt; MIN_PRIORITY) &#123; 32 throw new IllegalArgumentException(); 33 &#125; 34 if((g = getThreadGroup()) != null) &#123; 35 //如果优先级形参大于线程组最大线程最大优先级 36 if (newPriority &gt; g.getMaxPriority()) &#123; 37 //则使用线程组的优先级数据 38 newPriority = g.getMaxPriority(); 39 &#125; 40 //调用本地设置线程优先级方法 41 setPriority0(priority = newPriority); 42 &#125; 43 &#125; 线程中断有一个stop()实例方法可以强制终止线程，不过这个方法因为太过于暴力，已经被标记为过时方法，不建议程序员再使用，因为强制终止线程会导致数据不一致的问题。 这里关于线程中断的方法涉及三个： 1 //实例方法，通知线程中断，设置标志位 2 public void interrupt()&#123;&#125; 3 //静态方法，检查当前线程的中断状态，同时会清除当前线程的中断标志位状态 4 public static boolean interrupted()&#123;&#125; 5 //实例方法，检查当前线程是否被中断，其实是检查中断标志位 6 public boolean isInterrupted()&#123;&#125; interrupt() 方法解析 1 /** 2 * Interrupts this thread. 3 * 4 * &lt;p&gt; Unless the current thread is interrupting itself, which is 5 * always permitted, the &#123;@link #checkAccess() checkAccess&#125; method 6 * of this thread is invoked, which may cause a &#123;@link 7 * SecurityException&#125; to be thrown. 8 * 9 * &lt;p&gt; If this thread is blocked in an invocation of the &#123;@link 10 * Object#wait() wait()&#125;, &#123;@link Object#wait(long) wait(long)&#125;, or &#123;@link 11 * Object#wait(long, int) wait(long, int)&#125; methods of the &#123;@link Object&#125; 12 * class, or of the &#123;@link #join()&#125;, &#123;@link #join(long)&#125;, &#123;@link 13 * #join(long, int)&#125;, &#123;@link #sleep(long)&#125;, or &#123;@link #sleep(long, int)&#125;, 14 * methods of this class, then its interrupt status will be cleared and it 15 * will receive an &#123;@link InterruptedException&#125;. 16 * 17 * &lt;p&gt; If this thread is blocked in an I/O operation upon an &#123;@link 18 * java.nio.channels.InterruptibleChannel InterruptibleChannel&#125; 19 * then the channel will be closed, the thread&#39;s interrupt 20 * status will be set, and the thread will receive a &#123;@link 21 * java.nio.channels.ClosedByInterruptException&#125;. 22 * 23 * &lt;p&gt; If this thread is blocked in a &#123;@link java.nio.channels.Selector&#125; 24 * then the thread&#39;s interrupt status will be set and it will return 25 * immediately from the selection operation, possibly with a non-zero 26 * value, just as if the selector&#39;s &#123;@link 27 * java.nio.channels.Selector#wakeup wakeup&#125; method were invoked. 28 * 29 * &lt;p&gt; If none of the previous conditions hold then this thread&#39;s interrupt 30 * status will be set. &lt;/p&gt; 31 * 32 * &lt;p&gt; Interrupting a thread that is not alive need not have any effect. 33 * 34 * @throws SecurityException 35 * if the current thread cannot modify this thread 36 * 37 * @revised 6.0 38 * @spec JSR-51 39 */ 40 public void interrupt() &#123; 41 //检查是否是自身调用 42 if (this != Thread.currentThread()) 43 //检查安全权限,这可能导致抛出&#123;@link * SecurityException&#125;。 44 checkAccess(); 45 46 //同步代码块 47 synchronized (blockerLock) &#123; 48 Interruptible b = blocker; 49 //检查是否是阻塞线程调用 50 if (b != null) &#123; 51 //设置线程中断标志位 52 interrupt0(); 53 //此时抛出异常，将中断标志位设置为false,此时我们正常会捕获该异常，重新设置中断标志位 54 b.interrupt(this); 55 return; 56 &#125; 57 &#125; 58 //如无意外，则正常设置中断标志位 59 interrupt0(); 60 &#125; 线程中断方法不会使线程立即退出，而是给线程发送一个通知，告知目标线程，有人希望你退出啦～ 只能由自身调用，否则可能会抛出 SecurityException 调用中断方法是由目标线程自己决定是否中断，而如果同时调用了wait,join,sleep等方法，会使当前线程进入阻塞状态，此时有可能发生InterruptedException异常 被阻塞的线程再调用中断方法是不合理的 中断不活动的线程不会产生任何影响 检查线程是否被中断: 1 /** 2 * Tests whether this thread has been interrupted. The &lt;i&gt;interrupted 3 * status&lt;/i&gt; of the thread is unaffected by this method. 4 5 测试此线程是否已被中断。, 线程的&lt;i&gt;中断*状态&lt;/ i&gt;不受此方法的影响。 6 * 7 * &lt;p&gt;A thread interruption ignored because a thread was not alive 8 * at the time of the interrupt will be reflected by this method 9 * returning false. 10 * 11 * @return &lt;code&gt;true&lt;/code&gt; if this thread has been interrupted; 12 * &lt;code&gt;false&lt;/code&gt; otherwise. 13 * @see #interrupted() 14 * @revised 6.0 15 */ 16 public boolean isInterrupted() &#123; 17 return isInterrupted(false); 18 &#125; 静态方法,会清空当前线程的中断标志位： 1 /** 2 *测试当前线程是否已被中断。, 此方法清除线程的* &lt;i&gt;中断状态&lt;/ i&gt;。, 换句话说，如果要连续两次调用此方法，则* second调用将返回false（除非当前线程再次被中断，在第一次调用已清除其中断的*状态 之后且在第二次调用已检查之前）, 它） 3 * 4 * &lt;p&gt;A thread interruption ignored because a thread was not alive 5 * at the time of the interrupt will be reflected by this method 6 * returning false. 7 * 8 * @return &lt;code&gt;true&lt;/code&gt; if the current thread has been interrupted; 9 * &lt;code&gt;false&lt;/code&gt; otherwise. 10 * @see #isInterrupted() 11 * @revised 6.0 12 */ 13 public static boolean interrupted() &#123; 14 return currentThread().isInterrupted(true); 15 &#125; 总结记录自己阅读Thread类源码的一些思考，不过对于其中用到的很多本地方法只能望而却步，还有一些代码没有看明白，暂且先这样吧，如果有不足之处，请留言告知我，谢谢！后续会在实践中对Thread做出更多总结记录。","categories":[{"name":"Java 基础","slug":"Java-基础","permalink":"https://blog.fenxiangz.com/categories/Java-%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"Java线程","slug":"Java线程","permalink":"https://blog.fenxiangz.com/tags/Java%E7%BA%BF%E7%A8%8B/"},{"name":"源码","slug":"源码","permalink":"https://blog.fenxiangz.com/tags/%E6%BA%90%E7%A0%81/"}]},{"title":"Java 经典设计模式（3）：十一种行为型模式","slug":"design/2019-01-05_Java经典设计模式_3_十一种行为型模式","date":"2019-01-05T00:00:00.000Z","updated":"2020-12-20T16:47:02.950Z","comments":true,"path":"post/design/2019-01-05_Java经典设计模式_3_十一种行为型模式.html","link":"","permalink":"https://blog.fenxiangz.com/post/design/2019-01-05_Java%E7%BB%8F%E5%85%B8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F_3_%E5%8D%81%E4%B8%80%E7%A7%8D%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F.html","excerpt":"行为型模式细分为如下11种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。接下来对11种行为型模式逐个进行介绍。 一、策略模式策略模式定义了一系列算法，并将每个算法封装起来，使他们可以相互替换，且算法的变化不会影响到使用算法的客户。需要设计一个接口，为一系列实现类提供统一的方法，多个实现类实现该接口，设计一个抽象类（可有可无，属于辅助类，视实际需求是否添加），提供辅助函数。 首先统一接口： 12345package com.model.behaviour; public interface ICalculator &#123; public int calculate(String exp);&#125; 辅助类： 123456789101112package com.model.behaviour; public abstract class AbstractCalculator &#123; public int[] split(String exp, String opt) &#123; String array[] = exp.split(opt); int arrayInt[] = new int[2]; arrayInt[0] = Integer.parseInt(array[0]); arrayInt[1] = Integer.parseInt(array[1]); return arrayInt; &#125;&#125;","text":"行为型模式细分为如下11种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。接下来对11种行为型模式逐个进行介绍。 一、策略模式策略模式定义了一系列算法，并将每个算法封装起来，使他们可以相互替换，且算法的变化不会影响到使用算法的客户。需要设计一个接口，为一系列实现类提供统一的方法，多个实现类实现该接口，设计一个抽象类（可有可无，属于辅助类，视实际需求是否添加），提供辅助函数。 首先统一接口： 12345package com.model.behaviour; public interface ICalculator &#123; public int calculate(String exp);&#125; 辅助类： 123456789101112package com.model.behaviour; public abstract class AbstractCalculator &#123; public int[] split(String exp, String opt) &#123; String array[] = exp.split(opt); int arrayInt[] = new int[2]; arrayInt[0] = Integer.parseInt(array[0]); arrayInt[1] = Integer.parseInt(array[1]); return arrayInt; &#125;&#125; 三个实现类： 12345678910package com.model.behaviour; public class Plus extends AbstractCalculator implements ICalculator &#123; @Override public int calculate(String exp) &#123; int arrayInt[] = split(exp, &quot;\\\\+&quot;); return arrayInt[0] + arrayInt[1]; &#125;&#125; 1234567891011package com.model.behaviour; public class Minus extends AbstractCalculator implements ICalculator &#123; @Override public int calculate(String exp) &#123; int arrayInt[] = split(exp, &quot;\\\\-&quot;); return arrayInt[0] - arrayInt[1]; &#125; &#125; 12345678910package com.model.behaviour; public class Multiply extends AbstractCalculator implements ICalculator &#123; @Override public int calculate(String exp) &#123; int arrayInt[] = split(exp,&quot;\\\\*&quot;); return arrayInt[0]*arrayInt[1]; &#125; &#125; 测试类： 1234567891011package com.model.behaviour; public class StrategyTest &#123; public static void main(String[] args) &#123; String exp = &quot;8-2&quot;; ICalculator cal = new Minus(); int result = cal.calculate(exp); System.out.println(exp + &quot;=&quot; + result); &#125;&#125; 策略模式的决定权在用户，系统本身提供不同算法的实现，新增或者删除算法，对各种算法做封装。因此，策略模式多用在算法决策系统中，外部用户只需要决定用哪个算法即可。 二、模板方法模式解释一下模板方法模式，就是指：一个抽象类中，有一个主方法，再定义1…n个方法，可以是抽象的，也可以是实际的方法，定义一个类，继承该抽象类，重写抽象方法，通过调用抽象类，实现对子类的调用。 就是在AbstractCalculator类中定义一个主方法calculate，calculate()调用spilt()等，Plus和Minus分别继承AbstractCalculator类，通过对AbstractCalculator的调用实现对子类的调用，看下面的例子： 123456789101112131415161718192021package com.model.behaviour; public abstract class AbstractCalculator &#123; /*主方法，实现对本类其它方法的调用*/ public final int calculate(String exp,String opt)&#123; int array[] = split(exp,opt); return calculate(array[0],array[1]); &#125; /*被子类重写的方法*/ abstract public int calculate(int num1,int num2); public int[] split(String exp,String opt)&#123; String array[] = exp.split(opt); int arrayInt[] = new int[2]; arrayInt[0] = Integer.parseInt(array[0]); arrayInt[1] = Integer.parseInt(array[1]); return arrayInt; &#125; &#125; 123456789package com.model.behaviour; public class Plus extends AbstractCalculator &#123; @Override public int calculate(int num1,int num2) &#123; return num1 + num2; &#125; &#125; 1234567891011package com.model.behaviour; public class StrategyTest &#123; public static void main(String[] args) &#123; String exp = &quot;8+8&quot;; AbstractCalculator cal = new Plus(); int result = cal.calculate(exp, &quot;\\\\+&quot;); System.out.println(result); &#125; &#125; 三、观察者模式包括这个模式在内的接下来的四个模式，都是类和类之间的关系，不涉及到继承。 观察者模式很好理解，类似于邮件订阅和RSS订阅，当我们浏览一些博客或wiki时，经常会看到RSS图标，就这的意思是，当你订阅了该文章，如果后续有更新，会及时通知你。其实，简单来讲就一句话：当一个对象变化时，其它依赖该对象的对象都会收到通知，并且随着变化！对象之间是一种一对多的关系。 12345package com.model.behaviour; public interface Observer &#123; public void update(); &#125; 12345678910&#96;&#96;&#96; javapackage com.model.behaviour; public class Observer1 implements Observer &#123; @Override public void update() &#123; System.out.println(&quot;observer1 has received!&quot;); &#125; &#125; 12345678910package com.model.behaviour; public class Observer2 implements Observer &#123; @Override public void update() &#123; System.out.println(&quot;observer2 has received!&quot;); &#125; &#125; 12345678910111213141516package com.model.behaviour; public interface Subject &#123; /*增加观察者*/ public void add(Observer observer); /*删除观察者*/ public void del(Observer observer); /*通知所有的观察者*/ public void notifyObservers(); /*自身的操作*/ public void operation(); &#125; 1234567891011121314151617181920212223242526package com.model.behaviour; import java.util.Enumeration;import java.util.Vector; public abstract class AbstractSubject implements Subject &#123; private Vector&lt;Observer&gt; vector = new Vector&lt;Observer&gt;(); @Override public void add(Observer observer) &#123; vector.add(observer); &#125; @Override public void del(Observer observer) &#123; vector.remove(observer); &#125; @Override public void notifyObservers() &#123; Enumeration&lt;Observer&gt; enumo = vector.elements(); while(enumo.hasMoreElements())&#123; enumo.nextElement().update(); &#125; &#125; &#125; 1234567891011package com.model.behaviour; public class MySubject extends AbstractSubject &#123; @Override public void operation() &#123; System.out.println(&quot;update self!&quot;); notifyObservers(); &#125; &#125; 12345678910111213package com.model.behaviour; public class ObserverTest &#123; public static void main(String[] args) &#123; Subject sub = new MySubject(); sub.add(new Observer1()); sub.add(new Observer2()); sub.operation(); &#125; &#125; 运行结果： 123update self!observer1 has received!observer2 has received! 也许看完实例之后还是比较抽象，再将文字描述和代码实例看一两遍吧，然后结合工作中看哪些场景可以使用这种模式以加深理解。 四、迭代子模式顾名思义，迭代器模式就是顺序访问聚集中的对象，一般来说，集合中非常常见，如果对集合类比较熟悉的话，理解本模式会十分轻松。这句话包含两层意思：一是需要遍历的对象，即聚集对象，二是迭代器对象，用于对聚集对象进行遍历访问。 具体来看看代码实例： 123456789101112package com.model.behaviour; public interface Collection &#123; public Iterator iterator(); /* 取得集合元素 */ public Object get(int i); /* 取得集合大小 */ public int size();&#125; 1234567891011121314package com.model.behaviour; public interface Iterator &#123; // 前移 public Object previous(); // 后移 public Object next(); public boolean hasNext(); // 取得第一个元素 public Object first();&#125; 123456789101112131415161718192021package com.model.behaviour; public class MyCollection implements Collection &#123; public String string[] = &#123; &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot; &#125;; @Override public Iterator iterator() &#123; return new MyIterator(this); &#125; @Override public Object get(int i) &#123; return string[i]; &#125; @Override public int size() &#123; return string.length; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.model.behaviour; public class MyIterator implements Iterator &#123; private Collection collection; private int pos = -1; public MyIterator(Collection collection)&#123; this.collection = collection; &#125; @Override public Object previous() &#123; if(pos &gt; 0)&#123; pos--; &#125; return collection.get(pos); &#125; @Override public Object next() &#123; if(pos&lt;collection.size()-1)&#123; pos++; &#125; return collection.get(pos); &#125; @Override public boolean hasNext() &#123; if(pos&lt;collection.size()-1)&#123; return true; &#125;else&#123; return false; &#125; &#125; @Override public Object first() &#123; pos = 0; return collection.get(pos); &#125; &#125; 123456789101112package com.model.behaviour; public class Test &#123; public static void main(String[] args) &#123; Collection collection = new MyCollection(); Iterator it = (Iterator) collection.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125; &#125; &#125; 输出结果： 12345ABCDE 此处我们貌似模拟了一个集合类的过程，感觉是不是很爽？其实JDK中各个类也都是这些基本的东西，加一些设计模式，再加一些优化放到一起的，只要我们把这些东西学会了，掌握好了，我们也可以写出自己的集合类，甚至框架！ 五、责任链模式责任链模式，有多个对象，每个对象持有对下一个对象的引用，这样就会形成一条链，请求在这条链上传递，直到某一对象决定处理该请求。但是发出者并不清楚到底最终那个对象会处理该请求，所以，责任链模式可以实现，在隐瞒客户端的情况下，对系统进行动态的调整。 12345package com.model.behaviour; public interface Handler &#123; public void operator(); &#125; 123456789101112131415package com.model.behaviour; public abstract class AbstractHandler &#123; private Handler handler; public Handler getHandler() &#123; return handler; &#125; public void setHandler(Handler handler) &#123; this.handler = handler; &#125; &#125; 123456789101112131415161718package com.model.behaviour; public class MyHandler extends AbstractHandler implements Handler &#123; private String name; public MyHandler(String name) &#123; this.name = name; &#125; @Override public void operator() &#123; System.out.println(name + &quot;deal!&quot;); if (getHandler() != null) &#123; getHandler().operator(); &#125; &#125;&#125; 123456789101112131415package com.model.behaviour; public class Test &#123; public static void main(String[] args) &#123; MyHandler h1 = new MyHandler(&quot;h1&quot;); MyHandler h2 = new MyHandler(&quot;h2&quot;); MyHandler h3 = new MyHandler(&quot;h3&quot;); h1.setHandler(h2); h2.setHandler(h3); h1.operator(); &#125;&#125; 运行结果： 123h1deal!h2deal!h3deal! 此处强调一点就是，链接上的请求可以是一条链，可以是一个树，还可以是一个环，模式本身不约束这个，需要我们自己去实现，同时，在一个时刻，命令只允许由一个对象传给另一个对象，而不允许传给多个对象。 六、命令模式命令模式很好理解，举个例子，司令员下令让士兵去干件事情，从整个事情的角度来考虑，司令员的作用是，发出口令，口令经过传递，传到了士兵耳朵里，士兵去执行。这个过程好在，三者相互解耦，任何一方都不用去依赖其他人，只需要做好自己的事儿就行，司令员要的是结果，不会去关注到底士兵是怎么实现的。 12345package com.model.behaviour; public interface Command &#123; public void exe(); &#125; 123456789101112131415package com.model.behaviour; public class MyCommand implements Command &#123; private Receiver receiver; public MyCommand(Receiver receiver) &#123; this.receiver = receiver; &#125; @Override public void exe() &#123; receiver.action(); &#125; &#125; 1234567891011121314package com.model.behaviour; public class Invoker &#123; private Command command; public Invoker(Command command) &#123; this.command = command; &#125; public void action() &#123; command.exe(); &#125;&#125; 1234567891011package com.model.behaviour; public class Test &#123; public static void main(String[] args) &#123; Receiver receiver = new Receiver(); Command cmd = new MyCommand(receiver); Invoker invoker = new Invoker(cmd); invoker.action(); &#125; &#125; 命令模式的目的就是达到命令的发出者和执行者之间解耦，实现请求和执行分开，熟悉Struts的同学应该知道，Struts其实就是一种将请求和呈现分离的技术，其中必然涉及命令模式的思想！ 七、备忘录模式主要目的是保存一个对象的某个状态，以便在适当的时候恢复对象，个人觉得叫备份模式更形象些，通俗的讲下：假设有原始类A，A中有各种属性，A可以决定需要备份的属性，备忘录类B是用来存储A的一些内部状态，类C呢，就是一个用来存储备忘录的，且只能存储，不能修改等操作。 1234567891011121314151617181920212223242526package com.model.behaviour; public class Original &#123; private String value; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; public Original(String value) &#123; this.value = value; &#125; public Memento createMemento()&#123; return new Memento(value); &#125; public void restoreMemento(Memento memento)&#123; this.value = memento.getValue(); &#125; &#125; 123456789101112131415161718package com.model.behaviour; public class Memento &#123; private String value; public Memento(String value) &#123; this.value = value; &#125; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125;&#125; 123456789101112131415161718package com.model.behaviour; public class Storage &#123; private Memento memento; public Storage(Memento memento) &#123; this.memento = memento; &#125; public Memento getMemento() &#123; return memento; &#125; public void setMemento(Memento memento) &#123; this.memento = memento; &#125; &#125; 12345678910111213141516171819202122package com.model.behaviour; public class Test &#123; public static void main(String[] args) &#123; // 创建原始类 Original origi = new Original(&quot;egg&quot;); // 创建备忘录 Storage storage = new Storage(origi.createMemento()); // 修改原始类的状态 System.out.println(&quot;初始化状态为：&quot; + origi.getValue()); origi.setValue(&quot;niu&quot;); System.out.println(&quot;修改后的状态为：&quot; + origi.getValue()); // 回复原始类的状态 origi.restoreMemento(storage.getMemento()); System.out.println(&quot;恢复后的状态为：&quot; + origi.getValue()); &#125;&#125; 输出结果： 1234初始化状态为：egg修改后的状态为：niu恢复后的状态为：egg如果还不能理解，可以给Original类添加一个属性name，然后其他类进行相应的修改试试。 八、状态模式核心思想就是：当对象的状态改变时，同时改变其行为，很好理解！就拿QQ来说，有几种状态，在线、隐身、忙碌等，每个状态对应不同的操作，而且你的好友也能看到你的状态，所以，状态模式就两点：1、可以通过改变状态来获得不同的行为。2、你的好友能同时看到你的变化。 12345678910111213141516171819202122package com.model.behaviour; public class State &#123; private String value; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; public void method1()&#123; System.out.println(&quot;execute the first opt!&quot;); &#125; public void method2()&#123; System.out.println(&quot;execute the second opt!&quot;); &#125; &#125; 123456789101112131415161718192021222324252627package com.model.behaviour; public class Context &#123; private State state; public Context(State state) &#123; this.state = state; &#125; public State getState() &#123; return state; &#125; public void setState(State state) &#123; this.state = state; &#125; public void method() &#123; System.out.println(&quot;状态为：&quot; + state.getValue()); if (state.getValue().equals(&quot;state1&quot;)) &#123; state.method1(); &#125; else if (state.getValue().equals(&quot;state2&quot;)) &#123; state.method2(); &#125; &#125;&#125; 123456789101112131415161718package com.model.behaviour; public class Test &#123; public static void main(String[] args) &#123; State state = new State(); Context context = new Context(state); //设置第一种状态 state.setValue(&quot;state1&quot;); context.method(); //设置第二种状态 state.setValue(&quot;state2&quot;); context.method(); &#125; &#125; 运行结果： 1234状态为：state1execute the first opt!状态为：state2execute the second opt! 根据这个特性，状态模式在日常开发中用的挺多的，尤其是做网站的时候，我们有时希望根据对象的某一属性，区别开他们的一些功能，比如说简单的权限控制等。 九、访问者模式访问者模式把数据结构和作用于结构上的操作解耦合，使得操作集合可相对自由地演化。访问者模式适用于数据结构相对稳定算法又易变化的系统。因为访问者模式使得算法操作增加变得容易。若系统数据结构对象易于变化，经常有新的数据对象增加进来，则不适合使用访问者模式。访问者模式的优点是增加操作很容易，因为增加操作意味着增加新的访问者。访问者模式将有关行为集中到一个访问者对象中，其改变不影响系统数据结构。其缺点就是增加新的数据结构很困难。 访问者模式算是最复杂也是最难以理解的一种模式了。它表示一个作用于某对象结构中的各元素的操作。它使你可以在不改变各元素类的前提下定义作用于这些元素的新操作。 涉及角色： 1.Visitor 抽象访问者角色，为该对象结构中具体元素角色声明一个访问操作接口。该操作接口的名字和参数标识了发送访问请求给具体访问者的具体元素角色，这样访问者就可以通过该元素角色的特定接口直接访问它。 2.ConcreteVisitor.具体访问者角色，实现Visitor声明的接口。 3.Element 定义一个接受访问操作(accept())，它以一个访问者(Visitor)作为参数。 4.ConcreteElement 具体元素，实现了抽象元素(Element)所定义的接受操作接口。 5.ObjectStructure 结构对象角色，这是使用访问者模式必备的角色。它具备以下特性：能枚举它的元素；可以提供一个高层接口以允许访问者访问它的元素；如有需要，可以设计成一个复合对象或者一个聚集（如一个列表或无序集合）。 12345abstract class Element&#123; public abstract void accept(IVisitor visitor); public abstract void doSomething();&#125; 12345678class ConcreteElement1 extends Element&#123; public void doSomething()&#123; System.out.println(&quot;这是元素1&quot;); &#125; public void accept(IVisitor visitor)&#123; visitor.visit(this); &#125;&#125; 12345678class ConcreteElement2 extends Element&#123; public void doSomething()&#123; System.out.println(&quot;这是元素2&quot;); &#125; public void accept(IVisitor visitor)&#123; visitor.visit(this); &#125;&#125; 1234interface IVisitor&#123; public void visit(ConcreteElement1 el1); public void visit(ConcreteElement2 el2);&#125; 12345678class Visitor implements IVisitor&#123; public void visit(ConcreteElement1 el1)&#123; el1.doSomething(); &#125; public void visit(ConcreteElement2 el2)&#123; el2.doSomething(); &#125;&#125; 123456789101112131415class ObjectStruture&#123; public static List&lt;Element&gt; getList()&#123; List&lt;Element&gt;list = new ArrayList&lt;Element&gt;(); Random ran = newRandom(); for(int i = 0 ; i &lt; 10 ; i ++)&#123; int a=ran.nextInt(100); if(a&gt;50)&#123; list.add(new ConcreteElement1()); &#125;else&#123; list.add(new ConcreteElement2()); &#125; &#125; return list; &#125;&#125; 12345678public class Client&#123; public static void main (String[]args)&#123; List&lt;Element&gt; list = ObjectStruture.getList(); for(Element e:list)&#123; e.accept(new Visitor()); &#125; &#125;&#125; 十、中介者模式中介者模式（Mediator）：用一个中介对象来封装一系列的对象交互。中介者使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。 举例：在一个公司里面，有很多部门、员工（我们统称他们互相为Colleague“同事”），为了完成一定的任务，“同事”之间肯定有许多需要互相配合、交流的过程。如果由各个“同事”频繁地到处去与自己有关的“同事”沟通，这样肯定会形成一个多对多的杂乱的联系网络而造成工作效率低下。 此时就需要一位专门的“中介者”给各个“同事”分配任务，以及统一跟进大家的进度并在“同事”之间实时地进行交互，保证“同事”之间必须的沟通交流。很明显我们知道此时的“中介者”担任了沟通“同事”彼此之间的重要角色了，“中介者”使得每个“同事”都变成一对一的联系方式，减轻了每个“同事”的负担，增强工作效率。 同事类族： 123456789101112131415161718package com.model.behaviour; public abstract class AbstractColleague &#123; protected AbstractMediator mediator; /**既然有中介者，那么每个具体同事必然要与中介者有联系， * 否则就没必要存在于 这个系统当中，这里的构造函数相当 * 于向该系统中注册一个中介者，以取得联系 */ public AbstractColleague(AbstractMediator mediator) &#123; this.mediator = mediator; &#125; // 在抽象同事类中添加用于与中介者取得联系（即注册）的方法 public void setMediator(AbstractMediator mediator) &#123; this.mediator = mediator; &#125; &#125; 123456789101112131415161718192021//具体同事A package com.model.behaviour; public class ColleagueA extends AbstractColleague &#123; //每个具体同事都通过父类构造函数与中介者取得联系 public ColleagueA(AbstractMediator mediator) &#123; super(mediator); &#125; //每个具体同事必然有自己分内的事，没必要与外界相关联 public void self() &#123; System.out.println(&quot;同事A --&gt; 做好自己分内的事情 ...&quot;); &#125; //每个具体同事总有需要与外界交互的操作，通过中介者来处理这些逻辑并安排工作 public void out() &#123; System.out.println(&quot;同事A --&gt; 请求同事B做好分内工作 ...&quot;); super.mediator.execute(&quot;ColleagueB&quot;, &quot;self&quot;); &#125; &#125; 123456789101112131415161718//具体同事B package com.model.behaviour; public class ColleagueB extends AbstractColleague &#123; public ColleagueB(AbstractMediator mediator) &#123; super(mediator); &#125; public void self() &#123; System.out.println(&quot;同事B --&gt; 做好自己分内的事情 ...&quot;); &#125; public void out() &#123; System.out.println(&quot;同事B --&gt; 请求同事A做好分内工作 ...&quot;); super.mediator.execute(&quot;ColleagueA&quot;, &quot;self&quot;); &#125; &#125; 中介者类族： 1234567891011121314151617181920package com.model.behaviour; public abstract class AbstractMediator &#123; //中介者肯定需要保持有若干同事的联系方式 protected Hashtable&lt;String, AbstractColleague&gt; colleagues = new Hashtable&lt;String, AbstractColleague&gt;(); //中介者可以动态地与某个同事建立联系 public void addColleague(String name, AbstractColleague c) &#123; this.colleagues.put(name, c); &#125; //中介者也可以动态地撤销与某个同事的联系 public void deleteColleague(String name) &#123; this.colleagues.remove(name); &#125; //中介者必须具备在同事之间处理逻辑、分配任务、促进交流的操作 public abstract void execute(String name, String method); &#125; 123456789101112131415161718192021222324252627//具体中介者 package com.model.behaviour; public class Mediator extends AbstractMediator&#123; //中介者最重要的功能，来回奔波与各个同事之间 public void execute(String name, String method) &#123; if(&quot;self&quot;.equals(method))&#123; //各自做好分内事 if(&quot;ColleagueA&quot;.equals(name)) &#123; ColleagueA colleague = (ColleagueA)super.colleagues.get(&quot;ColleagueA&quot;); colleague.self(); &#125;else &#123; ColleagueB colleague = (ColleagueB)super.colleagues.get(&quot;ColleagueB&quot;); colleague.self(); &#125; &#125;else &#123; //与其他同事合作 if(&quot;ColleagueA&quot;.equals(name)) &#123; ColleagueA colleague = (ColleagueA)super.colleagues.get(&quot;ColleagueA&quot;); colleague.out(); &#125;else &#123; ColleagueB colleague = (ColleagueB)super.colleagues.get(&quot;ColleagueB&quot;); colleague.out(); &#125; &#125; &#125; &#125; 测试类： 123456789101112131415161718192021222324252627//测试类 package com.model.behaviour; public class Client &#123; public static void main(String[] args) &#123; //创建一个中介者 AbstractMediator mediator = new Mediator(); //创建两个同事 ColleagueA colleagueA = new ColleagueA(mediator); ColleagueB colleagueB = new ColleagueB(mediator); //中介者分别与每个同事建立联系 mediator.addColleague(&quot;ColleagueA&quot;, colleagueA); mediator.addColleague(&quot;ColleagueB&quot;, colleagueB); //同事们开始工作 colleagueA.self(); colleagueA.out(); System.out.println(&quot;======================合作愉快，任务完成！\\n&quot;); colleagueB.self(); colleagueB.out(); System.out.println(&quot;======================合作愉快，任务完成！&quot;); &#125; &#125; 运行结果： 123456789同事A --&gt; 做好自己分内的事情 ... 同事A --&gt; 请求同事B做好分内工作 ... 同事B --&gt; 做好自己分内的事情 ... ======================合作愉快，任务完成！ 同事B --&gt; 做好自己分内的事情 ... 同事B --&gt; 请求同事A做好分内工作 ... 同事A --&gt; 做好自己分内的事情 ... ======================合作愉快，任务完成！ 十一、解释器模式解释器模式：给定一种语言，定义他的文法的一种表示，并定义一个解释器，该解释器使用该表示来解释语言中句子。 解释器模式是一个比较少用的模式。 1234567891011121314151617181920212223242526272829package com.model.behaviour; public class Context &#123; private int num1; private int num2; public Context(int num1, int num2) &#123; this.num1 = num1; this.num2 = num2; &#125; public int getNum1() &#123; return num1; &#125; public void setNum1(int num1) &#123; this.num1 = num1; &#125; public int getNum2() &#123; return num2; &#125; public void setNum2(int num2) &#123; this.num2 = num2; &#125; &#125; 12345package com.model.behaviour; public interface Expression &#123; public int interpret(Context context); &#125; 123456789package com.model.behaviour; public class Minus implements Expression &#123; @Override public int interpret(Context context) &#123; return context.getNum1()-context.getNum2(); &#125; &#125; 123456789package com.model.behaviour; public class Plus implements Expression &#123; @Override public int interpret(Context context) &#123; return context.getNum1()+context.getNum2(); &#125; &#125; 1234567891011package com.model.behaviour; public class Test &#123; public static void main(String[] args) &#123; // 计算9+2-8的值 int result = new Minus().interpret((new Context(new Plus() .interpret(new Context(9, 2)), 8))); System.out.println(result); &#125;&#125; 原文：http://blog.csdn.net/u013142781/article/details/50825301","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://blog.fenxiangz.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://blog.fenxiangz.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"行为型模式","slug":"行为型模式","permalink":"https://blog.fenxiangz.com/tags/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/"}]},{"title":"Java经典设计模式（2）：七大结构型模式","slug":"design/2019-01-04_Java经典设计模式_2_七大结构型模式","date":"2019-01-04T00:00:00.000Z","updated":"2020-12-20T16:47:02.949Z","comments":true,"path":"post/design/2019-01-04_Java经典设计模式_2_七大结构型模式.html","link":"","permalink":"https://blog.fenxiangz.com/post/design/2019-01-04_Java%E7%BB%8F%E5%85%B8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F_2_%E4%B8%83%E5%A4%A7%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F.html","excerpt":"","text":"接下来我们看看结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。其中适配器模式主要分为三类：类的适配器模式、对象的适配器模式、接口的适配器模式。其中的对象的适配器模式是各种结构型模式的起源。 一、适配器模式适配器模式主要分为三类：类的适配器模式、对象的适配器模式、接口的适配器模式。 适配器模式将某个类的接口转换成客户端期望的另一个接口表示，目的是消除由于接口不匹配所造成的类的兼容性问题。有点抽象，我们来看看详细的内容。 1.1、类的适配器模式类的适配器模式核心思想就是：有一个Source类，拥有一个方法，待适配，目标接口是Targetable，通过Adapter类，将Source的功能扩展到Targetable里。 1234567package com.model.structure; public class Source &#123; public void method1() &#123; System.out.println(&quot;this is original method!&quot;); &#125; &#125; 123456789package com.model.structure; public interface Targetable &#123; /* 与原类中的方法相同 */ public void method1(); /* 新类的方法 */ public void method2();&#125; 1234567package com.model.structure; public class Adapter extends Source implements Targetable &#123; public void method2() &#123; System.out.println(&quot;this is the targetable method!&quot;); &#125;&#125; 123456789package com.model.structure; public class AdapterTest &#123; public static void main(String[] args) &#123; Targetable target = new Adapter(); target.method1(); target.method2(); &#125;&#125; AdapterTest的运行结果： 12this is original method!this is the targetable method! 1.2、对象的适配器模式对象的适配器模式的基本思路和类的适配器模式相同，只是将Adapter类作修改成Wrapper，这次不继承Source类，而是持有Source类的实例，以达到解决兼容性的问题。 123456789101112131415161718192021package com.model.structure; public class Wrapper implements Targetable &#123; private Source source; public Wrapper(Source source) &#123; super(); this.source = source; &#125; @Override public void method2() &#123; System.out.println(&quot;this is the targetable method!&quot;); &#125; @Override public void method1() &#123; source.method1(); &#125;&#125; 12345678910package com.model.structure; public class AdapterTest &#123; public static void main(String[] args) &#123; Source source = new Source(); Targetable target = new Wrapper(source); target.method1(); target.method2(); &#125;&#125; 运行结果跟类的适配器模式例子的一样。 1.3、接口的适配器模式接口的适配器是这样的：有时我们写的一个接口中有多个抽象方法，当我们写该接口的实现类时，必须实现该接口的所有方法，这明显有时比较浪费，因为并不是所有的方法都是我们需要的，有时只需要某一些，此处为了解决这个问题，我们引入了接口的适配器模式，借助于一个抽象类，该抽象类实现了该接口，实现了所有的方法，而我们不和原始的接口打交道，只和该抽象类取得联系，所以我们写一个类，继承该抽象类，重写我们需要的方法就行了。 这里看文字描述已经试够清楚的了，因此就不贴代码实例了。 二、装饰模式装饰模式：在不必改变原类文件和使用继承的情况下，动态地扩展一个对象的功能。它是通过创建一个包装对象，也就是装饰来包裹真实的对象。 装饰模式的特点： （1） 装饰对象和真实对象有相同的接口。这样客户端对象就能以和真实对象相同的方式和装饰对象交互。（2） 装饰对象包含一个真实对象的引用（reference）（3） 装饰对象接受所有来自客户端的请求。它把这些请求转发给真实的对象。（4） 装饰对象可以在转发这些请求以前或以后增加一些附加功能。这样就确保了在运行时，不用修改给定对象的结构就可以在外部增加附加的功能。在面向对象的设计中，通常是通过继承来实现对给定类的功能扩展。继承不能做到这一点，继承的功能是静态的，不能动态增删。 具体看看代码实例 12345package com.model.structure; public interface Sourceable &#123; public void method();&#125; 123456789package com.model.structure; public class Source implements Sourceable &#123; @Override public void method() &#123; System.out.println(&quot;the original method!&quot;); &#125;&#125; 123456789101112131415161718package com.model.structure; public class Decorator implements Sourceable &#123; private Sourceable source; public Decorator(Sourceable source) &#123; super(); this.source = source; &#125; @Override public void method() &#123; System.out.println(&quot;before decorator!&quot;); source.method(); System.out.println(&quot;after decorator!&quot;); &#125;&#125; 12345678910111213141516package com.model.structure; public class DecoratorTest &#123; public static void main(String[] args) &#123; //（1） 装饰对象和真实对象有相同的接口。这样客户端对象就能以和真实对象相同的方式和装饰对象交互。 //（2） 装饰对象包含一个真实对象的引用（reference） //（3） 装饰对象接受所有来自客户端的请求。它把这些请求转发给真实的对象。 //（4） 装饰对象可以在转发这些请求以前或以后增加一些附加功能。这样就确保了在运行时，不用修改给定对象的结构就可以在外部增加附加的功能。 // 在面向对象的设计中，通常是通过继承来实现对给定类的功能扩展。 // 继承不能做到这一点，继承的功能是静态的，不能动态增删。 Sourceable source = new Source(); Sourceable obj = new Decorator(source); obj.method(); &#125;&#125; 运行结果： 123before decorator!the original method!after decorator! 三、代理模式代理模式就是多一个代理类出来，替原对象进行一些操作。代理类就像中介，它比我们掌握着更多的信息。 具体看看代码实例。 12345package com.model.structure; public interface Sourceable &#123; public void method();&#125; 123456789package com.model.structure; public class Source implements Sourceable &#123; @Override public void method() &#123; System.out.println(&quot;the original method!&quot;); &#125;&#125; 1234567891011121314151617181920212223242526package com.model.structure; public class Proxy implements Sourceable &#123; private Source source; public Proxy() &#123; super(); this.source = new Source(); &#125; @Override public void method() &#123; before(); source.method(); atfer(); &#125; private void atfer() &#123; System.out.println(&quot;after proxy!&quot;); &#125; private void before() &#123; System.out.println(&quot;before proxy!&quot;); &#125;&#125; 123456789package com.model.structure; public class ProxyTest &#123; public static void main(String[] args) &#123; Sourceable source = new Proxy(); source.method(); &#125;&#125; 运行结果： 123before proxy!the original method!after proxy! 补充：（代理模式看起来很像装饰器模式。 对装饰器模式来说，装饰者（decorator）和被装饰者（decoratee）都实现同一个 接口。对代理模式来说，代理类（proxy class）和真实处理的类（real class）都实现同一个接口。 此外，不论我们使用哪一个模式，都可以很容易地在真实对象的方法前面或者后面加上自定义的方法。 然而，实际上，在装饰器模式和代理模式之间还是有很多差别的。装饰器模式关注于在一个对象上动态的添加方法，然而代理模式关注于控制对对象的访问。换句话 说，用代理模式，代理类（proxy class）可以对它的客户隐藏一个对象的具体信息。因此，当使用代理模式的时候，我们常常在一个代理类中创建一个对象的实例。并且，当我们使用装饰器模 式的时候，我们通常的做法是将原始对象作为一个参数传给装饰者的构造器。 我们可以用另外一句话来总结这些差别：使用代理模式，代理和真实对象之间的的关系通常在编译时就已经确定了，而装饰者能够在运行时递归地被构造。 ） 四、外观模式外观模式是为了解决类与类之间的依赖关系的，像spring一样，可以将类和类之间的关系配置到配置文件中，而外观模式就是将他们的关系放在一个Facade类中，降低了类类之间的耦合度，该模式中没有涉及到接口。 我们以一个计算机的启动过程为例，看看如下的代码： 123456789101112package com.model.structure; public class CPU &#123; public void startup() &#123; System.out.println(&quot;cpu startup!&quot;); &#125; public void shutdown() &#123; System.out.println(&quot;cpu shutdown!&quot;); &#125;&#125; 123456789101112package com.model.structure; public class Disk &#123; public void startup() &#123; System.out.println(&quot;disk startup!&quot;); &#125; public void shutdown() &#123; System.out.println(&quot;disk shutdown!&quot;); &#125;&#125; 123456789101112package com.model.structure; public class Memory &#123; public void startup() &#123; System.out.println(&quot;memory startup!&quot;); &#125; public void shutdown() &#123; System.out.println(&quot;memory shutdown!&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930package com.model.structure; public class Computer &#123; private CPU cpu; private Memory memory; private Disk disk; public Computer() &#123; cpu = new CPU(); memory = new Memory(); disk = new Disk(); &#125; public void startup() &#123; System.out.println(&quot;start the computer!&quot;); cpu.startup(); memory.startup(); disk.startup(); System.out.println(&quot;start computer finished!&quot;); &#125; public void shutdown() &#123; System.out.println(&quot;begin to close the computer!&quot;); cpu.shutdown(); memory.shutdown(); disk.shutdown(); System.out.println(&quot;computer closed!&quot;); &#125;&#125; 12345678910package com.model.structure; public class User &#123; public static void main(String[] args) &#123; Computer computer = new Computer(); computer.startup(); computer.shutdown(); &#125;&#125; 运行结果： 12345678910start the computer!cpu startup!memory startup!disk startup!start computer finished!begin to close the computer!cpu shutdown!memory shutdown!disk shutdown!computer closed! 五、桥接模式在软件系统中，某些类型由于自身的逻辑，它具有两个或多个维度的变化，那么如何应对这种“多维度的变化”？如何利用面向对象的技术来使得该类型能够轻松的沿着多个方向进行变化，而又不引入额外的复杂度？这就要使用Bridge模式。 在提出桥梁模式的时候指出，桥梁模式的用意是”将抽象化(Abstraction)与实现化(Implementation)脱耦，使得二者可以独立地变化”。这句话有三个关键词，也就是抽象化、实现化和脱耦。 抽象化：存在于多个实体中的共同的概念性联系，就是抽象化。作为一个过程，抽象化就是忽略一些信息，从而把不同的实体当做同样的实体对待。实现化：抽象化给出的具体实现，就是实现化。脱耦：所谓耦合，就是两个实体的行为的某种强关联。而将它们的强关联去掉，就是耦合的解脱，或称脱耦。在这里，脱耦是指将抽象化和实现化之间的耦合解脱开，或者说是将它们之间的强关联改换成弱关联。 下面我们来看看代码实例： 12345package com.model.structure; public interface Driver &#123; public void connect(); &#125; 123456789package com.model.structure; public class MysqlDriver implements Driver &#123; @Override public void connect() &#123; System.out.println(&quot;connect mysql done!&quot;); &#125;&#125; 123456789package com.model.structure; public class DB2Driver implements Driver &#123; @Override public void connect() &#123; System.out.println(&quot;connect db2 done!&quot;); &#125;&#125; 123456789101112131415161718package com.model.structure; public abstract class DriverManager &#123; private Driver driver; public void connect() &#123; driver.connect(); &#125; public Driver getDriver() &#123; return driver; &#125; public void setDriver(Driver driver) &#123; this.driver = driver; &#125;&#125; 12345678package com.model.structure; public class MyDriverManager extends DriverManager &#123; public void connect() &#123; super.connect(); &#125;&#125; 12345678910111213141516package com.model.structure; public class Client &#123; public static void main(String[] args) &#123; DriverManager driverManager = new MyDriverManager(); Driver driver1 = new MysqlDriver(); driverManager.setDriver(driver1); driverManager.connect(); Driver driver2 = new DB2Driver(); driverManager.setDriver(driver2); driverManager.connect(); &#125;&#125; 执行结果： 12connect mysql done!connect db2 done! 如果看完代码实例还不是很理解，我们想想如下两个维度扩展：（1）假设我想加一个OracleDriver，这是一个维度，很好理解，不多解释。（2）假设我们想在连接前后固定输出点什么，我们只需要加一个MyDriverManager2，代码如下： 1234567891011package com.model.structure; public class MyDriverManager2 extends DriverManager &#123; public void connect() &#123; System.out.println(&quot;before connect&quot;); super.connect(); System.out.println(&quot;after connect&quot;); &#125; &#125; 再将Client代码中的MyDriverManager 改成 MyDriverManager2 ，执行结果如下： 123456before connectconnect mysql done!after connectbefore connectconnect db2 done!after connect 六、组合模式组合模式，将对象组合成树形结构以表示“部分-整体”的层次结构，组合模式使得用户对单个对象和组合对象的使用具有一致性。掌握组合模式的重点是要理解清楚 “部分/整体” 还有 ”单个对象“ 与 “组合对象” 的含义。 组合模式让你可以优化处理递归或分级数据结构。 《设计模式》：将对象组合成树形结构以表示“部分整体”的层次结构。组合模式使得用户对单个对象和组合对象的使用具有一致性。 涉及角色： Component：是组合中的对象声明接口，在适当的情况下，实现所有类共有接口的默认行为。声明一个接口用于访问和管理Component子部件。 Leaf：在组合中表示叶子结点对象，叶子结点没有子结点。 Composite：定义有枝节点行为，用来存储子部件，在Component接口中实现与子部件有关操作，如增加(add)和删除(remove)等。 比如现实中公司内各部门的层级关系，请看代码： Component：是组合中的对象声明接口，在适当的情况下，实现所有类共有接口的默认行为。声明一个接口用于访问和管理Component子部件。 1234567891011121314151617181920212223242526272829package com.model.structure; public abstract class Company &#123; private String name; public Company() &#123; &#125; public Company(String name) &#123; super(); this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; protected abstract void add(Company company); protected abstract void romove(Company company); protected abstract void display(int depth); &#125; Composite：定义有枝节点行为，用来存储子部件，在Component接口中实现与子部件有关操作，如增加(add)和删除(remove)等。 1234567891011121314151617181920212223242526272829303132333435363738394041package com.model.structure; import java.util.ArrayList;import java.util.List; public class ConcreteCompany extends Company &#123; private List&lt;Company&gt; cList; public ConcreteCompany() &#123; cList = new ArrayList(); &#125; public ConcreteCompany(String name) &#123; super(name); cList = new ArrayList(); &#125; @Override protected void add(Company company) &#123; cList.add(company); &#125; @Override protected void display(int depth) &#123; StringBuilder sb = new StringBuilder(&quot;&quot;); for (int i = 0; i &lt; depth; i++) &#123; sb.append(&quot;-&quot;); &#125; System.out.println(new String(sb) + this.getName()); for (Company c : cList) &#123; c.display(depth + 2); &#125; &#125; @Override protected void romove(Company company) &#123; cList.remove(company); &#125;&#125; Leaf：在组合中表示叶子结点对象，叶子结点没有子结点。 123456789101112131415161718192021222324package com.model.structure; public class HRDepartment extends Company &#123; public HRDepartment(String name) &#123; super(name); &#125; @Override protected void add(Company company) &#123; &#125; @Override protected void display(int depth) &#123; StringBuilder sb = new StringBuilder(&quot;&quot;); for (int i = 0; i &lt; depth; i++) &#123; sb.append(&quot;-&quot;); &#125; System.out.println(new String(sb) + this.getName()); &#125; @Override protected void romove(Company company) &#123; &#125;&#125; 123456789101112131415161718192021222324package com.model.structure; public class FinanceDepartment extends Company &#123; public FinanceDepartment(String name) &#123; super(name); &#125; @Override protected void add(Company company) &#123; &#125; @Override protected void display(int depth) &#123; StringBuilder sb = new StringBuilder(&quot;&quot;); for (int i = 0; i &lt; depth; i++) &#123; sb.append(&quot;-&quot;); &#125; System.out.println(new String(sb) + this.getName()); &#125; @Override protected void romove(Company company) &#123; &#125;&#125; Client： 1234567891011121314151617181920212223242526272829303132333435package com.model.structure; public class Client &#123; public static void main(String[] args) &#123; Company root = new ConcreteCompany(); root.setName(&quot;北京总公司&quot;); root.add(new HRDepartment(&quot;总公司人力资源部&quot;)); root.add(new FinanceDepartment(&quot;总公司财务部&quot;)); Company shandongCom = new ConcreteCompany(&quot;山东分公司&quot;); shandongCom.add(new HRDepartment(&quot;山东分公司人力资源部&quot;)); shandongCom.add(new FinanceDepartment(&quot;山东分公司账务部&quot;)); Company zaozhuangCom = new ConcreteCompany(&quot;枣庄办事处&quot;); zaozhuangCom.add(new FinanceDepartment(&quot;枣庄办事处财务部&quot;)); zaozhuangCom.add(new HRDepartment(&quot;枣庄办事处人力资源部&quot;)); Company jinanCom = new ConcreteCompany(&quot;济南办事处&quot;); jinanCom.add(new FinanceDepartment(&quot;济南办事处财务部&quot;)); jinanCom.add(new HRDepartment(&quot;济南办事处人力资源部&quot;)); shandongCom.add(jinanCom); shandongCom.add(zaozhuangCom); Company huadongCom = new ConcreteCompany(&quot;上海华东分公司&quot;); huadongCom.add(new HRDepartment(&quot;上海华东分公司人力资源部&quot;)); huadongCom.add(new FinanceDepartment(&quot;上海华东分公司账务部&quot;)); Company hangzhouCom = new ConcreteCompany(&quot;杭州办事处&quot;); hangzhouCom.add(new FinanceDepartment(&quot;杭州办事处财务部&quot;)); hangzhouCom.add(new HRDepartment(&quot;杭州办事处人力资源部&quot;)); Company nanjingCom = new ConcreteCompany(&quot;南京办事处&quot;); nanjingCom.add(new FinanceDepartment(&quot;南京办事处财务部&quot;)); nanjingCom.add(new HRDepartment(&quot;南京办事处人力资源部&quot;)); huadongCom.add(hangzhouCom); huadongCom.add(nanjingCom); root.add(shandongCom); root.add(huadongCom); root.display(0); &#125;&#125; 运行结果： 12345678910111213141516171819202122北京总公司--总公司人力资源部--总公司财务部--山东分公司----山东分公司人力资源部----山东分公司账务部----济南办事处------济南办事处财务部------济南办事处人力资源部----枣庄办事处------枣庄办事处财务部------枣庄办事处人力资源部--上海华东分公司----上海华东分公司人力资源部----上海华东分公司账务部----杭州办事处------杭州办事处财务部------杭州办事处人力资源部----南京办事处------南京办事处财务部------南京办事处人力资源部 七、享元模式享元模式的主要目的是实现对象的共享，即共享池，当系统中对象多的时候可以减少内存的开销，通常与工厂模式一起使用。 一提到共享池，我们很容易联想到Java里面的JDBC连接池，想想每个连接的特点，我们不难总结出：适用于作共享的一些个对象，他们有一些共有的属性，就拿数据库连接池来说，url、driverClassName、username、password及dbname，这些属性对于每个连接来说都是一样的，所以就适合用享元模式来处理，建一个工厂类，将上述类似属性作为内部数据，其它的作为外部数据，在方法调用时，当做参数传进来，这样就节省了空间，减少了实例的数量。 看下数据库连接池的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.model.structure; import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;import java.util.Vector; public class ConnectionPool &#123; private Vector&lt;Connection&gt; pool; /* 公有属性 */ private String url = &quot;jdbc:mysql://localhost:3306/test&quot;; private String username = &quot;root&quot;; private String password = &quot;root&quot;; private String driverClassName = &quot;com.mysql.jdbc.Driver&quot;; private int poolSize = 100; private static ConnectionPool instance = null; Connection conn = null; /* 构造方法，做一些初始化工作 */ private ConnectionPool() &#123; pool = new Vector&lt;Connection&gt;(poolSize); for (int i = 0; i &lt; poolSize; i++) &#123; try &#123; Class.forName(driverClassName); conn = DriverManager.getConnection(url, username, password); pool.add(conn); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /* 返回连接到连接池 */ public synchronized void release() &#123; pool.add(conn); &#125; /* 返回连接池中的一个数据库连接 */ public synchronized Connection getConnection() &#123; if (pool.size() &gt; 0) &#123; Connection conn = pool.get(0); pool.remove(conn); return conn; &#125; else &#123; return null; &#125; &#125;&#125; 通过连接池的管理，实现了数据库连接的共享，不需要每一次都重新创建连接，节省了数据库重新创建的开销，提升了系统的性能！ 原文：http://blog.csdn.net/u013142781/article/details/50821155","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://blog.fenxiangz.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://blog.fenxiangz.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"结构型模式","slug":"结构型模式","permalink":"https://blog.fenxiangz.com/tags/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F/"}]},{"title":"Java 经典设计模式（1）：五大创建型模式","slug":"design/2019-01-03_Java经典设计模式_1_五大创建型模式","date":"2019-01-03T00:00:00.000Z","updated":"2020-12-20T16:47:02.949Z","comments":true,"path":"post/design/2019-01-03_Java经典设计模式_1_五大创建型模式.html","link":"","permalink":"https://blog.fenxiangz.com/post/design/2019-01-03_Java%E7%BB%8F%E5%85%B8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F_1_%E4%BA%94%E5%A4%A7%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F.html","excerpt":"","text":"一、概况总体来说设计模式分为三大类：（1）创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 （2）结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 （3）行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 二、设计模式的六大原则1、开闭原则（Open Close Principle）开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。 2、里氏代换原则（Liskov Substitution Principle）其官方描述比较抽象，可自行百度。实际上可以这样理解：（1）子类的能力必须大于等于父类，即父类可以使用的方法，子类都可以使用。（2）返回值也是同样的道理。假设一个父类方法返回一个List，子类返回一个ArrayList，这当然可以。如果父类方法返回一个ArrayList，子类返回一个List，就说不通了。这里子类返回值的能力是比父类小的。（3）还有抛出异常的情况。任何子类方法可以声明抛出父类方法声明异常的子类。 而不能声明抛出父类没有声明的异常。 3、依赖倒转原则（Dependence Inversion Principle）这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。 4、接口隔离原则（Interface Segregation Principle）这个原则的意思是：使用多个隔离的接口，比使用单个接口要好。还是一个降低类之间的耦合度的意思，从这儿我们看出，其实设计模式就是一个软件的设计思想，从大型软件架构出发，为了升级和维护方便。所以上文中多次出现：降低依赖，降低耦合。 5、迪米特法则（最少知道原则）（Demeter Principle）为什么叫最少知道原则，就是说：一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。 6、合成复用原则（Composite Reuse Principle）原则是尽量使用合成/聚合的方式，而不是使用继承。 三、创建型模式创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 3.1、工厂方法模式工厂方法模式分为三种：普通工厂模式、多个工厂方法模式和静态工厂方法模式。 3.1.1、普通工厂模式普通工厂模式就是建立一个工厂类，对实现了同一接口的一些类进行实例的创建。 12345package com.mode.create; public interface MyInterface &#123; public void print();&#125; 12345678package com.mode.create; public class MyClassOne implements MyInterface &#123; @Override public void print() &#123; System.out.println(&quot;MyClassOne&quot;); &#125;&#125; 12345678package com.mode.create; public class MyClassTwo implements MyInterface &#123; @Override public void print() &#123; System.out.println(&quot;MyClassTwo&quot;); &#125;&#125; 1234567891011121314package com.mode.create; public class MyFactory &#123; public MyInterface produce(String type) &#123; if (&quot;One&quot;.equals(type)) &#123; return new MyClassOne(); &#125; else if (&quot;Two&quot;.equals(type)) &#123; return new MyClassTwo(); &#125; else &#123; System.out.println(&quot;没有要找的类型&quot;); return null; &#125; &#125;&#125; 123456789package com.mode.create; public class FactoryTest &#123; public static void main(String[] args)&#123; MyFactory factory = new MyFactory(); MyInterface myi = factory.produce(&quot;One&quot;); myi.print(); &#125;&#125; 再回头来理解这句话：普通工厂模式就是建立一个工厂类，对实现了同一接口的一些类进行实例的创建。 3.1.2、多个工厂方法模式多个工厂方法模式，是对普通工厂方法模式的改进，多个工厂方法模式就是提供多个工厂方法，分别创建对象。 直接看代码吧，我们修改MyFactory和FactoryTest如下： 1234567891011package com.mode.create; public class MyFactory &#123; public MyInterface produceOne() &#123; return new MyClassOne(); &#125; public MyInterface produceTwo() &#123; return new MyClassTwo(); &#125;&#125; 123456789package com.mode.create; public class FactoryTest &#123; public static void main(String[] args)&#123; MyFactory factory = new MyFactory(); MyInterface myi = factory.produceOne(); myi.print(); &#125;&#125; 再回头来理解这句话：多个工厂方法模式，是对普通工厂方法模式的改进，多个工厂方法模式就是提供多个工厂方法，分别创建对象。 3.1.3、静态工厂方法模式静态工厂方法模式，将上面的多个工厂方法模式里的方法置为静态的，不需要创建实例，直接调用即可。 直接看代码吧，我们修改MyFactory和FactoryTest如下： 1234567891011package com.mode.create; public class MyFactory &#123; public static MyInterface produceOne() &#123; return new MyClassOne(); &#125; public static MyInterface produceTwo() &#123; return new MyClassTwo(); &#125;&#125; 12345678package com.mode.create; public class FactoryTest &#123; public static void main(String[] args)&#123; MyInterface myi = MyFactory.produceOne(); myi.print(); &#125;&#125; 再回顾：静态工厂方法模式，将上面的多个工厂方法模式里的方法置为静态的，不需要创建实例，直接调用即可。 3.2、抽象工厂模式工厂方法模式有一个问题就是，类的创建依赖工厂类，也就是说，如果想要拓展程序，必须对工厂类进行修改，这违背了闭包原则。 为解决这个问题，我们来看看抽象工厂模式：创建多个工厂类，这样一旦需要增加新的功能，直接增加新的工厂类就可以了，不需要修改之前的代码。 这样就符合闭包原则了。 下面来看看代码： MyInterface、MyClassOne、MyClassTwo不变。 新增如下接口和类： 12345package com.mode.create; public interface Provider &#123; public MyInterface produce(); &#125; 12345678package com.mode.create; public class MyFactoryOne implements Provider &#123; @Override public MyInterface produce() &#123; return new MyClassOne(); &#125;&#125; 12345678package com.mode.create; public class MyFactoryTwo implements Provider &#123; @Override public MyInterface produce() &#123; return new MyClassTwo(); &#125;&#125; 修改测试类FactoryTest如下： 12345678package com.mode.create; public class FactoryTest &#123; public static void main(String[] args)&#123; Provider provider = new MyFactoryOne(); MyInterface myi = provider.produce(); myi.print();&#125; 再回顾：抽象工厂模式就是创建多个工厂类，这样一旦需要增加新的功能，直接增加新的工厂类就可以了，不需要修改之前的代码。 3.3、单例模式单例模式，不需要过多的解释。 直接看代码吧： 12345678910111213141516package test; public class MyObject &#123; private static MyObject myObject; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; if (myObject != null) &#123; &#125; else &#123; myObject = new MyObject(); &#125; return myObject; &#125;&#125; 但是这样会引发多线程问题，详细解说可以看《Java多线程编程核心技术》书中的第六章。博主之前推荐过这本书，里面有电子完整版下载地址：http://blog.csdn.net/u013142781/article/details/50805655 3.4、建造者模式建造者模式：是将一个复杂的对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。 字面看来非常抽象，实际上它也十分抽象！！！！ 建造者模式通常包括下面几个角色： （1）Builder：给出一个抽象接口，以规范产品对象的各个组成成分的建造。这个接口规定要实现复杂对象的哪些部分的创建，并不涉及具体的对象部件的创建。 （2）ConcreteBuilder：实现Builder接口，针对不同的商业逻辑，具体化复杂对象的各部分的创建。 在建造过程完成后，提供产品的实例。 （3）Director：调用具体建造者来创建复杂对象的各个部分，在指导者中不涉及具体产品的信息，只负责保证对象各部分完整创建或按某种顺序创建。 （4）Product：要创建的复杂对象。 在游戏开发中建造小人是经常的事了，要求是：小人必须包括头，身体和脚。 下面我们看看如下代码： Product（要创建的复杂对象。）： 1234567891011121314151617181920212223242526272829303132package com.mode.create; public class Person &#123; private String head; private String body; private String foot; public String getHead() &#123; return head; &#125; public void setHead(String head) &#123; this.head = head; &#125; public String getBody() &#123; return body; &#125; public void setBody(String body) &#123; this.body = body; &#125; public String getFoot() &#123; return foot; &#125; public void setFoot(String foot) &#123; this.foot = foot; &#125;&#125; Builder（给出一个抽象接口，以规范产品对象的各个组成成分的建造。这个接口规定要实现复杂对象的哪些部分的创建，并不涉及具体的对象部件的创建。）： 12345678package com.mode.create; public interface PersonBuilder &#123; void buildHead(); void buildBody(); void buildFoot(); Person buildPerson();&#125; ConcreteBuilder（实现Builder接口，针对不同的商业逻辑，具体化复杂对象的各部分的创建。 在建造过程完成后，提供产品的实例。）： 12345678910111213141516171819202122232425package com.mode.create; public class ManBuilder implements PersonBuilder &#123; Person person; public ManBuilder() &#123; person = new Person(); &#125; public void buildBody() &#123; person.setBody(&quot;建造男人的身体&quot;); &#125; public void buildFoot() &#123; person.setFoot(&quot;建造男人的脚&quot;); &#125; public void buildHead() &#123; person.setHead(&quot;建造男人的头&quot;); &#125; public Person buildPerson() &#123; return person; &#125;&#125; Director（调用具体建造者来创建复杂对象的各个部分，在指导者中不涉及具体产品的信息，只负责保证对象各部分完整创建或按某种顺序创建。）： 12345678910package com.mode.create; public class PersonDirector &#123; public Person constructPerson(PersonBuilder pb) &#123; pb.buildHead(); pb.buildBody(); pb.buildFoot(); return pb.buildPerson(); &#125;&#125; 测试类： 1234567891011package com.mode.create; public class Test &#123; public static void main(String[] args) &#123; PersonDirector pd = new PersonDirector(); Person person = pd.constructPerson(new ManBuilder()); System.out.println(person.getBody()); System.out.println(person.getFoot()); System.out.println(person.getHead()); &#125;&#125; 回顾：建造者模式：是将一个复杂的对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。 3.5、原型模式该模式的思想就是将一个对象作为原型，对其进行复制、克隆，产生一个和原对象类似的新对象。 说道复制对象，我将结合对象的浅复制和深复制来说一下，首先需要了解对象深、浅复制的概念： 浅复制：将一个对象复制后，基本数据类型的变量都会重新创建，而引用类型，指向的还是原对象所指向的。 深复制：将一个对象复制后，不论是基本数据类型还有引用类型，都是重新创建的。简单来说，就是深复制进行了完全彻底的复制，而浅复制不彻底。 写一个深浅复制的例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.mode.create; import java.io.ByteArrayInputStream;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.io.Serializable; public class Prototype implements Cloneable, Serializable &#123; private static final long serialVersionUID = 1L; private int base; private Integer obj; /* 浅复制 */ public Object clone() throws CloneNotSupportedException &#123; // 因为Cloneable接口是个空接口，你可以任意定义实现类的方法名 // 如cloneA或者cloneB，因为此处的重点是super.clone()这句话 // super.clone()调用的是Object的clone()方法 // 而在Object类中，clone()是native（本地方法）的 Prototype proto = (Prototype) super.clone(); return proto; &#125; /* 深复制 */ public Object deepClone() throws IOException, ClassNotFoundException &#123; /* 写入当前对象的二进制流 */ ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(this); /* 读出二进制流产生的新对象 */ ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bis); return ois.readObject(); &#125; public int getBase() &#123; return base; &#125; public void setBase(int base) &#123; this.base = base; &#125; public Integer getObj() &#123; return obj; &#125; public void setObj(Integer obj) &#123; this.obj = obj; &#125;&#125; 测试类： 123456789101112131415161718package com.mode.create; import java.io.IOException; public class Test &#123; public static void main(String[] args) throws CloneNotSupportedException, ClassNotFoundException, IOException &#123; Prototype prototype = new Prototype(); prototype.setBase(1); prototype.setObj(new Integer(2)); /* 浅复制 */ Prototype prototype1 = (Prototype) prototype.clone(); /* 深复制 */ Prototype prototype2 = (Prototype) prototype.deepClone(); System.out.println(prototype1.getObj()==prototype1.getObj()); System.out.println(prototype1.getObj()==prototype2.getObj()); &#125;&#125; 原文：http://blog.csdn.net/u013142781/article/details/50816245","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://blog.fenxiangz.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"创建型模式","slug":"创建型模式","permalink":"https://blog.fenxiangz.com/tags/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"设计模式","slug":"设计模式","permalink":"https://blog.fenxiangz.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"MySQL的InnoDB的幻读问题","slug":"dateabase/mysql/2018-12-17_MySQL的InnoDB的幻读问题","date":"2018-12-17T00:00:00.000Z","updated":"2020-12-20T16:47:02.946Z","comments":true,"path":"post/dateabase/mysql/2018-12-17_MySQL的InnoDB的幻读问题.html","link":"","permalink":"https://blog.fenxiangz.com/post/dateabase/mysql/2018-12-17_MySQL%E7%9A%84InnoDB%E7%9A%84%E5%B9%BB%E8%AF%BB%E9%97%AE%E9%A2%98.html","excerpt":"","text":"MySQL InnoDB事务的隔离级别有四级，默认是“可重复读”（REPEATABLE READ）。 未提交读（READ UNCOMMITTED）。另一个事务修改了数据，但尚未提交，而本事务中的SELECT会读到这些未被提交的数据（脏读）。 提交读（READ COMMITTED）。本事务读取到的是最新的数据（其他事务提交后的）。问题是，在同一个事务里，前后两次相同的SELECT会读到不同的结果（不重复读）。 可重复读（REPEATABLE READ）。在同一个事务里，SELECT的结果是事务开始时时间点的状态，因此，同样的SELECT操作读到的结果会是一致的。但是，会有幻读现象（稍后解释）。 串行化（SERIALIZABLE）。读操作会隐式获取共享锁，可以保证不同事务间的互斥。 四个级别逐渐增强，每个级别解决一个问题。 脏读，最容易理解。另一个事务修改了数据，但尚未提交，而本事务中的SELECT会读到这些未被提交的数据。 不重复读。解决了脏读后，会遇到，同一个事务执行过程中，另外一个事务提交了新数据，因此本事务先后两次读到的数据结果会不一致。 幻读。解决了不重复读，保证了同一个事务里，查询的结果都是事务开始时的状态（一致性）。但是，如果另一个事务同时提交了新数据，本事务再更新时，就会“惊奇的”发现了这些新数据，貌似之前读到的数据是“鬼影”一样的幻觉。 借鉴并改造了一个搞笑的比喻： 脏读。假如，中午去食堂打饭吃，看到一个座位被同学小Q占上了，就认为这个座位被占去了，就转身去找其他的座位。不料，这个同学小Q起身走了。事实：该同学小Q只是临时坐了一小下，并未“提交”。 不重复读。假如，中午去食堂打饭吃，看到一个座位是空的，便屁颠屁颠的去打饭，回来后却发现这个座位却被同学小Q占去了。 幻读。假如，中午去食堂打饭吃，看到一个座位是空的，便屁颠屁颠的去打饭，回来后，发现这些座位都还是空的（重复读），窃喜。走到跟前刚准备坐下时，却惊现一个恐龙妹，严重影响食欲。仿佛之前看到的空座位是“幻影”一样。 一些文章写到InnoDB的可重复读避免了“幻读”（phantom read），这个说法并不准确。 做个试验：(以下所有试验要注意存储引擎和隔离级别) mysql&gt; show create table t_bitfly\\G;CREATE TABLE t_bitfly (id bigint(20) NOT NULL default ‘0’,value varchar(32) default NULL,PRIMARY KEY (id)) ENGINE=InnoDB DEFAULT CHARSET=gbk mysql&gt; select @@global.tx_isolation, @@tx_isolation;+———————–+—————–+| @@global.tx_isolation | @@tx_isolation |+———————–+—————–+| REPEATABLE-READ | REPEATABLE-READ |+———————–+—————–+ 试验一： t Session A Session B|| START TRANSACTION; START TRANSACTION;|| SELECT * FROM t_bitfly;| empty set| INSERT INTO t_bitfly| VALUES (1, ‘a’);|| SELECT * FROM t_bitfly;| empty set| COMMIT;|| SELECT * FROM t_bitfly;| empty set|| INSERT INTO t_bitfly VALUES (1, ‘a’);| ERROR 1062 (23000):| Duplicate entry ‘1’ for key 1v (shit, 刚刚明明告诉我没有这条记录的) 如此就出现了幻读，以为表里没有数据，其实数据已经存在了，傻乎乎的提交后，才发现数据冲突了。 试验二： t Session A Session B|| START TRANSACTION; START TRANSACTION;|| SELECT * FROM t_bitfly;| +——+——-+| | id | value || +——+——-+| | 1 | a || +——+——-+| INSERT INTO t_bitfly| VALUES (2, ‘b’);|| SELECT * FROM t_bitfly;| +——+——-+| | id | value || +——+——-+| | 1 | a || +——+——-+| COMMIT;|| SELECT * FROM t_bitfly;| +——+——-+| | id | value || +——+——-+| | 1 | a || +——+——-+|| UPDATE t_bitfly SET value=’z’;| Rows matched: 2 Changed: 2 Warnings: 0| (怎么多出来一行)|| SELECT * FROM t_bitfly;| +——+——-+| | id | value || +——+——-+| | 1 | z || | 2 | z || +——+——-+|v 本事务中第一次读取出一行，做了一次更新后，另一个事务里提交的数据就出现了。也可以看做是一种幻读。 那么，InnoDB指出的可以避免幻读是怎么回事呢？ http://dev.mysql.com/doc/refman/5.0/en/innodb-record-level-locks.html By default, InnoDB operates in REPEATABLE READ transaction isolation level and with the innodb_locks_unsafe_for_binlog system variable disabled. In this case, InnoDB uses next-key locks for searches and index scans, which prevents phantom rows (see Section 13.6.8.5, “Avoiding the Phantom Problem Using Next-Key Locking”). 准备的理解是，当隔离级别是可重复读，且禁用innodb_locks_unsafe_for_binlog的情况下，在搜索和扫描index的时候使用的next-key locks可以避免幻读。 关键点在于，是InnoDB默认对一个普通的查询也会加next-key locks，还是说需要应用自己来加锁呢？如果单看这一句，可能会以为InnoDB对普通的查询也加了锁，如果是，那和序列化（SERIALIZABLE）的区别又在哪里呢？ MySQL manual里还有一段： 13.2.8.5. Avoiding the Phantom Problem Using Next-Key Locking (http://dev.mysql.com/doc/refman/5.0/en/innodb-next-key-locking.html) To prevent phantoms, InnoDB uses an algorithm called next-key locking that combines index-row locking with gap locking. You can use next-key locking to implement a uniqueness check in your application: If you read your data in share mode and do not see a duplicate for a row you are going to insert, then you can safely insert your row and know that the next-key lock set on the successor of your row during the read prevents anyone meanwhile inserting a duplicate for your row. Thus, the next-key locking enables you to “lock” the nonexistence of something in your table. 我的理解是说，InnoDB提供了next-key locks，但需要应用程序自己去加锁。manual里提供一个例子： SELECT * FROM child WHERE id &gt; 100 FOR UPDATE; 这样，InnoDB会给id大于100的行（假如child表里有一行id为102），以及100-102，102+的gap都加上锁。 可以使用show innodb status来查看是否给表加上了锁。 再看一个实验，要注意，表t_bitfly里的id为主键字段。实验三： t Session A Session B|| START TRANSACTION; START TRANSACTION;|| SELECT * FROM t_bitfly| WHERE id&lt;=1| FOR UPDATE;| +——+——-+| | id | value || +——+——-+| | 1 | a || +——+——-+| INSERT INTO t_bitfly| VALUES (2, ‘b’);| Query OK, 1 row affected|| SELECT * FROM t_bitfly;| +——+——-+| | id | value || +——+——-+| | 1 | a || +——+——-+| INSERT INTO t_bitfly| VALUES (0, ‘0’);| (waiting for lock …| then timeout)| ERROR 1205 (HY000):| Lock wait timeout exceeded;| try restarting transaction|| SELECT * FROM t_bitfly;| +——+——-+| | id | value || +——+——-+| | 1 | a || +——+——-+| COMMIT;|| SELECT * FROM t_bitfly;| +——+——-+| | id | value || +——+——-+| | 1 | a || +——+——-+v 可以看到，用id&lt;=1加的锁，只锁住了id&lt;=1的范围，可以成功添加id为2的记录，添加id为0的记录时就会等待锁的释放。 MySQL manual里对可重复读里的锁的详细解释： http://dev.mysql.com/doc/refman/5.0/en/set-transaction.html#isolevel_repeatable-read For locking reads (SELECT with FOR UPDATE or LOCK IN SHARE MODE),UPDATE, and DELETE statements, locking depends on whether the statement uses a unique index with a unique search condition, or a range-type search condition. For a unique index with a unique search condition, InnoDB locks only the index record found, not the gap before it. For other search conditions, InnoDB locks the index range scanned, using gap locks or next-key (gap plus index-record) locks to block insertions by other sessions into the gaps covered by the range. 一致性读和提交读，先看实验，实验四： t Session A Session B|| START TRANSACTION; START TRANSACTION;|| SELECT * FROM t_bitfly;| +—-+——-+| | id | value || +—-+——-+| | 1 | a || +—-+——-+| INSERT INTO t_bitfly| VALUES (2, ‘b’);| COMMIT;|| SELECT * FROM t_bitfly;| +—-+——-+| | id | value || +—-+——-+| | 1 | a || +—-+——-+|| SELECT * FROM t_bitfly LOCK IN SHARE MODE;| +—-+——-+| | id | value || +—-+——-+| | 1 | a || | 2 | b || +—-+——-+|| SELECT * FROM t_bitfly FOR UPDATE;| +—-+——-+| | id | value || +—-+——-+| | 1 | a || | 2 | b || +—-+——-+|| SELECT * FROM t_bitfly;| +—-+——-+| | id | value || +—-+——-+| | 1 | a || +—-+——-+v 如果使用普通的读，会得到一致性的结果，如果使用了加锁的读，就会读到“最新的”“提交”读的结果。 本身，可重复读和提交读是矛盾的。在同一个事务里，如果保证了可重复读，就会看不到其他事务的提交，违背了提交读；如果保证了提交读，就会导致前后两次读到的结果不一致，违背了可重复读。 可以这么讲，InnoDB提供了这样的机制，在默认的可重复读的隔离级别里，可以使用加锁读去查询最新的数据。 http://dev.mysql.com/doc/refman/5.0/en/innodb-consistent-read.html If you want to see the “freshest” state of the database, you should use either the READ COMMITTED isolation level or a locking read:SELECT * FROM t_bitfly LOCK IN SHARE MODE; 结论：MySQL InnoDB的可重复读并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁度使用到的机制就是next-key locks。 ==================== 结尾 ==================== 作者: bitfly. 转载请注明来源或包含本信息. 谢谢链接: http://blog.bitfly.cn/post/mysql-innodb-phantom-read/","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/tags/MySQL/"},{"name":"ACID","slug":"ACID","permalink":"https://blog.fenxiangz.com/tags/ACID/"},{"name":"幻读","slug":"幻读","permalink":"https://blog.fenxiangz.com/tags/%E5%B9%BB%E8%AF%BB/"}]},{"title":"Mysql 分库分表简要记录","slug":"dateabase/mysql/2018-12-16_MySQL分库分表简要记录","date":"2018-12-16T00:00:00.000Z","updated":"2020-12-20T16:47:02.946Z","comments":true,"path":"post/dateabase/mysql/2018-12-16_MySQL分库分表简要记录.html","link":"","permalink":"https://blog.fenxiangz.com/post/dateabase/mysql/2018-12-16_MySQL%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%AE%80%E8%A6%81%E8%AE%B0%E5%BD%95.html","excerpt":"","text":"维度：用户：id取模时间：按年、按月、按周、按日","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/tags/MySQL/"},{"name":"SQL优化","slug":"SQL优化","permalink":"https://blog.fenxiangz.com/tags/SQL%E4%BC%98%E5%8C%96/"}]},{"title":"URL 和 URI 的区别","slug":"network/2018-12-15_URL和URI的区别","date":"2018-12-15T00:00:00.000Z","updated":"2020-12-20T16:47:02.983Z","comments":true,"path":"post/network/2018-12-15_URL和URI的区别.html","link":"","permalink":"https://blog.fenxiangz.com/post/network/2018-12-15_URL%E5%92%8CURI%E7%9A%84%E5%8C%BA%E5%88%AB.html","excerpt":"URI, URL, URN从上面的那幅图可以看出来，一共有三个不同的概念URI，URL，URN。这讨论这样的问题时，最好的方法就是回到原点啊，这里我们在RFC 3986: Uniform Resource Identifier (URI): Generic Syntax里面收集了点资料： “A Uniform Resource Identifier (URI) 是一个紧凑的字符串用来标示抽象或物理资源。”“A URI 可以进一步被分为定位符、名字或两者都是。术语“Uniform Resource Locator” (URL) 是URI的子集，除了确定一个资源，还提供一种定位该资源的主要访问机制(如其网络“位置”)。”","text":"URI, URL, URN从上面的那幅图可以看出来，一共有三个不同的概念URI，URL，URN。这讨论这样的问题时，最好的方法就是回到原点啊，这里我们在RFC 3986: Uniform Resource Identifier (URI): Generic Syntax里面收集了点资料： “A Uniform Resource Identifier (URI) 是一个紧凑的字符串用来标示抽象或物理资源。”“A URI 可以进一步被分为定位符、名字或两者都是。术语“Uniform Resource Locator” (URL) 是URI的子集，除了确定一个资源，还提供一种定位该资源的主要访问机制(如其网络“位置”)。” 那我们无所不知的维基百科把这段消化的很好，并描述的更加形象了： “URI可以分为URL，URI或同时具备locators 和names特性的一个东西。URN作用就好像一个人的名字，URL就像一个人的地址。换句话说：URN确定了东西的身份，URL提供了找到它的方式。” 通过这些描述我们可以得到一些结论： 1、首先，URL是URI的一种（通过那个图就看的出来吧）。所以有人跟你说URL不是URI，他就错了呗。但也不是所有的URI都是URL哦，就好像蝴蝶都会飞，但会飞的可不都是蝴蝶啊，你让苍蝇怎么想！2、让URI能成为URL的当然就是那个“访问机制”①，“网络位置”。例如： http:// 或 ftp:// 。3、URN是唯一标识的一部分，就是一个特殊的名字。 下面就来看看例子吧，当来也是来自权威的RFC： • ftp://ftp.is.co.za/rfc/rfc1808.txt (also a URL because of the protocol)• http://www.ietf.org/rfc/rfc2396.txt (also a URL because of the protocol)• ldap://[2001:db8::7]/c=GB?objectClass?one (also a URL because of the protocol)• mailto:&#x4a;&#111;&#x68;&#110;&#46;&#68;&#111;&#101;&#x40;&#101;&#120;&#x61;&#109;&#x70;&#x6c;&#101;&#46;&#x63;&#x6f;&#109; (also a URL because of the protocol)• news:comp.infosystems.www.servers.unix (also a URL because of the protocol)• tel:+1-816-555-1212• telnet://192.0.2.16:80/ (also a URL because of the protocol)• urn:oasis:names:specification:docbook:dtd:xml:4.1.2 以上列举的都是URI, 其中一些是URL。哪些是URL? 就是那些提供了访问机制的. 总结当我们替代web地址的时候，URI和URL那个更准确？ 基于我读的很多的文章，包括RFC，我想说URI更准确。 别急，我有我的理由： 我们经常使用的URI不是严格技术意义上的URL。例如：你需要的文件在files.hp.com. 这是URI，但不是URL–系统可能会对很多协议和端口都做出正确的反应。 你去http://files.hp.com 和 ftp://files.hp.com 可能得到完全不同的内容。这种情况可能更加普遍，想想不同谷歌域名上的不同服务啊。 所以，用URI吧，通常在技术上是正确的，URL可不一定。 最后“URL”这个术语正在被弃用。因此，就准确性而言，URI是相当安全选择。 译注：①访问机制可以理解为“协议方案”，既约定好的一种信息交换协议，可以是：http、ftp、https、git等，标准的URI协议有很多种，具体可以访问IANA-URI SCHEMES参考。 附：URI标准格式：scheme:[//[user:password@]host[:port]][/]path[?query][#fragment]说明： scheme：协议方案名称 user：用户名（登录信息，用于认证） password：密码（登录信息，用于认证） host：服务器地址 port：服务器端口号 path：带层次的文件路径 query：查询字符串 fragment：片段标识符 原文：https://danielmiessler.com/study/url_vs_uri/","categories":[{"name":"网络/术语","slug":"网络-术语","permalink":"https://blog.fenxiangz.com/categories/%E7%BD%91%E7%BB%9C-%E6%9C%AF%E8%AF%AD/"}],"tags":[{"name":"网络术语","slug":"网络术语","permalink":"https://blog.fenxiangz.com/tags/%E7%BD%91%E7%BB%9C%E6%9C%AF%E8%AF%AD/"},{"name":"URL","slug":"URL","permalink":"https://blog.fenxiangz.com/tags/URL/"},{"name":"URI","slug":"URI","permalink":"https://blog.fenxiangz.com/tags/URI/"}]},{"title":"Mysql 优化概要","slug":"dateabase/mysql/2018-12-15_MySQL优化概要","date":"2018-12-15T00:00:00.000Z","updated":"2020-12-20T16:47:02.946Z","comments":true,"path":"post/dateabase/mysql/2018-12-15_MySQL优化概要.html","link":"","permalink":"https://blog.fenxiangz.com/post/dateabase/mysql/2018-12-15_MySQL%E4%BC%98%E5%8C%96%E6%A6%82%E8%A6%81.html","excerpt":"","text":"索引优化配置记录没有使用索引的查询日志配置： mysql&gt; show variables like &#39;%log_que%&#39;; +-------------------------------+-------+ | Variable_name | Value | +-------------------------------+-------+ | log_queries_not_using_indexes | OFF | +-------------------------------+-------+ 1 row in set (0.01 sec) mysql&gt; set global log_queries_not_using_indexes=ON; Query OK, 0 rows affected (0.00 sec) mysql&gt; show variables like &#39;%log_que%&#39;; +-------------------------------+-------+ | Variable_name | Value | +-------------------------------+-------+ | log_queries_not_using_indexes | ON | +-------------------------------+-------+ 1 row in set (0.01 sec) 开启慢查日志配置： mysql&gt; show variables like &#39;%slow_query%&#39;; +---------------------+--------------------------------+ | Variable_name | Value | +---------------------+--------------------------------+ | slow_query_log | OFF | | slow_query_log_file | /var/lib/mysql/mysql-slow.log | +---------------------+--------------------------------+ 2 rows in set (0.01 sec) mysql&gt; set global slow_query_log=ON; Query OK, 0 rows affected (0.04 sec) mysql&gt; show variables like &#39;%slow_query%&#39;; +---------------------+--------------------------------+ | Variable_name | Value | +---------------------+--------------------------------+ | slow_query_log | ON | | slow_query_log_file | /var/lib/mysql/mysql-slow.log | +---------------------+--------------------------------+ 2 rows in set (0.01 sec) 多久算慢查询？设置慢查询时间配置： 全局： mysql&gt; SHOW GLOBAL VARIABLES LIKE &quot;long_query_time&quot;; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 11.000000 | +-----------------+-----------+ 1 row in set (0.01 sec) mysql&gt; set global long_query_time=1; Query OK, 0 rows affected (0.00 sec) mysql&gt; SHOW GLOBAL VARIABLES LIKE &quot;long_query_time&quot;; +-----------------+----------+ | Variable_name | Value | +-----------------+----------+ | long_query_time | 1.000000 | +-----------------+----------+ 1 row in set (0.00 sec) 会话： mysql&gt; SHOW SESSION VARIABLES LIKE &quot;long_query_time&quot;; +-----------------+----------+ | Variable_name | Value | +-----------------+----------+ | long_query_time | 2.000000 | +-----------------+----------+ 1 row in set (0.01 sec) mysql&gt; SET @@SESSION.long_query_time = 1; Query OK, 0 rows affected (0.00 sec) mysql&gt; SHOW SESSION VARIABLES LIKE &quot;long_query_time&quot;; +-----------------+----------+ | Variable_name | Value | +-----------------+----------+ | long_query_time | 1.000000 | +-----------------+----------+ 1 row in set (0.00 sec) 慢查日志分析工具：mysqldumpslow、pt-query-digest 通过慢查询日志发现有问题的SQL： 查询次数多且每次查询占用时间长的SQL； （pt-query-digest 分析的前几个查询） IO大的SQL； （pt-query-digest 分析中的 Rows examine项） 未命中索引的SQL。 （pt-query-digest 分析中的 Rows examine 和 Rows Send 的对比， 扫描大于发送行，说明索引命中率低） 执行计划分析EXPLAIN 用来查看MySQL执行一个SQL语句的执行计划。语法： &#123;EXPLAIN | DESCRIBE | DESC&#125; tbl_name [col_name | wild] &#123;EXPLAIN | DESCRIBE | DESC&#125; [explain_type] &#123;explainable_stmt | FOR CONNECTION connection_id&#125; explain_type: &#123; EXTENDED | PARTITIONS | FORMAT = format_name &#125; format_name: &#123; TRADITIONAL | JSON &#125; explainable_stmt: &#123; SELECT statement | DELETE statement | INSERT statement | REPLACE statement | UPDATE statement &#125; http://blog.fenxiangz.com/mysql/mysql-explain/ 下文转载说起MySQL的查询优化，相信大家收藏了一堆奇淫技巧：不能使用SELECT *、不使用NULL字段、合理创建索引、为字段选择合适的数据类型….. 你是否真的理解这些优化技巧？是否理解其背后的工作原理？在实际场景下性能真有提升吗？我想未必。因而理解这些优化建议背后的原理就尤为重要，希望本文能让你重新审视这些优化建议，并在实际业务场景下合理的运用。 MySQL逻辑架构如果能在头脑中构建一幅MySQL各组件之间如何协同工作的架构图，有助于深入理解MySQL服务器。下图展示了MySQL的逻辑架构图。 -&gt;MySQL逻辑架构，来自：高性能MySQL&lt;- MySQL逻辑架构整体分为三层，最上层为客户端层，并非MySQL所独有，诸如：连接处理、授权认证、安全等功能均在这一层处理。 MySQL大多数核心服务均在中间这一层，包括查询解析、分析、优化、缓存、内置函数(比如：时间、数学、加密等函数)。所有的跨存储引擎的功能也在这一层实现：存储过程、触发器、视图等。 最下层为存储引擎，其负责MySQL中的数据存储和提取。和Linux下的文件系统类似，每种存储引擎都有其优势和劣势。中间的服务层通过API与存储引擎通信，这些API接口屏蔽了不同存储引擎间的差异。 MySQL查询过程我们总是希望MySQL能够获得更高的查询性能，最好的办法是弄清楚MySQL是如何优化和执行查询的。一旦理解了这一点，就会发现：很多的查询优化工作实际上就是遵循一些原则让MySQL的优化器能够按照预想的合理方式运行而已。 当向MySQL发送一个请求的时候，MySQL到底做了些什么呢？ -&gt;MySQL查询过程&lt;- 客户端/服务端通信协议MySQL客户端/服务端通信协议是“半双工”的：在任一时刻，要么是服务器向客户端发送数据，要么是客户端向服务器发送数据，这两个动作不能同时发生。一旦一端开始发送消息，另一端要接收完整个消息才能响应它，所以我们无法也无须将一个消息切成小块独立发送，也没有办法进行流量控制。 客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置max_allowed_packet参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。 与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送。因而在实际开发中，尽量保持查询简单且只返回必需的数据，减小通信间数据包的大小和数量是一个非常好的习惯，这也是查询中尽量避免使用SELECT *以及加上LIMIT限制的原因之一。 查询缓存在解析一个查询语句前，如果查询缓存是打开的，那么MySQL会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。 MySQL将缓存存放在一个引用表（不要理解成table，可以认为是类似于HashMap的数据结构），通过一个哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、mysql库中的系统表，其查询结果都不会被缓存。比如函数NOW()或者CURRENT_DATE()会因为不同的查询时间，返回不同的查询结果，再比如包含CURRENT_USER或者CONNECION_ID()的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。 既然是缓存，就会失效，那查询缓存何时失效呢？MySQL的查询缓存系统会跟踪查询中涉及的每个表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。正因为如此，在任何的写操作时，MySQL必须将对应表的所有缓存都设置为失效。如果查询缓存非常大或者碎片很多，这个操作就可能带来很大的系统消耗，甚至导致系统僵死一会儿。而且查询缓存对系统的额外消耗也不仅仅在写操作，读操作也不例外： 任何的查询语句在开始之前都必须经过检查，即使这条SQL语句永远不会命中缓存 如果查询结果可以被缓存，那么执行完成后，会将结果存入缓存，也会带来额外的系统消耗 基于此，我们要知道并不是什么情况下查询缓存都会提高系统性能，缓存和失效都会带来额外消耗，只有当缓存带来的资源节约大于其本身消耗的资源时，才会给系统带来性能提升。但要如何评估打开缓存是否能够带来性能提升是一件非常困难的事情，也不在本文讨论的范畴内。如果系统确实存在一些性能问题，可以尝试打开查询缓存，并在数据库设计上做一些优化，比如： 用多个小表代替一个大表，注意不要过度设计 批量插入代替循环单条插入 合理控制缓存空间大小，一般来说其大小设置为几十兆比较合适 可以通过SQL_CACHE和SQL_NO_CACHE来控制某个查询语句是否需要进行缓存 最后的忠告是不要轻易打开查询缓存，特别是写密集型应用。如果你实在是忍不住，可以将query_cache_type设置为DEMAND，这时只有加入SQL_CACHE的查询才会走缓存，其他查询则不会，这样可以非常自由地控制哪些查询需要被缓存。 当然查询缓存系统本身是非常复杂的，这里讨论的也只是很小的一部分，其他更深入的话题，比如：缓存是如何使用内存的？如何控制内存的碎片化？事务对查询缓存有何影响等等，读者可以自行阅读相关资料，这里权当抛砖引玉吧。 语法解析和预处理MySQL通过关键字将SQL语句进行解析，并生成一颗对应的解析树。这个过程解析器主要通过语法规则来验证和解析。比如SQL中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据MySQL规则进一步检查解析树是否合法。比如检查要查询的数据表和数据列是否存在等等。 查询优化经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成查询计划。多数情况下，一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。 MySQL使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在MySQL可以通过查询当前会话的last_query_cost的值来得到其计算当前查询的成本。 123456789mysql&gt; select * from t_message limit 10;...省略结果集mysql&gt; show status like &#39;last_query_cost&#39;;+-----------------+-------------+| Variable_name | Value |+-----------------+-------------+| Last_query_cost | 6391.799000 |+-----------------+-------------+ 示例中的结果表示优化器认为大概需要做6391个数据页的随机查找才能完成上面的查询。这个结果是根据一些列的统计信息计算得来的，这些统计信息包括：每张表或者索引的页面个数、索引的基数、索引和数据行的长度、索引的分布情况等等。 有非常多的原因会导致MySQL选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL认为的最优跟我们想的不一样（我们希望执行时间尽可能短，但MySQL值选择它认为成本小的，但成本小并不意味着执行时间短）等等。 MySQL的查询优化器是一个非常复杂的部件，它使用了非常多的优化策略来生成一个最优的执行计划： 重新定义表的关联顺序（多张表关联查询时，并不一定按照SQL中指定的顺序进行，但有一些技巧可以指定关联顺序） 优化MIN()和MAX()函数（找某列的最小值，如果该列有索引，只需要查找B+Tree索引最左端，反之则可以找到最大值，具体原理见下文） 提前终止查询（比如：使用Limit时，查找到满足数量的结果集后会立即终止查询） 优化排序（在老版本MySQL会使用两次传输排序，即先读取行指针和需要排序的字段在内存中对其排序，然后再根据排序结果去读取数据行，而新版本采用的是单次传输排序，也就是一次读取所有的数据行，然后根据给定的列排序。对于I/O密集型应用，效率会高很多） 随着MySQL的不断发展，优化器使用的优化策略也在不断的进化，这里仅仅介绍几个非常常用且容易理解的优化策略，其他的优化策略，大家自行查阅吧。 查询执行引擎在完成解析和优化阶段以后，MySQL会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为handler API。查询过程中的每一张表由一个handler实例表示。实际上，MySQL在查询优化阶段就为每一张表创建了一个handler实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。 返回结果给客户端查询执行的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL仍然会返回这个查询的相关信息，比如改查询影响到的行数以及执行时间等等。 如果查询缓存被打开且这个查询可以被缓存，MySQL也会将结果存放到缓存中。 结果集返回客户端是一个增量且逐步返回的过程。有可能MySQL在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足①中所描述的通信协议的数据包发送，再通过TCP协议进行传输，在传输过程中，可能对MySQL的数据包进行缓存然后批量发送。 回头总结一下MySQL整个查询执行过程，总的来说分为6个步骤： 客户端向MySQL服务器发送一条查询请求 服务器首先检查查询缓存，如果命中缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段 服务器进行SQL解析、预处理、再由优化器生成对应的执行计划 MySQL根据执行计划，调用存储引擎的API来执行查询 将结果返回给客户端，同时缓存查询结果 性能优化建议看了这么多，你可能会期待给出一些优化手段，是的，下面会从3个不同方面给出一些优化建议。但请等等，还有一句忠告要先送给你：不要听信你看到的关于优化的“绝对真理”，包括本文所讨论的内容，而应该是在实际的业务场景下通过测试来验证你关于执行计划以及响应时间的假设。 Scheme设计与数据类型优化选择数据类型只要遵循小而简单的原则就好，越小的数据类型通常会更快，占用更少的磁盘、内存，处理时需要的CPU周期也更少。越简单的数据类型在计算时需要更少的CPU周期，比如，整型就比字符操作代价低，因而会使用整型来存储ip地址，使用DATETIME来存储时间，而不是使用字符串。 这里总结几个可能容易理解错误的技巧： 通常来说把可为NULL的列改为NOT NULL不会对性能提升有多少帮助，只是如果计划在列上创建索引，就应该将该列设置为NOT NULL。 对整数类型指定宽度，比如INT(11)，没有任何卵用。INT使用32位（4个字节）存储空间，那么它的表示范围已经确定，所以INT(1)和INT(20)对于存储和计算是相同的。 UNSIGNED表示不允许负值，大致可以使正数的上限提高一倍。比如TINYINT存储范围是-128 ~ 127，而UNSIGNED TINYINT存储的范围却是0 - 255。 通常来讲，没有太大的必要使用DECIMAL数据类型。即使是在需要存储财务数据时，仍然可以使用BIGINT。比如需要精确到万分之一，那么可以将数据乘以一百万然后使用BIGINT存储。这样可以避免浮点数计算不准确和DECIMAL精确计算代价高的问题。 TIMESTAMP使用4个字节存储空间，DATETIME使用8个字节存储空间。因而，TIMESTAMP只能表示1970 - 2038年，比DATETIME表示的范围小得多，而且TIMESTAMP的值因时区不同而不同。 大多数情况下没有使用枚举类型的必要，其中一个缺点是枚举的字符串列表是固定的，添加和删除字符串（枚举选项）必须使用ALTER TABLE（如果只只是在列表末尾追加元素，不需要重建表）。 schema的列不要太多。原因是存储引擎的API工作时需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后在服务器层将缓冲内容解码成各个列，这个转换过程的代价是非常高的。如果列太多而实际使用的列又很少的话，有可能会导致CPU占用过高。 大表ALTER TABLE非常耗时，MySQL执行大部分修改表结果操作的方法是用新的结构创建一个张空表，从旧表中查出所有的数据插入新表，然后再删除旧表。尤其当内存不足而表又很大，而且还有很大索引的情况下，耗时更久。当然有一些奇淫技巧可以解决这个问题，有兴趣可自行查阅。 创建高性能索引索引是提高MySQL查询性能的一个重要途径，但过多的索引可能会导致过高的磁盘使用率以及过高的内存占用，从而影响应用程序的整体性能。应当尽量避免事后才想起添加索引，因为事后可能需要监控大量的SQL才能定位到问题所在，而且添加索引的时间肯定是远大于初始添加索引所需要的时间，可见索引的添加也是非常有技术含量的。 接下来将向你展示一系列创建高性能索引的策略，以及每条策略其背后的工作原理。但在此之前，先了解与索引相关的一些算法和数据结构，将有助于更好的理解后文的内容。 索引相关的数据结构和算法通常我们所说的索引是指B-Tree索引，它是目前关系型数据库中查找数据最为常用和有效的索引，大多数存储引擎都支持这种索引。使用B-Tree这个术语，是因为MySQL在CREATE TABLE或其它语句中使用了这个关键字，但实际上不同的存储引擎可能使用不同的数据结构，比如InnoDB就是使用的B+Tree。 B+Tree中的B是指balance，意为平衡。需要注意的是，B+树索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。 在介绍B+Tree前，先了解一下二叉查找树，它是一种经典的数据结构，其左子树的值总是小于根的值，右子树的值总是大于根的值，如下图①。如果要在这课树中查找值为5的记录，其大致流程：先找到根，其值为6，大于5，所以查找左子树，找到3，而5大于3，接着找3的右子树，总共找了3次。同样的方法，如果查找值为8的记录，也需要查找3次。所以二叉查找树的平均查找次数为(3 + 3 + 3 + 2 + 2 + 1) / 6 = 2.3次，而顺序查找的话，查找值为2的记录，仅需要1次，但查找值为8的记录则需要6次，所以顺序查找的平均查找次数为：(1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.3次，因为大多数情况下二叉查找树的平均查找速度比顺序查找要快。 -&gt;二叉查找树和平衡二叉树&lt;- 由于二叉查找树可以任意构造，同样的值，可以构造出如图②的二叉查找树，显然这棵二叉树的查询效率和顺序查找差不多。若想二叉查找数的查询性能最高，需要这棵二叉查找树是平衡的，也即平衡二叉树（AVL树）。 平衡二叉树首先需要符合二叉查找树的定义，其次必须满足任何节点的两个子树的高度差不能大于1。显然图②不满足平衡二叉树的定义，而图①是一课平衡二叉树。平衡二叉树的查找性能是比较高的（性能最好的是最优二叉树），查询性能越好，维护的成本就越大。比如图①的平衡二叉树，当用户需要插入一个新的值9的节点时，就需要做出如下变动。-&gt;平衡二叉树旋转&lt;- 通过一次左旋操作就将插入后的树重新变为平衡二叉树是最简单的情况了，实际应用场景中可能需要旋转多次。至此我们可以考虑一个问题，平衡二叉树的查找效率还不错，实现也非常简单，相应的维护成本还能接受，为什么MySQL索引不直接使用平衡二叉树？ 随着数据库中数据的增加，索引本身大小随之增加，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级。可以想象一下一棵几百万节点的二叉树的深度是多少？如果将这么大深度的一颗二叉树放磁盘上，每读取一个节点，需要一次磁盘的I/O读取，整个查找的耗时显然是不能够接受的。那么如何减少查找过程中的I/O存取次数？ 一种行之有效的解决方法是减少树的深度，将二叉树变为m叉树（多路搜索树），而B+Tree就是一种多路搜索树。理解B+Tree时，只需要理解其最重要的两个特征即可：第一，所有的关键字（可以理解为数据）都存储在叶子节点（Leaf Page），非叶子节点（Index Page）并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。其次，所有的叶子节点由指针连接。如下图为高度为2的简化了的B+Tree。 -&gt;简化B+Tree&lt;- 怎么理解这两个特征？MySQL将每个节点的大小设置为一个页的整数倍（原因下文会介绍），也就是在节点空间大小一定的情况下，每个节点可以存储更多的内结点，这样每个结点能索引的范围更大更精确。所有的叶子节点使用指针链接的好处是可以进行区间访问，比如上图中，如果查找大于20而小于30的记录，只需要找到节点20，就可以遍历指针依次找到25、30。如果没有链接指针的话，就无法进行区间查找。这也是MySQL使用B+Tree作为索引存储结构的重要原因。 MySQL为何将节点大小设置为页的整数倍，这就需要理解磁盘的存储原理。磁盘本身存取就比主存慢很多，在加上机械运动损耗（特别是普通的机械硬盘），磁盘的存取速度往往是主存的几百万分之一，为了尽量减少磁盘I/O，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存，预读的长度一般为页的整数倍。 页是计算机管理存储器的逻辑块，硬件及OS往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（许多OS中，页的大小通常为4K）。主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 MySQL巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了读取一个节点只需一次I/O。假设B+Tree的高度为h，一次检索最多需要h-1I/O（根节点常驻内存），复杂度$O(h) = O(\\log_{M}N)$。实际应用场景中，M通常较大，常常超过100，因此树的高度一般都比较小，通常不超过3。 最后简单了解下B+Tree节点的操作，在整体上对索引的维护有一个大概的了解，虽然索引可以大大提高查询效率，但维护索引仍要花费很大的代价，因此合理的创建索引也就尤为重要。 仍以上面的树为例，我们假设每个节点只能存储4个内节点。首先要插入第一个节点28，如下图所示。 -&gt;leaf page和index page都没有满&lt;- 接着插入下一个节点70，在Index Page中查询后得知应该插入到50 - 70之间的叶子节点，但叶子节点已满，这时候就需要进行也分裂的操作，当前的叶子节点起点为50，所以根据中间值来拆分叶子节点，如下图所示。 -&gt;Leaf Page拆分&lt;- 最后插入一个节点95，这时候Index Page和Leaf Page都满了，就需要做两次拆分，如下图所示。-&gt; Leaf Page与Index Page拆分 &lt;- 拆分后最终形成了这样一颗树。-&gt;最终树&lt;- B+Tree为了保持平衡，对于新插入的值需要做大量的拆分页操作，而页的拆分需要I/O操作，为了尽可能的减少页的拆分操作，B+Tree也提供了类似于平衡二叉树的旋转功能。当Leaf Page已满但其左右兄弟节点没有满的情况下，B+Tree并不急于去做拆分操作，而是将记录移到当前所在页的兄弟节点上。通常情况下，左兄弟会被先检查用来做旋转操作。就比如上面第二个示例，当插入70的时候，并不会去做页拆分，而是左旋操作。 -&gt;左旋操作&lt;- 通过旋转操作可以最大限度的减少页分裂，从而减少索引维护过程中的磁盘的I/O操作，也提高索引维护效率。需要注意的是，删除节点跟插入节点类型，仍然需要旋转和拆分操作，这里就不再说明。 高性能策略通过上文，相信你对B+Tree的数据结构已经有了大致的了解，但MySQL中索引是如何组织数据的存储呢？以一个简单的示例来说明，假如有如下数据表： 1234567CREATE TABLE People( last_name varchar(50) not null, first_name varchar(50) not null, dob date not null, gender enum(&#96;m&#96;,&#96;f&#96;) not null, key(last_name,first_name,dob)); 对于表中每一行数据，索引中包含了last_name、first_name、dob列的值，下图展示了索引是如何组织数据存储的。 -&gt;索引如何组织数据存储，来自：高性能MySQL&lt;- 可以看到，索引首先根据第一个字段来排列顺序，当名字相同时，则根据第三个字段，即出生日期来排序，正是因为这个原因，才有了索引的“最左原则”。 1、MySQL不会使用索引的情况：非独立的列“独立的列”是指索引列不能是表达式的一部分，也不能是函数的参数。比如： 1select * from where id + 1 &#x3D; 5 我们很容易看出其等价于 id = 4，但是MySQL无法自动解析这个表达式，使用函数是同样的道理。 2、前缀索引如果列很长，通常可以索引开始的部分字符，这样可以有效节约索引空间，从而提高索引效率。 3、多列索引和索引顺序在多数情况下，在多个列上建立独立的索引并不能提高查询性能。理由非常简单，MySQL不知道选择哪个索引的查询效率更好，所以在老版本，比如MySQL5.0之前就会随便选择一个列的索引，而新的版本会采用合并索引的策略。举个简单的例子，在一张电影演员表中，在actor_id和film_id两个列上都建立了独立的索引，然后有如下查询： 1select film_id,actor_id from film_actor where actor_id &#x3D; 1 or film_id &#x3D; 1 老版本的MySQL会随机选择一个索引，但新版本做如下的优化： 123select film_id,actor_id from film_actor where actor_id &#x3D; 1 union all select film_id,actor_id from film_actor where film_id &#x3D; 1 and actor_id &lt;&gt; 1 当出现多个索引做相交操作时（多个AND条件），通常来说一个包含所有相关列的索引要优于多个独立索引。 当出现多个索引做联合操作时（多个OR条件），对结果集的合并、排序等操作需要耗费大量的CPU和内存资源，特别是当其中的某些索引的选择性不高，需要返回合并大量数据时，查询成本更高。所以这种情况下还不如走全表扫描。 因此explain时如果发现有索引合并（Extra字段出现Using union），应该好好检查一下查询和表结构是不是已经是最优的，如果查询和表都没有问题，那只能说明索引建的非常糟糕，应当慎重考虑索引是否合适，有可能一个包含所有相关列的多列索引更适合。 前面我们提到过索引如何组织数据存储的，从图中可以看到多列索引时，索引的顺序对于查询是至关重要的，很明显应该把选择性更高的字段放到索引的前面，这样通过第一个字段就可以过滤掉大多数不符合条件的数据。 索引选择性是指不重复的索引值和数据表的总记录数的比值，选择性越高查询效率越高，因为选择性越高的索引可以让MySQL在查询时过滤掉更多的行。唯一索引的选择性是1，这时最好的索引选择性，性能也是最好的。 理解索引选择性的概念后，就不难确定哪个字段的选择性较高了，查一下就知道了，比如： 1SELECT * FROM payment where staff_id &#x3D; 2 and customer_id &#x3D; 584 是应该创建(staff_id,customer_id)的索引还是应该颠倒一下顺序？执行下面的查询，哪个字段的选择性更接近1就把哪个字段索引前面就好。 123select count(distinct staff_id)&#x2F;count(*) as staff_id_selectivity, count(distinct customer_id)&#x2F;count(*) as customer_id_selectivity, count(*) from payment 多数情况下使用这个原则没有任何问题，但仍然注意你的数据中是否存在一些特殊情况。举个简单的例子，比如要查询某个用户组下有过交易的用户信息： 1select user_id from trade where user_group_id &#x3D; 1 and trade_amount &gt; 0 MySQL为这个查询选择了索引(user_group_id,trade_amount)，如果不考虑特殊情况，这看起来没有任何问题，但实际情况是这张表的大多数数据都是从老系统中迁移过来的，由于新老系统的数据不兼容，所以就给老系统迁移过来的数据赋予了一个默认的用户组。这种情况下，通过索引扫描的行数跟全表扫描基本没什么区别，索引也就起不到任何作用。 推广开来说，经验法则和推论在多数情况下是有用的，可以指导我们开发和设计，但实际情况往往会更复杂，实际业务场景下的某些特殊情况可能会摧毁你的整个设计。 4、避免多个范围条件实际开发中，我们会经常使用多个范围条件，比如想查询某个时间段内登录过的用户： 1select user.* from user where login_time &gt; &#39;2017-04-01&#39; and age between 18 and 30; 这个查询有一个问题：它有两个范围条件，login_time列和age列，MySQL可以使用login_time列的索引或者age列的索引，但无法同时使用它们。 5、覆盖索引如果一个索引包含或者说覆盖所有需要查询的字段的值，那么就没有必要再回表查询，这就称为覆盖索引。覆盖索引是非常有用的工具，可以极大的提高性能，因为查询只需要扫描索引会带来许多好处： 索引条目远小于数据行大小，如果只读取索引，极大减少数据访问量 索引是有按照列值顺序存储的，对于I/O密集型的范围查询要比随机从磁盘读取每一行数据的IO要少的多 6、使用索引扫描来排序MySQL有两种方式可以生产有序的结果集，其一是对结果集进行排序的操作，其二是按照索引顺序扫描得出的结果自然是有序的。如果explain的结果中type列的值为index表示使用了索引扫描来做排序。 扫描索引本身很快，因为只需要从一条索引记录移动到相邻的下一条记录。但如果索引本身不能覆盖所有需要查询的列，那么就不得不每扫描一条索引记录就回表查询一次对应的行。这个读取操作基本上是随机I/O，因此按照索引顺序读取数据的速度通常要比顺序地全表扫描要慢。 在设计索引时，如果一个索引既能够满足排序，有满足查询，是最好的。 只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向也一样时，才能够使用索引来对结果做排序。如果查询需要关联多张表，则只有ORDER BY子句引用的字段全部为第一张表时，才能使用索引做排序。ORDER BY子句和查询的限制是一样的，都要满足最左前缀的要求（有一种情况例外，就是最左的列被指定为常数，下面是一个简单的示例），其他情况下都需要执行排序操作，而无法利用索引排序。 12&#x2F;&#x2F; 最左列为常数，索引：(date,staff_id,customer_id)select staff_id,customer_id from demo where date &#x3D; &#39;2015-06-01&#39; order by staff_id,customer_id 7、冗余和重复索引冗余索引是指在相同的列上按照相同的顺序创建的相同类型的索引，应当尽量避免这种索引，发现后立即删除。比如有一个索引(A,B)，再创建索引(A)就是冗余索引。冗余索引经常发生在为表添加新索引时，比如有人新建了索引(A,B)，但这个索引不是扩展已有的索引(A)。 大多数情况下都应该尽量扩展已有的索引而不是创建新索引。但有极少情况下出现性能方面的考虑需要冗余索引，比如扩展已有索引而导致其变得过大，从而影响到其他使用该索引的查询。 8、删除长期未使用的索引定期删除一些长时间未使用过的索引是一个非常好的习惯。 关于索引这个话题打算就此打住，最后要说一句，索引并不总是最好的工具，只有当索引帮助提高查询速度带来的好处大于其带来的额外工作时，索引才是有效的。对于非常小的表，简单的全表扫描更高效。对于中到大型的表，索引就非常有效。对于超大型的表，建立和维护索引的代价随之增长，这时候其他技术也许更有效，比如分区表。最后的最后，**explain后再提测是一种美德**。 特定类型查询优化优化COUNT()查询COUNT()可能是被大家误解最多的函数了，它有两种不同的作用，其一是统计某个列值的数量，其二是统计行数。统计列值时，要求列值是非空的，它不会统计NULL。如果确认括号中的表达式不可能为空时，实际上就是在统计行数。最简单的就是当使用COUNT(*)时，并不是我们所想象的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计所有的行数。 我们最常见的误解也就在这儿，在括号内指定了一列却希望统计结果是行数，而且还常常误以为前者的性能会更好。但实际并非这样，如果要统计行数，直接使用COUNT(*)，意义清晰，且性能更好。 有时候某些业务场景并不需要完全精确的COUNT值，可以用近似值来代替，EXPLAIN出来的行数就是一个不错的近似值，而且执行EXPLAIN并不需要真正地去执行查询，所以成本非常低。通常来说，执行COUNT()都需要扫描大量的行才能获取到精确的数据，因此很难优化，MySQL层面还能做得也就只有覆盖索引了。如果不还能解决问题，只有从架构层面解决了，比如添加汇总表，或者使用redis这样的外部缓存系统。 优化关联查询在大数据场景下，表与表之间通过一个冗余字段来关联，要比直接使用JOIN有更好的性能。如果确实需要使用关联查询的情况下，需要特别注意的是： 确保ON和USING字句中的列上有索引。在创建索引的时候就要考虑到关联的顺序。当表A和表B用列c关联的时候，如果优化器关联的顺序是A、B，那么就不需要在A表的对应列上创建索引。没有用到的索引会带来额外的负担，一般来说，除非有其他理由，只需要在关联顺序中的第二张表的相应列上创建索引（具体原因下文分析）。 确保任何的GROUP BY和ORDER BY中的表达式只涉及到一个表中的列，这样MySQL才有可能使用索引来优化。 要理解优化关联查询的第一个技巧，就需要理解MySQL是如何执行关联查询的。当前MySQL关联执行的策略非常简单，它对任何的关联都执行嵌套循环关联操作，即先在一个表中循环取出单条数据，然后在嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为为止。然后根据各个表匹配的行，返回查询中需要的各个列。 太抽象了？以上面的示例来说明，比如有这样的一个查询： 123SELECT A.xx,B.yy FROM A INNER JOIN B USING(c)WHERE A.xx IN (5,6) 假设MySQL按照查询中的关联顺序A、B来进行关联操作，那么可以用下面的伪代码表示MySQL如何完成这个查询： 1234567891011outer_iterator &#x3D; SELECT A.xx,A.c FROM A WHERE A.xx IN (5,6);outer_row &#x3D; outer_iterator.next;while(outer_row) &#123; inner_iterator &#x3D; SELECT B.yy FROM B WHERE B.c &#x3D; outer_row.c; inner_row &#x3D; inner_iterator.next; while(inner_row) &#123; output[inner_row.yy,outer_row.xx]; inner_row &#x3D; inner_iterator.next; &#125; outer_row &#x3D; outer_iterator.next;&#125; 可以看到，最外层的查询是根据A.xx列来查询的，A.c上如果有索引的话，整个关联查询也不会使用。再看内层的查询，很明显B.c上如果有索引的话，能够加速查询，因此只需要在关联顺序中的第二张表的相应列上创建索引即可。 优化LIMIT分页当需要分页操作时，通常会使用LIMIT加上偏移量的办法实现，同时加上合适的ORDER BY字句。如果有对应的索引，通常效率会不错，否则，MySQL需要做大量的文件排序操作。 一个常见的问题是当偏移量非常大的时候，比如：LIMIT 10000 20这样的查询，MySQL需要查询10020条记录然后只返回20条记录，前面的10000条都将被抛弃，这样的代价非常高。 优化这种查询一个最简单的办法就是尽可能的使用覆盖索引扫描，而不是查询所有的列。然后根据需要做一次关联查询再返回所有的列。对于偏移量很大时，这样做的效率会提升非常大。考虑下面的查询： 1SELECT film_id,description FROM film ORDER BY title LIMIT 50,5; 如果这张表非常大，那么这个查询最好改成下面的样子： 1234SELECT film.film_id,film.descriptionFROM film INNER JOIN ( SELECT film_id FROM film ORDER BY title LIMIT 50,5) AS tmp USING(film_id); 这里的延迟关联将大大提升查询效率，让MySQL扫描尽可能少的页面，获取需要访问的记录后在根据关联列回原表查询所需要的列。 有时候如果可以使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描，这样就可以避免使用OFFSET，比如下面的查询： 123SELECT id FROM t LIMIT 10000, 10;改为：SELECT id FROM t WHERE id &gt; 10000 LIMIT 10; 其他优化的办法还包括使用预先计算的汇总表，或者关联到一个冗余表，冗余表中只包含主键列和需要做排序的列。 优化UNIONMySQL处理UNION的策略是先创建临时表，然后再把各个查询结果插入到临时表中，最后再来做查询。因此很多优化策略在UNION查询中都没有办法很好的时候。经常需要手动将WHERE、LIMIT、ORDER BY等字句“下推”到各个子查询中，以便优化器可以充分利用这些条件先优化。 除非确实需要服务器去重，否则就一定要使用UNION ALL，如果没有ALL关键字，MySQL会给临时表加上DISTINCT选项，这会导致整个临时表的数据做唯一性检查，这样做的代价非常高。当然即使使用ALL关键字，MySQL总是将结果放入临时表，然后再读出，再返回给客户端。虽然很多时候没有这个必要，比如有时候可以直接把每个子查询的结果返回给客户端。 结语理解查询是如何执行以及时间都消耗在哪些地方，再加上一些优化过程的知识，可以帮助大家更好的理解MySQL，理解常见优化技巧背后的原理。希望本文中的原理、示例能够帮助大家更好的将理论和实践联系起来，更多的将理论知识运用到实践中。 其他也没啥说的了，给大家留两个思考题吧，可以在脑袋里想想答案，这也是大家经常挂在嘴边的，但很少有人会思考为什么？ 有非常多的程序员在分享时都会抛出这样一个观点：尽可能不要使用存储过程，存储过程非常不容易维护，也会增加使用成本，应该把业务逻辑放到客户端。既然客户端都能干这些事，那为什么还要存储过程？ JOIN本身也挺方便的，直接查询就好了，为什么还需要视图呢？ 参考资料[1] 姜承尧 著；MySQL技术内幕-InnoDB存储引擎；机械工业出版社，2013[2] Baron Scbwartz 等著；宁海元 周振兴等译；高性能MySQL（第三版）; 电子工业出版社， 2013[3] 由 B-/B+树看 MySQL索引结构 原文：http://www.jianshu.com/p/d7665192aaaf","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/tags/MySQL/"},{"name":"SQL优化","slug":"SQL优化","permalink":"https://blog.fenxiangz.com/tags/SQL%E4%BC%98%E5%8C%96/"}]},{"title":"Ubuntu 包错误或不一致时的清理方式","slug":"linux/2018-11-17_Ubuntu包错误或不一致时的清理方式","date":"2018-11-17T00:00:00.000Z","updated":"2020-12-20T16:47:02.979Z","comments":true,"path":"post/linux/2018-11-17_Ubuntu包错误或不一致时的清理方式.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2018-11-17_Ubuntu%E5%8C%85%E9%94%99%E8%AF%AF%E6%88%96%E4%B8%8D%E4%B8%80%E8%87%B4%E6%97%B6%E7%9A%84%E6%B8%85%E7%90%86%E6%96%B9%E5%BC%8F.html","excerpt":"","text":"命令模板： sudo mv /var/lib/dpkg/info/&lt;packagename&gt;.* /tmp/ sudo dpkg --remove --force-remove-reinstreq &lt;packagename&gt; sudo apt-get remove &lt;packagename&gt; sudo apt-get autoremove &amp;&amp; sudo apt-get autoclean 清理Docker示例： sudo mv /var/lib/dpkg/info/docker.* /tmp/ sudo dpkg --remove --force-remove-reinstreq docker.io sudo apt-get remove docker docker-ce docker.io containerd.io sudo apt-get autoremove &amp;&amp; sudo apt-get autoclean 参考链接： https://askubuntu.com/questions/148715/how-to-fix-package-is-in-a-very-bad-inconsistent-state-error/510887#510887 https://github.com/docker/for-linux/issues/52#issuecomment-333563492","categories":[{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://blog.fenxiangz.com/tags/Ubuntu/"},{"name":"包管理","slug":"包管理","permalink":"https://blog.fenxiangz.com/tags/%E5%8C%85%E7%AE%A1%E7%90%86/"}]},{"title":"Vert.x - 从零开始变(micro)大拿","slug":"java/advance/2018-11-05_java_vertx","date":"2018-11-05T00:00:00.000Z","updated":"2020-12-20T16:47:02.960Z","comments":true,"path":"post/java/advance/2018-11-05_java_vertx.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2018-11-05_java_vertx.html","excerpt":"","text":"Clement Escoffier, Julien Viet, 沈勇 译Version 0.6,Dec, 23th, 2017 原文：http://escoffier.me/vertx-hol/ 1. 前言让我们从头开始…​. Vert.x . 什么是Vert.x? 这是一个好问题，也许是一个非常好的起点。 如果你访问了 Vert.x 网站, Vert.x 的定义是 “用来在JVM上构建反应式（reactive）应用程序的工具箱”。 这个定义不是十分清楚，是吗？什么是 工具箱 ? 什么是 反应式（reactive)应用程序 ? 在这个实验里，我们将要讲解这些术语，并且用Vert.x建立一个应用程序，以此说明什么是Vert.x。这个应用程序是由一组 微服务(microservices) 组成的。再说这是一个流行趋势。不是吗？ 实际上，Vert.x已经极大的促进了 微服务(microservices) 。 我们要写的应用程序会是这样的： 基于 Vert.x (这是你为什么看这篇文章的愿意，对吧?) 分布式的系统 将要是反应式(reactive)系统 可以让我们体会点儿乐趣 这个练习可以让参加这得到一些 Vert.x 初级的，第一手的经验。从第一行代码开始，建立服务，消费服务并且最终吧所有的组件装配起来，秉承一个完整的反应式(reactive)系统。 这可以说明什么是反应式(reactive)系统。什么是反应式编程，怎么怎么基于多个反应式微服务 (reactive microservices) 构建应用程序。 多个反应式微服务，多个，这个很重要。 这个练习需要带上你的电脑。请带上你的Windows, OSX或者Linux电脑。我们需要 Apache Maven (3.3+)。 你将要学到什么： 什么是 Vert.x, 怎么使用异步非阻塞的并开发模式 怎么使用Vert.x开发微服务。我们将使用多种微服务并且应用服务发现(service discovery) 什么是 verticles 并且如何使用他们。 怎么使用 Vert.x 事件总线发送，接收消息。 怎么使用 Vert.x 暴露 HTTP 终结点，当然还有怎么消费它们。 怎么组合使用异步操作 怎么在同一个应用中使用多种语言 怎么在 Vert.x 里操作数据库 怎么在异步结果 (asymnc results), 未来返回(future)，异常处理器(exception handlers)和断路器(circuit broakers) 中处理故障 怎么在 Vert.x 里使用 RxJava 还有很多其他的知识，就不一一列举了…​ 2. Vert.x我们会非常简要的介绍一下 Vert.x。你可能还记得，在上一章节，我们已经说过： Vert.x 是 “在JVM上构建反应式程序(reactive applications)的工具箱”。 这里有三个重点： 工具箱, 反应式 和 “基于JVM”. 首先，Vert.x是一个 工具箱。 这是说Vert.x并不是一个应用服务器，一个容器或者一个框架。 它也并不是一个JavaScript开发库。Vert.x是一个朴素的老的 jar 文件，所以一个Vert.x应用程序实际上是一个使用这个 jar 文件的程序。 Vert.x并不强制一个打包的方式。所有Vert.x 模块(components) 都是朴素 平淡 的 jar 文件。 这将怎样影响你的应用程序呢？让我们想象你在使用一个项目构建工具，比方说Maven或者Gradle, 去建立你的应用程许，一个 Vert.x 应用程序，其实就是吧 vertx-core 加入到依赖项里。 你想使用其他的 Vert.x 组件吗？请把它驾到你的依赖项里。这很简单，毫无负担，不是吗。 启动这个程序就是启动一个简单实现了 public static void main(String[] args) 的类。我们不需要任何特殊的IDE或者插件去安装和开始使用 Vert.x。 因此，使用 Vert.x 的精彩特性，你只需要把它写在你的代码里。但是稍等一下，我们很快就会用起来了。 其次， Vert.x 是 反应式 。它就是要用来建立反应式应用程序，或者更贴切的说法是 系统 。 反应式系统 [1] 在 反应式宣言 已经有了很好的定义。这虽然不是一个非常长的文档，我们还是把它缩减到一下四个要点： 及时相应：一个反应式系统需要在 合理 的时间处理请求 (你可以根据应用场景自己定义 合理 )。 有弹性： 一个反应式系统必须在出现 故障 (崩溃，超时， 500 错误 …​) 的情况下，仍然及时作出相应。系统的是 为了故障而设计的 并且能够合理的处理故障。 有灵活性: 一个反应式系统必须在不同负载情况下及时相应。因此，它必须能够向上，向下扩展。并且能够在极小资源的情况下处理一定的负载。 消息驱动: 反应式系统里的各个组件之间，通过 异步消息传递 相互作用。 再次，Vert.x 是一个事件驱动和非阻塞的。事件被投递到一个 永不阻塞的 事件循环 里。为什么呢？不象传统系统，象 “企业级” 系统，Vert.x只使用非常少的线程。 有一些线程是 事件循环, 它们在 处理器（Handlers） 之间派发事件。如果你把某个线程阻塞了，事件将不能继续派发。这个执行模式将影响你如何写代码，不同于 传统mofrl 阻塞代码，你的代码将是异步的 [2] 和非阻塞的 [3] 举一个例子，如果你要得到一个基于URL的资源，你需要这么做： URL site = new URL(&quot;http://vertx.io/&quot;); BufferedReader in = new BufferedReader(new InputStreamReader(site.openStream())); String inputLine; while ((inputLine = in.readLine()) != null) &#123; System.out.println(inputLine); &#125; in.close(); 但是用 Vert.x 的话，你就需要这么做： vertx.createHttpClient().getNow(80, &quot;vertx.io&quot;, &quot;&quot;, response -&gt; &#123; response.bodyHandler(System.out::println); &#125;); 这两段代码有以下不同： 第一个用的是同步调用的并且有可能被阻塞：所有指令都是 顺序 执行的,而且可能把线程阻塞相当长的一段时间(因为网站本身可能会很慢)。 基于 Vert.x 的程序是异步和非阻塞的：在和HTTP服务器建立连接的过程中，线程(事件循环)本身已经被释放了，所以它可以做其他的工作。当响应返回的时候， 那个相同 的事件循环会调用 回调函数。大多数的 Vert.x 组件都是单线程的(只有一个线程去访问它)，所以不需要任何同步机制。另外，使用 Vert.x ，象 DNS 解析这样的事件都是异步和非阻塞的 (其实Java DNS解析是阻塞的)。 最后，Vert.x 应用是运行在 “JVM之上” ，Java虚拟机版本8以上。 这说明 Vert.x 应用程序可以使用任何可以运行在 JVM 上的语言开发。这包括 Java (当然如此), Groovy, Ceylon, Ruby, JavaScript, Kotlin 和 Scala 。 我们甚至可以把所有这些语言混合成任意组合。Vert.x 程序的多语言的特性可以让我们使用在具体任务中使用适合的语言。 Vert.x 让你实现的分布式应用，可以使用内建的 TCP 和 HTTP 服务器和客户端，也可以让你使用 Vert.x 事件总线，这是一个轻量级的收发消息的机制。 使用事件总线，我们可以发送消息到 地址(address) 。它支持一下三种分布模式： 点对点 ： 消息指发送给一个监听这个地址上的 消费者(consumer) 。 发布/订阅 ： 消息会被所有监听在这个地址上的所有 消费者(consumer) 收到。 请求/应答 ： 消息回发送给一个 消费者(consumer) , 它 应答 这个消息并且把另外一个 消息 发送回初始的发送者。 哇唔！，这已经有很多信息需要同学们消化了…​ 但是，你可能还要追问：我用 Vert.x 可以建立什么样的应用？ 我们会说， Vert.x 是难以置信的灵活，简单的网络工具，复杂的现代Web应用， HTTP/REST 微服务，高强度的事件处理或者负载高企的后端消息总线应用，Vert.x 都能非常好的适应。 它真的非常快，不会限制你。最后一点，但不是最无赶紧要的一点， Vert.x提供合适的工具去构建反应式系统；及时响应, 灵活的, 有弹性和异步 的系统。 3. 揭开微服务(microservice)的神秘面纱除非你之前两年一直在山洞里生活，你肯能已经听说过 微服务(microservices) 。 那么，什么是 微服务 呢？ 为了回答这个问题，让我们看看老鸟怎么说： 微服务架构风格是通过一组小服务来构建一个应用程序的方法。每个服务运行在他自己的进程里，它们通过轻量级的机制相互通信，通常是 HTTP 资源 API。 这些服务对应这业务要求，并且每个服务可以独立的使用全自动方式部署。 这里极少需要的中心化管理这些服务。它们很有可能是用不同的编程语言编写，并且有可能使用完全不同的存储技术。 — Martin Fowlerhttp://martinfowler.com/articles/microservices.html 微服务 是一种 架构风格 ，这说明它是一个 元素和关系类型的规范，加上限制条件和如何使用它们的方法 [4]。我相信到此为止，我让你比刚开始更摸不到头脑了。 不用担心。我们从另外的方式来说明。 我们为什么需要 微服务 ？ 三个字 灵活性 [5]。 让我们想象一下，我们有一个挺大的应用。 像任何一个大的应用一样，维护它就像一个噩梦。增加功能需要太长的时间。 很多过时的技术还在线上(什么？Corba已经不时髦了吗？)，任何改变都需要执行一个50个步骤组成的流程，并且需要5个层级的管理组织来确认。 很明显，我们需要几个组在这个应用上工作，它们有不同的需求和日程表。好吧，这个应用就是 怪兽级应用 。 我们怎么才能让开发和维护这个系统更有效呢？ 微服务就是这个问题的一个答案。它就是要减少 上线时间 。 为了做到这一点， 微服务 架构风格建议如下几点： 把应用本身分割成一个相互不耦合的组件集合。每个组件都提供事先定义好的 服务 (定义好的 是说一个已知的接口或者API) 允许组件之间可以选择任意通信协议。REST可能经常用到，但是不是必须的。 允许使用任意的编程语言和技术去实现组件。 允许任何一个组件独立的开发，发布和部署。 允许部署方法是给予组件选择的 流水线 自动化的 完成。 允许整个应用的整体协调机制可以缩减到最小。 在这个实验课，我们不会涉及到第5点，但是你很容易理解 Vert.x 并不会限制你怎么部署你的组件。 你可以应用任何你觉得最适合你的系统环境的方式部署，比方说 ssh, ansible, puppet, docker, cloud, fabric8 甚至是用软盘。 第6点，尽管它很有意思，但同时经常被误解。开发相互独立的几个软件并且神奇地在运行时相互协作，这不是很酷吗？是的，技术上讲，我说的 神奇地 并不是魔术。 为了能够让他们一起工作，我们需要运行时的服务 发现 。服务发现机制可以用各种方式做到。我们能够考虑到的有：对服务地址硬编码(这通常不是一个好主意)，使用 DNS 检索服务，或者其它高科技方法。 有一个服务发现机制存在的情况下，我们系统中的各个组件之间可以在不同位置和环境之间透明的相互交互。我们也可以很容易的在组件之间负载平衡，比方说使用轮叫方式，这也可以在一定程度上提高系统对故障的容忍度(我们可以在一个服务提供者失效的情况下，使用另外一个)。 虽然微服务理论上没有必要是分布式的，但是它经常是分布式的。它具有分布式系统所有的优点和限制：共识算法（FLP），CAP理论，一致性，系统监视，和其他出现故障的可能行。所以，微服务应用需 要从一开始就要涉及如何处理故障。 在我们继续之前，还有几个需要讲的点： 微服务 不 是一个高科技的新概念。70年代，80年代的学术论文已经定义了非常类似的架构风格。当然他们用的是不同的词。 一个需要理解的重点是：微服务不是银弹。除非精心管理，它有可能提高你应用的复杂性，因为这是一个分布式的架构。最后，一个微服务架构不肯能包治百病。 如果微服务可以快速发布，适配好，独立，和可替换的话，我们还会严重担心。每个微服务在建立指出都可以被其他提供相同 服务 / API / 接口 替代。 (本质上，这就是一个 Liskov substitution principle 应用)。 如果你已经干开发10年了，你可能会问微服务和SOA的不同之处。对很多人来说，大小是一个不同点。但这并不准确，一个服务并不一定非常小，“微服务”这个词有时候也会引起误解。微服务和SOA的目的不同，但是有很多相同点： 服务： 一个可以用API，客户端或代理等访问的定义好的功能 服务提供者：一个组件提供了一个服务 服务消费者：一个组件在消费一个服务 服务发现：一个机制用来让消费者找到提供者 这两种概念都是从 面向服务计算 继承下来的。它们都关注解分解和管理独立软件模块。你有可能没有听说过但是可能用过这些技术：COM, Corba, Jini, OSGi, 和 Web Service。它们都是不同的 面向服务计算 的具体实现。 最后，有些人会晤解微服务必须是 RESTful 的。这不是事实。微服务可以依赖不同的交互风格，当然最适合的是： RPC， 事件，消息，流，等等。在这个实验课，我们会使用 RESTful 服务，异步 RPC 和消息源。 4. 微交易员应用现在你了解了 Vert.x和微服务。是时候开始讨论我们需要在实验课实现的程序了。 这是一个假金融程序，它会赚虚拟货币。应用本事是一组微服务构成的: 报价生成器(quote generator) - 这是一个不现实的模拟器，它会生成三个虚构公司的股票报价。MicroHard, Divinator, 和 Black Coat 。市场信息会被发不到 Vert.x 事件总线上。 交易员(traders) - 有几个组件是在接受从报价生成器发布的报价，他们会决定买卖某个特定股票。为了能够作决定，它们依赖 投资组合(portfolio) 服务。 投资组合(portfolio) - 这个服务管理着投资组合里的所有股票和他们的价值。它暴露了一个 服务代理(service proxy), 也就是一个在 Vert.x 事件总线上的异步RPC服务。每一个成功的操作，它都会发送一个消息到事件总线上。它使用报价生成器来计算当前投资组合的价值。 审计(audit) - 这是法律要求的。我们需要需要记录所有的操作。审计组件会从时间总线和地址上接收所有投资组合服务的操作。它会把这些信息写到数据库里。它还会提供一个 REST 端点来让其它组件获得最后的几笔操作。 仪表盘(dashboard) - 这个图形界面会让你看到自己在变得富有。：） 我们可以看一下架构： 这个应用使用了以下集中类型的服务: HTTP 端点(endpoint) - 像 REST API 这个服务可以通过 HTTP URL 找到。 服务代理 - 这是暴露在事件总线上的异步服务。它使用了 RPC 交互机制。这种服务可以通过事件总线上的地址找到。 服务源(Message sources) - 这些组件会像事件总线上发布消息。这种服务可以通过事件总线上的地址找到。 这些组件都在同一个网络环境上运行(在这个实验课里，它们是在同一台机器上的不同进程而已）。 仪表盘会显示所有可用的服务，每个公司的报价，我们交易员最后几笔交易操作，和当前投资组合的状态。它同时还显示不同代理的状态信息。 虽然已经有一些代码显示了 Vert.x 的功能，我们将要亲自实现这个应用的关键部分。需要我们写的代码都被清除的标明 TODO，就像下面的例子： //TODO// ----// your code here// ---- 5. 预先准备我们要学习新知识，马上就要编程序了…​ 但是，在我们开始之前，我们需要安装一些软件。 5.1. 硬件 操作系统: Mac OS X (10.8+), Windows 7+, Ubuntu 12+, CentOS 7+, Fedora 22+ 内存: 至少需要 4 GB+, 最好 8 GB 5.2. Java开发工具包我们需要安装 JDK 8+。 最新的 JDK 可以从一下链接下载: Oracle JDK 8 你可以使用 Oracle JDK 或者 OpenJDK。 5.3. Maven 你可以从这里下载 https://maven.apache.org/download.cgi 。 解压到一个目录下以后，把它驾到你的 PATH 环境变量里。 5.4. IDE我们推荐你使用 IDE. 你可以使用 Eclipse, IntelliJ 或者 Netbeans。 5.4.1. 没有 IDE ?如果你没有 IDE, 一下步骤可以让你准备好 Eclipse。 从这里 下载页面 下载 Eclipse. 在 Eclipse Package 列表, 选择 Eclipse IDE for Java Developers 。他会把你带到一个有 Download 按钮的页面。 下载之后解压。 在目标路径里，你可以找到 Eclipse 执行程序。运行它就好了。 Eclipse 会问你需不需要建立一个工作空间。 打开 Eclipse 之后点 Workbench 箭头 (在右上角). 6. 我们开始 !6.1. 获取代码git clone https://github.com/cescoffier/vertx-microservices-workshop 你可以把代码作为 Maven 导入到 IDE 里。你可以参考 IDE 的文档。 对与 Eclipse: 点击 File - Import …​ - Maven - Existing Maven Projects 选择你克隆源代码的位置 点击 Finish 并且稍等…​ 你会看到几个编译错误。这是因为 Eclipse 默认设置没有把 src/main/generated 作为 源代码跟路径(source root) 。 右键点击 portfolio-service/src/main/generated 并且选择 Build Path → Use as Source Folder. 完整的解决方案在 solution 路径下. 6.2. 编译打包整个构建过程是由 Apache Maven 管理的。 编译整个应用，你只需要： cd vertx-microservices-workshop mvn clean install 我们之前说过， Vert.x 不依赖 Apache Maven 。 你可以使用任何构建工具 (甚至不需要构建工具) 。 6.3. 集群和服务发现基础设施现在差不多可以开始了。如果你还记得在微服务那一章节，我们需要服务发现机制。非常幸运的是， Vert.x 提供了这种机制。 为了让每个组件发现服务，它需要把服务的 记录(record) 存储在某个可以访问到的地方。 默认配置情况下， Vert.x 服务发现机制会使用分布式映射(map)，它可以被所有的集群成员访问到。 所以当你启动 Vert.x 应用程序并且开启集群模式的时候，它会加入一个集群。 这个集群里的节点都可以： 加入和离开这个集群，也就是管理 成员(menber) 路径。(成员 != 服务) 通过分布式映射，锁，计数器等分享数据 访问事件总线 在我们的上下文里，我们不需要配置什么。这个实验课材料里已经给出了集群的配置。它是基于单点通讯并使用 127.0.0.1 IP 地址。(因此你不能在相邻的计算机上找到这些服务)。这个集群是基于 Hazelcast。如果你感兴趣的话，请查看配置文件 vertx-workshop-common/src/main/resources/cluster.xml。 7. 第一个微服务 - 报价生成器免责声明，这个报价生成器纯属虚构，报价全部是随机生成…​ 7.1. 项目结构让我们看一下这个项目，所有项目都有类似的结构。 ├── README.md &lt;--- 组件的描述 ├── pom.xml &lt;--- Maven 文件 └── src ├── conf │ └── config.json &lt;--- 项目配置文件，应用程序在启动时要加载配置项 └── main └── java └── io.vertx.workshop.quote ├── GeneratorConfigVerticle.java &lt;--- 竖直体(Vertical) ├── QuoteVerticle.java └── RestQuoteAPIVerticle.java 让我们从 pom.xml 文件开始。文件定义了 Maven 如何编译打包： 定义依赖项 编译java代码和处理资源(如果有的话) 构建胖jar包 fat-jar 一个胖jar包 fat-jar (也叫 uber jar 或者 shaded jar) 是一个方便打包 Vert.x 应用程序的方法。 它生成一个包含应用程序和所有依赖项的 uber-jar ，当然也有 Vert.x。然后，你只需要 java -jar 命令来启动这个程序，这样省去了 CLASSPATH 的配置。 等一下，我们说过，Vert.x 并不需要特定的打包方式。这确实是对的。 胖jar包只是为了方便，你也可以用普通jar包，OSGi 捆(budles)。 创建出的 fat-jar 使用了一个相同的主类，它 (io.vertx.workshop.common.Launcher) 在 vertx-workshop-common 里已经提供了。 这个 Launcher 类会启动 Vert.x 实例，配置它，并且部署 主竖直体(main-verticle)。 在此说明，这只是为了方便，你可以写你自己的 主(main) 类。 7.2. 竖直体(Verticle)你也许已经注意到了，代码由三个 竖直体(verticles) 组成。但是它们是什么？竖直体是一种 Vert.x 应用代码的结构。 这不是必须的，但是这确实非常方便。一个竖直体是部署在 Vert.x 实例之上的一段代码。 一个竖直体可以访问它部署到的 vertx 实例。它也可以部署其他竖直体。 让我们打开 GeneratorConfigVerticle 类，并且看一下 start 方法。 @Override public void start() &#123; super.start(); JsonArray quotes = config().getJsonArray(&quot;companies&quot;); for (Object q : quotes) &#123; JsonObject company = (JsonObject) q; // 使用配置参数启动竖直体 vertx.deployVerticle(MarketDataVerticle.class.getName(), new DeploymentOptions().setConfig(company)); &#125; vertx.deployVerticle(RestQuoteAPIVerticle.class.getName()); publishMessageSource(&quot;market-data&quot;, ADDRESS, rec -&gt; &#123; if (!rec.succeeded()) &#123; rec.cause().printStackTrace(); &#125; System.out.println(&quot;Market-Data service published : &quot; + rec.succeeded()); &#125;); publishHttpEndpoint(&quot;quotes&quot;, &quot;localhost&quot;, config().getInteger(&quot;http.port&quot;, 8080), ar -&gt; &#123; if (ar.failed()) &#123; ar.cause().printStackTrace(); &#125; else &#123; System.out.println(&quot;Quotes (Rest endpoint) service published : &quot; + ar.succeeded()); &#125; &#125;); &#125; 竖直体可以通过 config() 方法得到配置参数。这里它的到了需要模拟的公司的详细信息。 配置参数是通过 JsonObject 传递的。 Vert.x 经常使用 JSON，所以你将在这个实现课里看到非常多的 JSON。 对每个配置参数里的公司，这个竖直体都会部署另一个相应的竖直体，传入提取的配置参数。 最后，这个竖直体，部署另外一个竖直体来提供一个简单的 HTTP API。 这个函数的剩余部分和服务发现有关系。我们在微服务章节提到过服务发现。 这个组件生成报价信息，并且发送到事件总线上。为了让其他的组件得知消息发送到什么地方 (消息发送地址），它注册地址。 market-data 就是这个服务的地址， ADDRESS 是事件总线的地址，消息将发送到这个地址上。 最后一个参数是一个 处理器(Handler)， 它会在注册完成之后被触发。 handler 会收到一个 AsyncResult 结构体。 请记住， Vert.x 是在提倡异步，非阻塞开发风格。发布服务会花时间 (实际上，它会建立一个记录，并把这个记录写到后端，然后通知所有人), 我们不能阻塞事件循环，所以这个方法也是异步的。异步方法都会有一个 Handler 作为最后一个参数，它会在操作完成之后被调用。 这个 Handler 收到一个 AsyncResult，表示操作成功与否。你将要在 Vert.x 里看到很多这样的模式： // 返回 X 类型的一个对象的异步方法。 operation(param1, param2, Handler &lt; AsyncResult &lt; X &gt;&gt; ); // 收到X 类型的一个对象的处理器。 ar -&gt; &#123; if (ar.succeeded()) &#123; // 用 X 继续做工作。 X x = ar.result(); &#125; else &#123; // 操作失败了。 Throwable cause = ar.cause(); &#125; &#125; 如果你还记得程序的架构，报价生成器也会提供一个 HTTP 端点来返回报价的最新值 (你会实现它)。 像上一个服务一样，它需要被发布。为了发布，我们给出了它的详细位置 (服务器地址，端口…​): publishHttpEndpoint(&quot;quotes&quot;, &quot;localhost&quot;, config().getInteger(&quot;http.port&quot;, 8080), ar -&gt; &#123; if (ar.failed()) &#123; ar.cause().printStackTrace(); &#125; else &#123; System.out.println(&quot;Quotes (Rest endpoint) service published : &quot; + ar.succeeded()); &#125; &#125;); 7.3. REST 报价服务端点是时候开发这个程序的一些部分了(你手里已经有需要工具了)。请打开 RestQuoteAPIVerticle。这是一个从 AbstractVerticle 扩充的竖直体。 在 start 方法，你需要： 注册一个事件总线的消费者来收集最新的报价信息 ( 从`quotes` 映射)。 接收 HTTP 请求，并且返回报价列表，或者当 name 被指定的情况下返回单个公司的报价。 开工…​. 7.3.1. 任务 - 实现一个接收事件的处理器第一个任务是建立一个 处理器(Handler)，这是一个事件到达时被调用的方法。处理器是 Vert.x 非常重要的的部分，所以理解它们很重要。 在这个任务里，任何一个消息送达到 address 指定的事件总线(它接收每一个从报价生成器产生的报价)时，Handler 就会被调用。 他是一个 消息消费者 (message consumer) 。 message 参数就是收到的消息。 实现的逻辑就是收到消息体(通过 body() 方法)。然后获取消息体中报价的 name， 然后在 quotes 映射里添加一个 name → quote 的条目。 JsonObject quote = message.body(); // 1 quotes.put(quote.getString(&quot;name&quot;), quote); // 2 首先，它得到了消息体 (1)。这是一个 JSON 对象。然后把它存储到 quotes 映射里 (2)。 7.3.2. 任务 - 实现一个处理 HTTP 请求的处理器现在你已经实现了第一个 Handler，让我们实现第二个。这个处理器不从事件总线获得消息，而是获得 HTTP 请求。 这个服务器绑定在 HTTP 服务器上。它会在每次 HTTP 请求到达这个服务器的时候背调到。它负责写响应。 为了处理 HTTP 请求，我们需要 HTTP 服务器。幸运的是，Vert.x 可以让你用以下方法建立 HTTP 服务： vertx.createHttpServer().requestHandler(request -&gt; &#123;...&#125;).listen(port, resultHandler); 请求处理器会做以下步骤来产生响应的内容： Write the content of the request handler to respond to the request: 请求需要在消息头里把 content-type 设置成 application/json。 获取 name 参数，这是一个公司名。 如果公司名没有设置，通过 json 格式返回所有的报价。 如果公司名设置了，返回存储的报价信息，或者在不知道公司名的情况下返回 404。 我们可以通过 request.response() 拿到一个请求的响应。 我们通过 response.end(content) 给响应写入内容。 为了生成 JSON 表示的对象，我们使用 Json.encode 方法。 HttpServerResponse response = request.response().putHeader(&quot;content-type&quot;, &quot;application/json&quot;); String company = request.getParam(&quot;name&quot;); if (company == null) &#123; String content = Json.encodePrettily(quotes); response.end(content); &#125; else &#123; JsonObject quote = quotes.get(company); if (quote == null) &#123; response.setStatusCode(404).end(); &#125; else &#123; response.end(quote.encodePrettily()); &#125; &#125; 从 request 对象里拿到 response 获得 name 参数 (查询参数) 把映射编码成 JSON 格式 用 end(…​) 把响应写入并刷新它 如果名字不能匹配出一个公司，设置状态值 404 你有可能疑惑为什么不需要同步。实际上，我们读写映射时，没有使用任何同步机制。 这其实是 Vert.x 的一个主要特性： 所有这些代码会在 同一个 事件循环中运行。所以它永远被 同一个 线程访问。不会有有并发。 Map&lt;String, JsonObject&gt; 也可以被简单替换成 JsonObject 他基本上就是 Map&lt;String, Object&gt;。 7.4. 是时候启动报价生成器了现在，我们把这个微服务编译打包成 fat-jar. 在终端: cd quote-generator mvn package 然后，打开一个新的终端并且启动它： java -jar target/quote-generator-1.0-SNAPSHOT-fat.jar 这个命令启动了应用程序。主类创建了一个集群化的 Vert.x 实例，并且从 src/conf/config.json 里读取配置参数。 这个配置提供了 REST 服务需要的 HTTP 端口 (35000)。 我们可以打开浏览器看一下链接 http://localhost:35000 。 返回的内容看起来应该是： &#123; &quot;MacroHard&quot;: &#123; &quot;volume&quot;: 100000, &quot;shares&quot;: 51351, &quot;symbol&quot;: &quot;MCH&quot;, &quot;name&quot;: &quot;MacroHard&quot;, &quot;ask&quot;: 655.0, &quot;bid&quot;: 666.0, &quot;open&quot;: 600.0 &#125;, &quot;Black Coat&quot;: &#123; &quot;volume&quot;: 90000, &quot;shares&quot;: 45889, &quot;symbol&quot;: &quot;BCT&quot;, &quot;name&quot;: &quot;Black Coat&quot;, &quot;ask&quot;: 654.0, &quot;bid&quot;: 641.0, &quot;open&quot;: 300.0 &#125;, &quot;Divinator&quot;: &#123; &quot;volume&quot;: 500000, &quot;shares&quot;: 251415, &quot;symbol&quot;: &quot;DVN&quot;, &quot;name&quot;: &quot;Divinator&quot;, &quot;ask&quot;: 877.0, &quot;bid&quot;: 868.0, &quot;open&quot;: 800.0 &#125; &#125; 它给了当前报价的详情。报价将会每3秒更新一次。你可以刷新你的浏览器来得到最新的数据。 我们可以启动仪表盘。在另外一个终端，进入 $project-home/trader-dashboard 路径并且执行下面的命令： mvn clean package java -jar target/trader-dashboard-1.0-SNAPSHOT-fat.jar 接着，打开浏览器访问 http://localhost:8080。你应该能看到： 有一些部分还没有内容，在一开始这不意外…​ 7.5. 你不是金融专家 ?也许你没有习惯金融和他的专业名字…​ 我也没有，我们讨论的范围金融是非常简化的版本。 为了方便理解，让我们定义哪些非常重要的字段。 name : 公司名 symbol : 公司名缩写 shares : 股票的数量，我们可以购买它 open : 股票在开盘时的价格 ask : 股票在你买的时候的价格，既卖价 bid : 股票在你卖的时候的价格，既买价 你可以访问 Wikipedia 得到更详细的说明。 8. 事件总线服务 - 投资组合服务在报价生成器里，我们已经尝试了一些基础的 Vert.x 开发： 异步 API 和 AsyncResult 实现处理器 Handler 从事件总线上接收消息 在投资组合组件里，我们将要实现一个 事件总线服务(event bus service)。一个 投资组合(Portfolio) 存储了拥有的股份和可用的现金。 8.1. 远程过程调用(RPC) 和异步远程过程调用微服务不仅是关于 REST。它可以用不同的交互操作方式暴露出来。 并且 远程过程调用 就是其中的一种。 通过 RPC ，一个组件可以使用本地调用有效的发送一个请求到另外一个组件，请求会被打包到一个消息里并且发送给被调用者。 结果会以类似的方式发送回调用者作为过程调用的结果。 这样的互操作的好处是强类型。这比非结构化的消息减少了出问题的可能性。但是，这使调用者和被调用者之间的耦合增强了。调用者要知道怎么调用： 怎么调用这个服务 这个服务运行在哪里？ 传统的 RPC 有一个讨厌的缺点： 调用者必须等待，直到接收到应答。一次调用至少需要两个网络消息，这明显是一个阻塞的调用。 另外，这不能很好的处理分布式通信中各个环节都可能出现的故障。 幸运的是，Vert.x 提供了一个不同方式的 RPC ：异步 RPC。 异步 RPC 和同步 RPC 有相同的原则。 但是，它不会 等 响应，它传入一个 Handler&lt;AsymcResult&lt;X&gt;&gt;，以便在结果收到以后调用。 AsyncResult 会通知 Handler 这次调用是成功了还是失败了。当成功返回时，处理器会得到结果。 这种异步 RPC 有几个有点： 调用者不会被阻塞 它可以处理故障 你可以不用在事件总线收发消息。底层实现上，它帮助你组包和解包。 8.2. 异步服务接口建立一个异步 RPC 的服务，或者事件总线的服务，或者一个服务代理。你首先需要一个 Java 接口来声明异步方法。 请打开 io.vertx.workshop.portfolio.PortfolioService 类看一下。 这个类被标注成： ProxyGen - 开启事件服务代理和服务器的代码生成 VertxGen - 开启多 Vert.x 语言支持代理的代码生成。 我们看一下第一个方法： void getPortfolio(Handler&lt;AsyncResult&lt;Portfolio&gt;&gt; resultHandler); 这个方法帮助你获取一个 Portafolio 对象。就像上面解释的那样，这个消息是异步的，所以它需要一个 Handler 参数来接收 AsyncResult&lt;Portfolio&gt;。 其他的方法也都使用这种模式。 你可能已经注意到这个包里有 package-info.java 文件。 这个文件是用来开启服务代理代码生成的。 8.3. 数据对象Portfolio 对象是一个数据对象。事件总线代理支持有限的类型，对于不支持的类型，它必须使用 数据对象(data object) (请查看 文档知道所有支持的类型)。 数据类型是 Java 类型，它们遵循以下相知： 他必须标注成 DataObject 它必须有一个空构建函数，一个拷贝构建函数，和一个基于 JsonObject 的构建函数 它需要有一个 toJson 方法来构建相应的 JsonObject 字段必须是有 属性(property) (有 getters and setters) 我们可以打开 io.vertx.workshop.portfolio.Portfolio 类，看一下它的内容。正如你看到的，所有的 JSON 处理都是通过自动生成的 converters 完成的。 所以一个数据对象非常接近一个简单的 bean。 8.4. 实现服务完美，我们已经有了一个服务的异步接口。我们需要实现服务本身。这里，我们需要实现三个方法： getPortfolio 如何返回 AsyncResult 对象 sendActionOnTheEventBus 来发送异步消息到事件总线上 evaluate 计算当前投资组合的当前价值 8.4.1. 任务 - 创建 AsyncResult 实例正如我们已经看到的，我们的异步服务有 Handler&lt;AsyncResult&lt;Portfolio&gt;&gt; 参数。 所以当我们实现这个服务的时候， 我们需要使用 AsyncResult 类调用 Handler。 要想知道这是怎么工作的，让我们实现 getPortfolio 方法吧： 在 io.vertx.workshop.portfolio.impl.PortfolioServiceImpl，填充 getPortfolio 方法. 它应该调用 resultHandler 的 handle 方法，传入一个 成功 (sucessful) 异步结果。这个对象可以通过 Vert.x Future 方法获得。 resultHandler.handle(Future.succeededFuture(portfolio)); 哇唔…​ 只需要一行代码而已？让我们解析以下： resultHandler.handle : 这是在调用 Handler. Handler&lt;X&gt; 只有一个方法 (handle(X)). Future.succeededFuture : 这是构建一个表明成功的 AsyncResult 实例，传递的值是一个返回结果 (portfolio) 等一下，AsyncResult 和 Future`有什么关系？ 一个 `Future 表示一个行动的结果，可能成功，可能失败，还没有确定。 如果 Future 只是为了用来表明成功或者失败的状态，结果可能是 null。 Future 对象背后的操作可能成功或者失败。 AsyncResult 是一个表示一个操作成功和失败的结构。所以， Future 是一种 AsyncResult。 在 Vert.x里， AsyncResult 实例是通过 Future 类型构建的。 AsyncResult 描述了: 成功返回，它会封装一个结果 一个故障，它会封装一个 Throwable 实例 你知道吗？术语 Future 早在1977就有了, Promise 是在 1976…​ 都不是新鲜事了。 那么，这些怎么和异步 RPC 工作的呢？让我们看一下时序图： 8.4.2. 任务 - 在事件总线上发送事件在上一章，我们注册了一个接收事件总线服务的消费者。现在是时候知道如何发送消息了。我们通过 vert.x.eventBus() 访问事件总线。 通过这个对象，我们可以： send : 发送点对点消息 publish : 把一个消息广播给所有注册在这个地址的消费者 send 并传入 Handler&lt;AsyncResult&lt;Message&gt;&gt;&gt; 参数: 发送一个点对点消息，并且期望有一个返回。 在最后一点，请注意 AsyncResult&lt;Message&gt;。这是一个异步结果，也可能永远不会到达 (这会被认为是故障)。 好，返回我们的代码。我们已经提供了 buy 和 sell 方法，它们只是在买卖股份的时候做一些检查而已。 一旦买卖动作发出了，我们会向事件总线发送消息，它们会被 审计服务 和 仪表盘服务 同时消费。 所以，我们会使用 publish 方法。 要写 sendActionOnTheEventBus 的函数体，为了能够在 EVENT_ADDRESS 地址上发布一个包含 JsonObject 作为消息体的消息。 这个对象必须包含以下条目： action → 动作 (买或者卖) quote → 报价， Json格式 date → 日期 (long 型，以毫秒为单位) amount → 数量 owned → 拥有这个股票的最新数量 vertx.eventBus().publish(EVENT_ADDRESS, new JsonObject() .put(&quot;action&quot;, action) .put(&quot;quote&quot;, quote) .put(&quot;date&quot;, System.currentTimeMillis()) .put(&quot;amount&quot;, amount) .put(&quot;owned&quot;, newAmount) ); 让我们深入探讨以下： 它得到了 EventBus 实例，并且在上面调用 publish. 第一个参数 address 是消息发送的地址。 消息体是一个 JsonObject，它包含这个动作的所有信息。 (买卖，报价 (这是另一个 JSON 对象), 日期…​） 8.4.3. 任务 - 协调异步方法和消费 HTTP 端点 - 投资组合估值要实现的最后一个方法是 evaluate。这个方法计算当前投资组合的估值。 为了能够估值，这个函数需要访问股票的当前价值 (最后的报价)。这就需要消费 HTTP 端点，它是现在报价生成器里。 为此，你要： 发现这个服务。 调用这个服务，来获得每一个你拥有股票的报价。 当所有的调用都结束了，计算估值并且把估值返回给调用者。 这有一点儿棘手。让我们一步一步来。第一步，在 evaluate ，我们需要拿到报价生成器提供的 HTTP 端点 (服务)。 这个服务的名字叫 quotes。我们在上一章已经发布了这个服务。因此，我们直接获得这个服务。 填充 evaluate 方法来获得 quotes 服务。我们可以通过 HttpEndpoint.getWebClient 得到 HTTP 服务。 服务的名字 name 是 quote 。如果你得不到这个服务，我们会得到一个异步的失败结果给处理器。 负责，调用 computeEvaluation。 HttpEndpoint.getWebClient(discovery, new JsonObject().put(&quot;name&quot;, &quot;quotes&quot;), client -&gt; &#123; if (client.failed()) &#123; // 失败了... resultHandler.handle(Future.failedFuture(client.cause())); &#125; else &#123; // 我们得到了一个客户端 WebClient webClient = client.result(); computeEvaluation(webClient, resultHandler); &#125; &#125;); 获得服务的 Web 客户端。 客户端没有得到 (服务没有找到)，报错 我们拿到了客户端，继续…​ 这里是 computeEvaluation 方法的实现： // 每一家公司都需要调用一次服务 List&lt;Future&gt; results = portfolio.getShares() .entrySet().stream().map(entry -&gt; getValueForCompany(webClient, entry.getKey(), entry.getValue())) .collect(Collectors.toList()); // 我们只能在所有结果都得到以后返回，为此我们建立了组合预期。这是个集合处理器(set handler) // 在所有预期都被赋值以后调用 CompositeFuture.all(results).setHandler(ar -&gt; &#123; double sum = results.stream().mapToDouble(fut -&gt; (double) fut.result()).sum(); resultHandler.handle(Future.succeededFuture(sum)); &#125;); 首先，我们得到了一个 Future 列表，每个都会得到一家公司的股票估值 (1)。 这些估值是异步到达的 (他调用 HTTP 服务获得最新值)。我们不知道什么时候所有这些 Future 都返回并且赋值了。 幸运的是， Vert.x 为这种情况提供了 CompositeFuture (2)。 CompositeFuture.all 当所有的 Future 都赋值返回之后，它注册的的处理器会被调用。 所以当处理器被调用的时候，我们知道所有的预期都已经得到了数值，我们可以计算总和了 (3) 。 最终，我们使用 resultHandler 把这个总和发送给客户端 (4)。 好，我们只是用 getValueForCompany 方法来调用服务。下一步我们会写这个方法的内容。我们会需要建立一个 Future 对象，通过它报告操作完成了。 这个预期是这个方法的 “返回” 结果。然后，它会调用 HTTP 端点(/?name= + encode(company))。 当应答返回时，检查状态 (应该是 200) 然后获得消息体 (用 bodyHandler)。这个消息题会被用 buffer.toJsonObject() 处理成 JsonObject。 估值是通过 numberOfShares * bid 价格 (从消息体获得)。估值计算出来后，完成 future。 别忘了，如果有故障，我们需要把故障也上报给 future。为了简单，如果这个公司我们不知道 (应答的状态 不 是 200)，我们假设股票的估值是 0.0。 private Future&lt;Double&gt; getValueForCompany(WebClient client, String company, int numberOfShares) &#123; // 创建预期对象，它将要在收到估值的时候得到赋值 Future&lt;Double&gt; future = Future.future(); client.get(&quot;/?name=&quot; + encode(company)).as(BodyCodec.jsonObject()).send(ar -&gt; &#123; if (ar.succeeded()) &#123; HttpResponse&lt;JsonObject&gt; response = ar.result(); if (response.statusCode() == 200) &#123; double v = numberOfShares * response.body().getDouble(&quot;bid&quot;); future.complete(v); &#125; else &#123; future.complete(0.0); &#125; &#125; else &#123; future.fail(ar.cause()); &#125; &#125;); return future; &#125; 首先，我们建立一个 Futuer 对象，这将是这个方法的返回值 (1)。然后，我们调用 Web 客户端的 get 方法获得公司最新的报价 (2)。 get 准备了请求消息，但是在 send 调用之前，不会发出消息。这个 Web 客户端已经配置了正确的 IP 和端口 (服务发现可以做到这些)。 然后，我们可以读消息体并且计算估值。做完这些，我们就把估值赋值给 Future (3)。 如果找不到这个公司，他就是估值为 0.0 (4)。 如果收发消息的过程中有什么错误发生，我们用 Throwable 给 future 设置错误状态 (5)。 8.5. 任务 - 发布一个服务现在，服务的实现已经完成。让我们发布它吧！首先，我们需要一个 vertical 来建立这个具体的·服务对象， 在事件中线上注册这个服务并且发布这个服务到服务发现基础设施上。 打开 io.vertx.workshop.portfolio.impl.PortfolioVerticle 类。start 方法就做了我们所说的步骤: 建立服务对象: PortfolioServiceImpl service = new PortfolioServiceImpl(vertx, discovery, config().getDouble(“money”, 10000.00)); 在事件总线上注册服务，我们用到了 ProxyHelper: ProxyHelper.registerService(PortfolioService.class, vertx, service, ADDRESS); 在服务发现基础设施上发布服务，让其他人知道它的存在： publishEventBusService(“portfolio”, ADDRESS, PortfolioService.class, ar -&gt; { if (ar.failed()) { ar.cause().printStackTrace(); } else { System.out.println(“Portfolio service published : “ + ar.succeeded()); }}); publishEventBusService 的实现如下： // 建立服务记录 Record record = EventBusService.createRecord(name, address, serviceClass); // 发布这个服务记录 discovery.publish(record, ar -&gt; &#123; if (ar.succeeded()) &#123; registeredRecords.add(record); completionHandler.handle(Future.succeededFuture()); &#125; else &#123; completionHandler.handle(Future.failedFuture(ar.cause())); &#125; &#125;); 我们做完了吗？不…​ 我们还有一个服务需要发布。记住，当买卖股份的时候，我们同时在服务总线上发送消息。这也是一个服务 (准确的讲，它是一个消息源服务)。 在 start 方法的末尾，写一些代码来发布 portfolio-events 服务。EVENT_ADDRESS 是事件服务总线地址。 对于不同的服务类型，有不同的 publish 方法 publishMessageSource(&quot;portfolio-events&quot;, EVENT_ADDRESS, ar -&gt; &#123; if (ar.failed()) &#123; ar.cause().printStackTrace(); &#125; else &#123; System.out.println(&quot;Portfolio Events service published : &quot; + ar.succeeded()); &#125; &#125;); 现在，我们做完了，我们需要编译打包和运行这个服务。 8.6. 运行了 !编译打包： cd portfolio-service mvn clean package 然后，在另一个终端启动它： java -jar target/portfolio-service-1.0-SNAPSHOT-fat.jar 看，投资组合服务启动了。它发现了 quotes 报价服务，并且随时提供服务。 我们回到仪表盘，你应该可以看到一些新的服务，而且左上角会显示 现金(cash) 。 仪表盘通过异步 RPC 机制消费投资组合服务。编译的时候，基于 JavaScript 的客户端已经生成了，页面可以使用 SockJS 的通信机制和他沟通。 实现层上，事件总线和 SockJS 之间还有一个桥接器。 好吧，是时候，买卖一些股份了。请关注下一章。 9. 算法交易在 投资组合(portfolio) 项目里，我们已经实现了管理投资组合的事件总线服务。 在报价生成器里，我们把报价发送给事件总线。 交易员程序是消费这两个服务的组件，它只有一个目的：赚钱(或者赔钱…​)! 在这一章，我们将要开发两个交易员程序 (它们的交易逻辑非常简单愚蠢，我们显然可以做的更好)： 第一个交易员程序是用 Java 开发的 第二个交易员程序是用 Kotlin 开发的 9.1. 愚蠢的算法交易员在我们开始实现它之前，让我们介绍以下它们显然不符合逻辑的交易算法。 算法交易员随机挑选一个公司名字，随机选择股份数量 (x)。 他随机买或者卖这家公司的 x 个股份。 它不检查他是不是有足够的股份或者现金，他只是试着买卖…​ 这个逻辑在 io.vertx.workshop.trader.impl.TraderUtils 里实现了。 9.2. 部署几个使用不同语言实现的竖直体compulsive-trader 项目包含了一个 主竖直体 (io.vertx.workshop.trader.impl.MainVerticle) 它即将配置和部署这些竖直体： @Overridepublic void start() throws Exception &#123; // Java 交易员 vertx.deployVerticle(JavaCompulsiveTraderVerticle.class.getName(), new DeploymentOptions().setInstances(2)); // Kotlin 交易员 ... vertx.deployVerticle(&quot;GroovyCompulsiveTraderVerticle.groovy&quot;); &#125; JavaCompulsiveTraderVerticle 在部署时传入了参数 DeploymentOptions (1)。 配置里的 实例数量 默认是 2, 所以 Vert.x 不止一次实例化这个竖直体，它被实例话了两次 (生成了两个对象)。 所以以上代码部署了三个交易员。 Groovy 竖直体是通过文件名部署的。竖直体文件是在 src/main/resources 路径下，会被移到胖 jar 包的根路径下。 现在是时候实现这些竖直体了。 9.3. Java 版的交易员程序打开 io.vertx.workshop.trader.impl.JavaCompulsiveTraderVerticle 类，在 TODO 的位置，需要： 初始化交易员 获取我们需要的两个服务 当两个服务都获得到的时候，在每一个市场数据到达的时候应用交易逻辑买卖股票 9.3.1. 任务 - 1. 初始化交易员首先，我们看一下 start 方法的定义：start(Future&lt;Void&gt; future) 。 这个 future 让我们在初始化结束的时候通知成功与否。 我们必须显式的完成这个 future 或者让他失败。 为了初始化这个交易员，去掉 future.fail 语句，并且使用 TraderUtils 初始化 company 和 numberOfShares 变量。 String company = TraderUtils.pickACompany(); int numberOfShares = TraderUtils.pickANumber(); System.out.println(&quot;Java compulsive trader configured for company &quot;+company +&quot; and shares: &quot;+numberOfShares); 9.3.2. 任务 - 2. 获得几个服务交易员需要 Portfolio 服务和 market 服务 (发送市场数据的消息源)。 我们需要在拿到这两个服务之后才能启动交易逻辑。 这里我们使用到了上一章提到 Future组合。 紧跟着我们已经写的代码，写下获得两个服务的代码。当两个服务都获得以后 (使用 all 组合操作)，什么也不用做。handler 会在下一步填入 ： // 我们需要获得两个服务，建立两个预期对象来获得服务 Future&lt;MessageConsumer&lt;JsonObject&gt;&gt; marketFuture = Future.future(); Future&lt;PortfolioService&gt; portfolioFuture = Future.future(); // 获得服务，使用 &quot;特定&quot; 完成来给预期的赋值 MessageSource.getConsumer(discovery, new JsonObject().put(&quot;name&quot;, &quot;market-data&quot;), marketFuture.completer()); EventBusService.getProxy(discovery, PortfolioService.class, portfolioFuture.completer()); // 当一切结束，(服务都得到以后), 设置处理器 CompositeFuture.all(marketFuture, portfolioFuture).setHandler(ar -&gt; &#123; // 下一步.... &#125;); 首先，我们建立两个 Future 对象，它们会在得到服务之后收到结果 (1)。 在 (2) 我们获得消息源服务并且使用特定的 Handler 来赋值 Future。 completer本质上是一个 Handler，它得到结果并且八只设置给 Future 或者把它置为失败。 我们在 (3) 中使用了相同的方法来获取 Portfolio 服务。 最终，在 (4)，我们建立了 CompositeFuture，它会在所有列表里的 Future 都被赋值以后，调用相应的 Handler 9.3.3. 任务 - 3. 应用交易逻辑 基本上要做完了！现在我们写最后一个处理器。如果获取服务失败，我们在 future 上报告故障。 负责，在市场服务上注册一个消息消费者。每次你得到一个消息，应用 TraderUtils.dumbTradingLogic 方法。 接着，我们完成 future。 if (ar.failed()) &#123; future.fail(&quot;One of the required service cannot &quot; + &quot;be retrieved: &quot; + ar.cause()); &#125; else &#123; // 我们的服务: PortfolioService portfolio = portfolioFuture.result(); MessageConsumer&lt;JsonObject&gt; marketConsumer = marketFuture.result(); // 监听市场行情... marketConsumer.handler(message -&gt; &#123; JsonObject quote = message.body(); TraderUtils.dumbTradingLogic(company, numberOfShares, portfolio, quote); &#125;); future.complete(); &#125; 首先，我们需要检查服务成功获得 (1)。如果成功了，我们发 Future 对象打开，取出服务 (2)。 我们给 marketConsumer 消息源服务设置一个 Handler 来执行交易逻辑 (4)。 最终，在第 (5) 行，我们完成 start 的传入参数 future。这里没有任何 结果 ，我们只是通知这个完成状态。 同样，请注意 future.fail 是在标注一个初始化失败。 9.3.4. 一小段代码这里是完整的代码： super.start(); String company = TraderUtils.pickACompany(); int numberOfShares = TraderUtils.pickANumber(); System.out.println(&quot;Java compulsive trader configured for company &quot; + company + &quot; and shares: &quot; + numberOfShares); // 我们需要获得两个服务，建立两个预期对象来获得服务 Future&lt;MessageConsumer&lt;JsonObject&gt;&gt; marketFuture = Future.future(); Future&lt;PortfolioService&gt; portfolioFuture = Future.future(); // 获得服务，使用 &quot;特定&quot; 完成来给预期的赋值 MessageSource.getConsumer(discovery, new JsonObject().put(&quot;name&quot;, &quot;market-data&quot;), marketFuture.completer()); EventBusService.getProxy(discovery, PortfolioService.class, portfolioFuture.completer()); // 当一切结束，(服务都得到以后), 设置处理器 CompositeFuture.all(marketFuture, portfolioFuture).setHandler(ar -&gt; &#123; if (ar.failed()) &#123; future.fail(&quot;One of the required service cannot &quot; + &quot;be retrieved: &quot; + ar.cause()); &#125; else &#123; // 我们的服务： PortfolioService portfolio = portfolioFuture.result(); MessageConsumer&lt;JsonObject&gt; marketConsumer = marketFuture.result(); // 监听市场行情... marketConsumer.handler(message -&gt; &#123; JsonObject quote = message.body(); TraderUtils.dumbTradingLogic(company, numberOfShares, portfolio, quote); &#125;); future.complete(); &#125; &#125;); 9.3.5. 运行交易员我们可以运行交易员程序，看一下它是怎么进行市场交易的。先编译打包： cd compulsive-traders mvn clean package 然后启动这个程序： java -jar target/compulsive-traders-1.0-SNAPSHOT-fat.jar 如果你回到仪表盘，你可以看到投资组合的一些变化。 9.4. 任务 - 写一个 Groovy 竖直体这个 Groovy 交易员使用的是相同的交易逻辑，但是，这个竖直体是用 Groovy 语言开发的。为了方便理解，代码和 Java 版的十分类似。 打开 src/main/resources/GroovyCompulsiveTraderVerticle.groovy。这个竖直体是 Groovy 脚本。 所以竖直体的 start 方法就是这个脚本的内容. Vert.x 是支持 Groovy 类型的. 如果你不了解 Groovy，可以直接把方案拷贝粘贴。如果你了解 Groovy, 你可以尝试按照 Java 交易员的逻辑实现以下交易员。 Groovy 版本的 Vert.x API 都在 io.vertx.groovy.x.y 里，比方说 io.vertx.groovy.core.CompositeFuture。 Json 对象就是 Groovy 映射 (map)，所以在 Java 里的 MessageConsumer&lt;JsonObject&gt; 就是 Groovy 里的 MessageConsumer&lt;Map&gt;。 import io.vertx.core.CompositeFuture import io.vertx.core.Future import io.vertx.core.eventbus.MessageConsumer import io.vertx.core.json.JsonObject import io.vertx.servicediscovery.ServiceDiscovery import io.vertx.servicediscovery.types.EventBusService import io.vertx.servicediscovery.types.MessageSource import io.vertx.workshop.portfolio.PortfolioService import io.vertx.workshop.trader.impl.TraderUtils def company = TraderUtils.pickACompany(); def numberOfShares = TraderUtils.pickANumber(); println(&quot;Groovy compulsive trader configured for company &quot; + company + &quot; and shares: &quot; + numberOfShares); // 建立发现服务对象。 def discovery = ServiceDiscovery.create(vertx); Future&lt;MessageConsumer&lt;Map&gt;&gt; marketFuture = Future.future(); Future&lt;PortfolioService&gt; portfolioFuture = Future.future(); MessageSource.getConsumer(discovery, JsonObject.mapFrom([&quot;name&quot;: &quot;market-data&quot;]), marketFuture.completer()); EventBusService.getProxy(discovery, Class.forName(&quot;io.vertx.workshop.portfolio.PortfolioService&quot;), portfolioFuture.completer()); // 成功结束以后 (所有服务都得到了), 执行处理器代码 CompositeFuture.all(marketFuture, portfolioFuture).setHandler( &#123; ar -&gt; if (ar.failed()) &#123; System.err.println(&quot;One of the required service cannot be retrieved: &quot; + ar.cause()); &#125; else &#123; // 我们的服务： PortfolioService portfolio = portfolioFuture.result(); MessageConsumer&lt;Map&gt; marketConsumer = marketFuture.result(); // 监听市场行情... marketConsumer.handler( &#123; message -&gt; Map quote = message.body(); TraderUtils.dumbTradingLogic(company, numberOfShares, portfolio, quote); &#125;); &#125; &#125;); 正如你所见，代码和 Java 版非常相近。我们可以指出一些不同点： 好吧，这是 Groovy。当一个接口被标记成 @VertxGen 的时候，Vert.x 会把这个接口对所有(可配置)语言的支持都翻译出来 在所有 import 语句里，你可以看到它们导入了依赖的包(和 Java 相同)…​ 我们必须必须建立一个服务发现器。 这个例子用 Groovy 开发完毕，它和 JavaScript, Ruby 或者 Ceylon 非常相似。 我们在新的版本里已经没有办法找到 io.vertx.groovy. 等包。所以在写本文的时候，我们需要用 io.vertx. 包来替代它们。并且稍微改变了一些代码，以便他能够执行。另外，新编本里带有一个 Kontlin 实现的交易员。你可以使用类名 io.vertx.workshop.trader.impl.KotlinCompulsiveTraderVerticle 来找到它。并且用 vertx.deployVerticle(String className) 来启动它。 是时候重新编译打包，重启我们的交易员程序了。 键入 CTRL+C 关闭正在执行的交易员。然后重新编译打包： mvn clean package 接着，启动我们的应用程序： java -jar target/compulsive-traders-1.0-SNAPSHOT-fat.jar 如果你回到仪表盘，你可以看到投资组合的一些变动。现在我们有三个交易员帮助让你更富有了。：） 10. 审计服务法律就是法律…​ Sarbanes-Oxley 动议 要求你保存你在金融市场上做过的所有交易的跟踪记录。审计服务把你买卖的股票金额记录到数据库中。 这是 HSQL 数据库，当然其他的数据库也是相似的，即使是 no-sql 数据库。 10.1. 数据的异步访问基于之前的讨论， Vert.x 是异步的，你千万不能阻塞事件循环。你知道什么是阻塞操作吗？ 数据库访问，现在流行的 JDBC 就是阻塞的！ 幸运的是， Vert.x 提供了一个异步 JDBC 客户端。 原则非常简单(这适用于所有访问阻塞系统的客户端)： Worker ? 是的, Vert.x 里的标注 workers (使用一个独立的线程池) 是用来执行阻塞代码的。 他可以是一个被标注为 worker的竖直体 (verticle) 或者是一个实现了`vertx.executeBlocking` 构建函数。 虽然有这些功能，你还是最好不要滥用这个功能能，否则你会降低系统的扩展性。 和数据库互操作，通常不会是一个单独的操作。他会是一连串操作的组合。例如： 获得数据库连接 删除一些数据表 建立一些数据表 关闭数据库连接 所以，我们需要组合这些操作，而且在需要的时候报故障。我们将会在审计组件中看到怎么做。 in the Audit component. 10.2. 审计服务这就是审计服务： 从事件总线上收听金融操作 把收到的操作存储到数据库中 暴露一个 REST API，通过它，可以拿到最后10次操作。 与数据互操作，我们用到了 vertx-jdbc-client, 这是 JDBC 的异步版本。所以你会看到一些 SQL 代码 （你会喜欢的）。 10.3. Rxjava10.3.1. 简介Vert.x 使用简单的异步回调，它的 预期(Future) 对象是一个有用的工具帮助你协调回调函数。 RxJava 实现了在 JVM 上的反应式扩展库，它可以用来组合异步和基于事件的程序。 基于 RxJava，你通过数据流象进行代码建模(也叫可观察的 Observable)。这些数据流是数据传输的管道。 这些 观察者(Observable) 可以代表有限，或者无限的数据流，甚至是只有一个元素的数据流。 当我们确切知道一个流只有一个元素时，我们可以方便的使用 单例(Single) 类型。 最后，一个 结束(Completable) 代表一个没有任何元素的流，也就是说。它只能以没有结果的方式结束，或者失败。 使用这些反应式类型，订阅(subscription) 操作就可以了。 Observable&lt;String&gt; observable = getStringObservable(); // Subscribe to the stream observable.subscribe(item -&gt; &#123; // Received a String item &#125;, error -&gt; &#123; // Error termination =&gt; no more items &#125;, () -&gt; &#123; // Normal termination =&gt; no more items &#125;); 单例由于只有一个元素，用起来比较简单。它们和预期(Future)/承诺(Promise)除了有很多值得注意的不同以外还是有一些共同点的： 一个 预期/承诺 是异步操作的结果。也就是说，你启动了一个服务器，你得到的承诺是，服务器会返回绑定的结果。 一个单例结果通常在订阅的时候有一个副作用。也就是说，你订阅了一个单例，副作用导致服务启动后，单例只会通知单个绑定的结果。 Single&lt;String&gt; single = getStringSingle(); // Subscribe to the single single.subscribe(item -&gt; &#123; // Completion with the string item &#125;, error -&gt; &#123; // Completion with an error &#125;); 10.3.2. 组合和转换RxJava 提供了一组非常有用的针对组合，和转换异步流的操作。 在这个实验课里，你会使用重要的几个重要的操作：map, flatMap and zip 。 map 操作可以同步的把一个操作的结果进行转换。 // Transform the stream of strings into a stream of Buffer Observable&lt;Buffer&gt; observable = getStringObservable().map(s -&gt; vertx.fileSystem().readFileBlocking(s)); // Transform the string single into a Buffer single Single&lt;Buffer&gt; single = getStringSingle().map(s -&gt; vertx.fileSystem().readFileBlocking(s)); map 的一个不足之处就是它的同步性。为了从文件里获得内容，我们必须使用 阻塞 版本的文件系统API，这就破坏了 Vert.x的黄金规则。 幸运的是，我们还有一个异步版本的映射，flatMap。 // Transform the stream of strings into a stream of Buffer Observable&lt;Buffer&gt; observable = getStringObservable().flatMap(s -&gt; &#123; Single&lt;Buffer&gt; single = vertx.fileSystem().rxReadFile(); return single.toObservable(); &#125;); // Transform the string single into a Buffer single Single&lt;Buffer&gt; single = getStringSingle().flatMap(s -&gt; &#123; Single&lt;Buffer&gt; single = vertx.fileSystem().rxReadFile(); return single; &#125;); zip 操作符可以把多个 Observable/Single 组合成一个单独的结果，我们看到的结果是 Single: Single&lt;String&gt; single1 = getStringSingle(); Single&lt;String&gt; single2 = getStringSingle(); Single&lt;String&gt; single3 = getStringSingle(); Single&lt;String&gt; combinedSingle = Single.zip(single1, single2, single3, (s1,s2,s3) -&gt; s1 + s2 + s3); combinedSingle.subscribe(s -&gt; &#123; // Got the three concatenated strings &#125;, error -&gt; &#123; // At least one of single1, single2 or single3 failed &#125;); 它工作起来象 Observable，但是为了简洁起见，我们在这里就不多说了。 10.4. Vert.x RxVert.x 有一个 Rx 异步版本的 API， 它的包都有前缀 io.vertx.rxjava， 例如 io.vertx.rxjava.core.Vertx`是 Rx 版本的 `io.vertx.core.Vertx。 这些 RX化的Vert.x 暴露的异步方法都是单例 Single 和流类型 Observable. 10.4.1. Vert.x 流类型 ReadStream&lt;T&gt; 是对反应式 T 项目序列的建模。举例来说，HttpServerRequest 就是 ReadStream&lt;Buffer&gt;。 RX化 版本暴露了 toObservable() 方法，用来把流转换成 Observable&lt;T&gt;。 import io.vertx.rxjva.core.Vertx; import io.vertx.rxjva.core.http.HttpServer; ... Vertx vertx = Vert.vertx(); HttpServer server = vertx.createHttpServer(); server.requestHandler(request -&gt; &#123; if (request.path().equals(&quot;/upload&quot;)) &#123; Observable&lt;Buffer&gt; observable = request.toObservable(); observable.subscribe(buffer -&gt; &#123; // Got an uploaded buffer &#125;, error -&gt; &#123; // Got an error =&gt; no more buffers &#125;, () -&gt; &#123; // Done =&gt; no more buffers &#125;); &#125; &#125;); 在本节，我们不会使用 Observable，反应式流并不是我们的重点。但是我们会应用 Single 。 10.4.2. Vert.x 单例每一个异步方法，例如最后一个参数类型是 Handler&lt;AsyncResult&lt;T&gt;&gt; 的方法，都会有一个 RX化 的版本。它的名字就是在原方法名前加上 rx 。 它的参数和原函数基本一致，只是去掉了最后一个参数。它会有一个返回值，一个单例 Single 的异步类型。 不象原方法，调用 rx 版本并不会真正的调用什么。你会拿到一个单例，你会在订阅这个单例的时候调用到实际的方法。 import io.vertx.rxjva.core.Vertx; import io.vertx.rxjva.core.http.HttpServer; ... Vertx vertx = Vert.vertx(); HttpServer server = vertx.createHttpServer(); server.requestHandler(request -&gt; ...); // 这个单例建立了，但是服务并没有在这时启动 Single&lt;HttpServer&gt; listenSingle = server.rxListen(8080); // 出发服务器启动 listenSingle.subscribe( server -&gt; &#123; // 服务器启动，并且绑定到 8080 端口 &#125;, error -&gt; &#123; // 服务器没有启动 &#125;); 10.5. 任务 - 返回单例的组合方法打开 io.vertx.workshop.audit.impl.AuditVerticle 类。这个竖直体第一个重要的细节是 start 方法。 象 Java 交易员的 start 方法一样，这个方法是异步的，并且通过传入的 Future 对象报告完成状态。 public void start(Future&lt;Void&gt; future) &#123; super.start(); // 建立 jdbc 客户端 jdbc = JDBCClient.createNonShared(vertx, config()); // TODO // ---- Single&lt;MessageConsumer&lt;JsonObject&gt;&gt; ready = Single.error(new UnsupportedOperationException(&quot;not yet implemented&quot;)); // ---- readySingle.doOnSuccess(consumer -&gt; &#123; // 如果成功，我们把消息处理函数设置好。它会把消息存储到数据库 consumer.handler(message -&gt; storeInDatabase(message.body())); &#125;).subscribe(consumer -&gt; &#123; // 成功完成竖直体的启动 future.complete(); &#125;, error -&gt; &#123; // 通知竖直体启动失败 future.fail(error); &#125;); &#125; 当 Future 被赋值以后，Vert.x 会认为竖直体成功部署了。当竖直提不能启动的时候，它也有可能上报一个失败。 初始化审计服务，我们需要以下操作： 准备数据库 (表） 启动 HTTP 服务并且暴露 REST API。另外发布这个服务。 从消息源接收股票操作 所以，这很清楚是三个毫不相干的操作。但是，审计服务必须在所有三个操作全都完成。 请用以下代码替换 TODO 代码块。这写代码(用对象提供的方法)获取了三个单例对象，并且 等(wait) 这三个任务都完成。 这三个单例应该组成一个 Single&lt;MessageConsumer&lt;JsonObject&gt;&gt; 。 当这个单例成功的时候，它会在投资组合消息源上注册一个消息监听器。监听器会把每个接收到的消息存储到数据库。 它完成的时候，会调用 future.complete() 和 future.fail(cause)，这样会通知 Vert.x 开始过程已经完成 （或者成功或者失败）。 Single&lt;Void&gt; databaseReady = initializeDatabase(config().getBoolean(&quot;drop&quot;, false)); Single&lt;Void&gt; httpEndpointReady = configureTheHTTPServer() .flatMap(server -&gt; rxPublishHttpEndpoint(&quot;audit&quot;, &quot;localhost&quot;, server.actualPort())); Single&lt;MessageConsumer&lt;JsonObject&gt;&gt; messageConsumerReady = retrieveThePortfolioMessageSource(); Single&lt;MessageConsumer&lt;JsonObject&gt;&gt; readySingle = Single.zip( databaseReady, httpEndpointReady, messageConsumerReady, (db, http, consumer) -&gt; consumer); 首先我们生成了三个单例 Single，每个执行活动一个。我们将很快看到它们是怎么建立的。然后，我们把所有单例用 Single.zip 函数组合起来。 zip 函数返回消费者单例，这是我们唯一关心的一个。 10.6. 任务 - 实现一个返回单例的方法现在我们的竖直体扩充 (extends) 了 RxMicroServiceVerticle 基类。 这个类和 MicroServiceVerticle 提供了相同的方法接口，不同点是使用了 Rx 单例。 我们已经提到过，异步方法的特征是最后一个参数是 Handler。 这里有一个等价的语法，就是在操作结束之后，返回一个 Single 对象。 void asyncMethod(a, b, Handler&lt;AsyncResult&lt;R&gt;&gt; handler);// 语义等价Single&lt;R&gt; asyncMethod(a, b); 实际上，调用者可以订阅这个 Single 对象来执行异步操作，并且得到操作完成和失败的通知。 Single&lt;R&gt; single = asyncMethod(a, b);single.subscribe(r -&gt; &#123; // 基于结果完成工作 &#125;, err -&gt; &#123; // 处理故障 &#125;); 让我们用这种模式实现 configureTheHTTPServer 方法。在这个方法里，我们将要用一个新的 Vert.x 组件：Vert.x Web。 Vert.x Web是一个 Vert.x 用来建立现代web应用的扩展。这里我们将要用到 Router，这可以让我们轻松实现 REST API (类似 Hapi 和 ExressJS)。 所以： 建立一个 Router 对象，使用函数: Router.router(vertx) 在路由器上注册一个路径 (在跟路径上 /) , 就叫 retrieveOperations 建立一个 HTTP 服务器，然它负责处理在 router.accept 上的请求。 获取配置文件里写好的端口。如果没有设置，我们会传入 0 （程序会选择一个随机端口）。 因为服务会暴露在服务记录上，消费者可以获得这个端口，所以我们可以随机使用一个端口。 使用 rxListen 启动这个服务。这是一个 rx版本的监听方法，所以它返回一个单例。 // Use a Vert.x Web router for this REST API. Router router = Router.router(vertx); router.get(&quot;/&quot;).handler(this::retrieveOperations); return vertx.createHttpServer() .requestHandler(router::accept) .rxListen(config().getInteger(&quot;http.port&quot;, 0)); It creates a Router. The Router is an object from Vert.x web that ease the creation of REST API with Vert.x. We won’t go into too much details here, but if you want to implement REST API with Vert.x, this is the way to go. On our Router we declare a route: when a request arrive on /, it calls this Handler. Then, we create the HTTP server. The requestHandler is a specific method of the router, and we return the result of the rxListen method. 这样的话，调用者调用这个方法，并且得到一个 单例(Single)。它可以订阅这个单例来绑定服务器，并且得到成功失败的通知。 如果你阅读 retrieveThePortfolioMessageSource 方法，你能看到类似的模式。 10.7. 使用异步 JDBC在 start 方法里，我们调用了 initializeDatabase。让我们看看这个方法，它使用了另一种活动组合。这个方法： 得到了一个数据库连接 删除了数据表 建立了数据表 关闭了连接(不管前两步操作是不是成功) 所有这些操作都有可能失败。 在上一个段落，你已经见到返回 Single 的方法了。链(Chains) 是另外一种函数组合的方式。 你有一个输入 你执行第一个 Function，从步骤 (1) 拿到输入，并且返回一个 Single 你执行第二个 Function，从步骤 (2) 拿到输入，并且返回一个 Single …​. 整个链的返回会是一个 Single 对象。如果任何一个链上的操作失败，这个 Single 被标记成失败，否则，它成功完成，返回最后一个操作的结果： Single&lt;X&gt; chain = input.flatMap(function1).flatMap(function2).flatMap(function3); 所以链接函数，我们只是需要几个 Functions 和一个能够触发整个链的 Single。 让我们先建立一个 Single： // 这是我们Rx操作的起始点。 // 这个单例会在数据库连接建立之后完成。 // 我们会使用这个单例作为连接的引用，在关闭连接的时候我们会用到它 Single&lt;SQLConnection&gt; connectionRetrieved = jdbc.rxGetConnection(); 然后，我们会用 flatMap 方法继续组合单例。flatMap 会传入 SQLConnection 参数和返回一个包含数据库初始化结果的单例。 我们建立了一个批处理 rxBatch 执行这个批处理，并且返回你一个单例作为结果 最终，我们用 doAfterTerminate 关闭这个连接 Single&lt;List&gt; resultSingle = connectionRetrieved .flatMap(conn -&gt; { // 当我们得到了数据库连接 // 准备批处理 List&lt;String&gt; batch = new ArrayList&lt;&gt;(); if (drop) &#123; // 数据表被删除以后，我们会重新建立 batch.add(DROP_STATEMENT); &#125; // 直接建立数据库表 batch.add(CREATE_TABLE_STATEMENT); // 我们组合了一个批处理语句 Single&lt;List&lt;Integer&gt;&gt; next = conn.rxBatch(batch); // 不管结果如何，只要有数据连接，我们就关闭它。 return next.doAfterTerminate(conn::close); }); resultSingle 是最后返回的是 Single&lt;List&lt;Integer&gt;&gt; 。因为上层的调用函数并不需要任何细节信息，我们只需要返回 Single&lt;Void&gt; 。 这用 map 操作很容易做到。 return resultSingle.&lt;Void&gt;map(null); 你瞧。 10.8. 任务 - 异步 JDBC 和基于回调的组合你可能会问为什么我们需要做这种组合。让我们实现一个没有任何组合的操作 (只用回调函数)。这个 retrieveOperations 方法会在 HTTP 请求到达时被调用，它会返回一个包含10个最新股票操作的 JSON 对象。换一种说法： 得到一个数据库连接 查询数据库 操作返回数据集拿到股票操作列表 把股票操作列表写入到 HTTP 响应中 关闭数据库 步骤 (1) 和 (2) 都是异步的. (5) 也是异步的, 而且我们没有必要等待最终完成。 在代码里，我们不会使用组合 (这是这个练习的目的)。 retrieveOperations 里，我们使用 处理器(Handlers) / 回调(Callbacks) 。 // 1 - 得到一个连接 jdbc.getConnection(ar -&gt; &#123; SQLConnection connection = ar.result(); if (ar.failed()) &#123; context.fail(ar.cause()); &#125; else &#123; // 2. 执行查询 connection.query(SELECT_STATEMENT, result -&gt; &#123; ResultSet set = result.result(); // 3. 建立股票操作列表 List&lt;JsonObject&gt; operations = set.getRows().stream() .map(json -&gt; new JsonObject(json.getString(&quot;OPERATION&quot;))) .collect(Collectors.toList()); // 4. 把列表写入 HTTP 响应 context.response().setStatusCode(200).end(Json.encodePrettily(operations)); // 5. 关闭连接 connection.close(); &#125;); &#125; &#125;); 很显然，我们也可以不使用组合操作。但是你可以想象以下，如果你有好几个异步操作来链接，这会使回调异常复杂。这会是回调地狱 (callback hell)。 在这种情况下，我们建议：使用 Vert.x 组合操作或者使用 rx化 版本的 Vert.x API。 10.9. 展示一下 !让我们看看这是怎么工作的。 首先，我们先构建包： cd audit-service mvn clean package 然后，你需要启动这个程序： java -jar target/audit-service-1.0-SNAPSHOT-fat.jar 重启并且刷新仪表盘，然后你可以看到在右上角看到股票操作啦！ 11. 仪表盘解析这一节是关于仪表盘的。它的内容包括： 如何配置 Vert.x web 来暴露静态资源 如何实现实现一个 REST 端点委派给另外一个 REST 终结点 (代理模式) 如何保护微服务相互操作不被故障影响(异常处理，超时，回路代理) 如何配置一个 SockJS - 事件总线桥接器 如何从浏览器上消费一个事件总线代理 仪表盘是一个单独的竖直体 (io.vertx.workshop.dashboard.DashboardVerticle). 11.1. Vert.x Web 和静态文件在之前几节里，我们提到过 Vert.x web 是一种用来建立 Web 应用的 Vert.x 组件。 这个架构是以 路由器(Router) 为核心的。 你创建一个路由器并且配置 路由(routes)。对每一个路由，你都需要配置 HTTP 动词 和 路径（path) 和相对应的 处理器(Handler)。 在收到对应请求时，处理器会被调用。路由器(router) 可以这样创建： Router router = Router.router(vertx); Vert.x web 提供了一组 Handler 累处理标准的任务，像处理静态文件： // Static content router.route(&quot;/*&quot;).handler(StaticHandler.create()); 这样，所有在 webroot下(默认值) 或者服务器根目录下的文件都可以通过服务访问到了。 举例说明，webroot/index.html 就可以通过 http://0.0.0.0:8080/index.html 访问到了。 路由器配置好以后，我们就可以启动 HTTP 服务来使用这个路由器处理请求了： vertx.createHttpServer() .requestHandler(router::accept) .listen(8080); 11.2. 委派 REST 调用我们经常需要实现基于 REST API 的服务。由于每个调用都会阻塞一个线程，在其他传统架构上，我们需要等所有 REST API 都返回，这个模式有可能会非常费时低效。 使用 Vert.x 委派会把这个问题变简单。异步和非阻塞。 在仪表盘的例子里，我们想要获得股票操作列表。这个列表是审计服务提供的。 所以，仪表盘我们这样定义路由： router.get(&quot;/operations&quot;).handler(this::callAuditService); 处理器是这样的: private void callAuditService(RoutingContext context) &#123; if (client == null) &#123; context.response() .putHeader(&quot;content-type&quot;, &quot;application/json&quot;) .setStatusCode(200) .end(new JsonObject().put(&quot;message&quot;, &quot;No audit service&quot;).encode()); &#125; else &#123; client.get(&quot;/&quot;).send(ar -&gt; &#123; if (ar.succeeded()) &#123; HttpResponse&lt;Buffer&gt; response = ar.result(); context.response() .putHeader(&quot;content-type&quot;, &quot;application/json&quot;) .setStatusCode(200) .end(response.body()); &#125; &#125;); &#125; &#125; 审计服务会在竖直体的 start 函数里拿到。如果服务没有找到，这个处理器返回 no audit service 消息。 否则，他会使用找到的 HTTP 客户端调用审计服务 REST API。应答消息只是简单的把 HTTP 请求的返回传递给客户端。 11.3. 任务 - 用异常处理器和超时管理故障我们已经在上一节看到了在`callAuditService` 是怎么工作的…​ 好吧，那么我们怎么在一个分布式系统中知道： 远端出故障了。 我们最好在这个方法里处理故障。 Vert.x 提供了四种处理故障的方法： 异步结果和预期(future)返回故障信息 异常处理器(exception handlers) 超时(timeout) 断路器(circuit breaker) 我们已经了解了第一个方式。这一小节将会应用超时。下一个小节将会使用断路器。 把下面的 callAuditServiceTimeout 方法拷贝到 DashboardVertical, 他会设置 HTTP 请求的 超时(timeout)。 RoutingContext 的 fail 方法会在故障的时候调用。 如果返回结果在超时之前没有返回，这个异步结果就会失败。 不要忘了把 `/operations`` 处理器换成这个方法。 private void callAuditServiceTimeout(RoutingContext context) &#123; if (client == null) &#123; context.response() .putHeader(&quot;content-type&quot;, &quot;application/json&quot;) .setStatusCode(200) .end(new JsonObject().put(&quot;message&quot;, &quot;No audit service&quot;).encode()); &#125; else &#123; client.get(&quot;/&quot;) .timeout(5000) .send(ar -&gt; &#123; if (ar.succeeded()) &#123; HttpResponse&lt;Buffer&gt; response = ar.result(); context.response() .putHeader(&quot;content-type&quot;, &quot;application/json&quot;) .setStatusCode(200) .end(response.body()); &#125; else &#123; context.fail(ar.cause()); &#125; &#125;); &#125; &#125; 做完上面的步骤，我们重新编译打包仪表盘 cd trader-dashboard mvn clean package 然后，在另一个终端启动它： java -jar target/trader-dashboard-1.0-SNAPSHOT-fat.jar 刷新仪表盘页面。在审计服务终端，终止这个服务然后查看仪表盘如何反应(你可以用 inspector/dev tools 查看 AJAX 请求)。然后，重新启动审计服务。发生了什么？ 11.4. 任务 - 使用断路器管理故障断路器是一个可以信赖的模式，它可以用一个简单的状态机来表示： 因为它可以光滑的从故障里恢复，所以这个模式在基于微服务的应用里非常流行。 一个断路器从 close 状态开始。这时它监视一个操作。每次这个操作失败的时候，它增加故障计数。 当计数超过某个阀值的时候，断路器就打开，变成 open 状态。在打开状态，操作本身不会再被调用，另外一个回调会被立即调用。 过一段时间以后，断路器会变成半开 half-open 状态。在这个状态，操作在第一个请求到达时被调用。其它请求会被重定向到回滚方法。 如果操作调用失败，断路器重新打开，直到另一次重试。如果重试成功，它会关闭。 断路器的实现有很多版本。Netflix Hystrix 是最流行的一种。Vert.x 提供了自己的实现。实际上，Hystrix 并不强制 Vert.x 改变线程模型，使用 Hystrix 是可能的，只是有些笨重。 在 DashboardVerticle.java 文件里，断路器 circuit 在 start 方法里被初始化了： circuit = CircuitBreaker.create( &quot;http-audit-service&quot;, //断路器的名字 vertx, new CircuitBreakerOptions() .setMaxFailures(2) //多少次失败会打开 open 断路器 .setFallbackOnFailure(true) //当检查到一个故障以后，是不是需要调用回滚 fallback 函数。即使关闭 close 状态也不例外 .setResetTimeout(2000) //从打开状态到半开状态转换需要多长时间 .setTimeout(1000)) //操作持续多久没有完成，会被认为是操作失败 .openHandler(v -&gt; retrieveAuditService()); 在断路器转换成打开状态的时候，需要执行的处理。这里我们重新获取审计服务 使用断路器，我们可以写一个 callAuditServiceWithExceptionHandlerWithCircuitBreaker 方法来发现审计服务。 circuit.&lt;Buffer&gt;executeWithFallback 函数会被用到。别忘了修改路由 /operations 的处理器。 做完这些步骤，可以重新编译打包，重新启动仪表盘。你可以关闭审计服务，看看仪表盘的行为。重启审计服务，你可以从仪表盘看到断路器的状态。 Show me the code 11.5. SockJS - 事件总线桥接器SockJS 是运行在浏览器上的 JavaScript 函数库。它提供了类似 WebSocket 的对象。 SockJS 提供了一个明了的，跨不同浏览器的，Javascript API。我们可以用它建立低延迟，全双工，跨域通信的浏览器和网络服务器之间的通道。 从实现机制上讲，SockJS 会先尝试使用本地 WebSockets。如果不工作，他会使用一些针对具体浏览器的特殊传输层协议。并且把他们包装成 WebSocket 类似的抽象层。 SockJS 客户端需要有一个服务器对端来进行通信。如你所愿，Vert.x 实现了它！ 使用 SockJS - 事件总线桥接器，浏览器可从事件总线上以发送和接收消息。 为了启动桥接器，你需要以下代码： SockJSHandler sockJSHandler = SockJSHandler.create(vertx); //(1) BridgeOptions options = new BridgeOptions(); options .addOutboundPermitted(new PermittedOptions().setAddress(&quot;market&quot;)) //(2) .addOutboundPermitted(new PermittedOptions().setAddress(&quot;portfolio&quot;)) .addOutboundPermitted(new PermittedOptions().setAddress(&quot;service.portfolio&quot;)) .addInboundPermitted(new PermittedOptions().setAddress(&quot;service.portfolio&quot;)); sockJSHandler.bridge(options); //(3) router.route(&quot;/eventbus/*&quot;).handler(sockJSHandler); //(4) 在 (1) 里, 我们建立了 SockJSHandler。我们要配置它，默认情况下，由于安全的要求，没有任何消息会被传输。 一组允许的地址被配置到桥接器里 (2) 。 对外的地址是为了消息能从事件总线发送到浏览器，对内地址是为了消息能从浏览器到事件总线。 最后，在 (3) 和 (4) ，我们配置了处理器并且在 router 建立了一个路由器。/eventbus/* 路径是用来让 SockJS 客户端(在浏览器里)协商链接，接收和发送消息。 这不是唯一的以个事件总线上的桥接器。我们还有 TCP 事件总线侨界器针对本地系统。 请注意，SockJS 桥接器也可以从 Node.JS 里调用。 11.6. 从浏览器消费事件总线的服务像上面说的，通过SockJS和事件总线的桥接器，浏览器可以和事件总线之间收发消息。 事件总线服务是通过总线传递消息的，所以我们在浏览器上是可以实现服务的客户端的。 Vert.x 会为你生成客户端。 因此，如果你打开 index.html 文件，你可以看到： &lt;script src=&quot;libs/portfolio_service-proxy.js&quot;&gt;&lt;/script&gt; 这就导入了 Vert.x 生成的脚本。接下来，我们可以使用这个服务： var service = new PortfolioService(eventbus, &quot;service.portfolio&quot;); service.getPortfolio(function (err, res) &#123; // .... &#125; 很好，你可以直接从浏览器调用服务了。 12. 结语你做到了！或者你只是跳到了这一节。无论如何，祝贺你。我们希望你享受整个实验课，并且学到了一些东西。 这里有很多 Vert.x 能做的事情，我们还没有涉及到。 不要忘了，反应式系统的原则，和 Vert.x 实际上需要我们在变成思路上有所变化。 Vert.x 是一个建立反应式系统的工具包。 异步，非阻塞的开发模式可能在第一眼看上去难以理解，但是它很快就会变成方便的工具。 同样不该忘记的是，计算机是异步的，所以使用这种开发模式是开启计算机计算能力的正确方法。 我建议，如果你愿意，你可以阅读下面的参考资料： Vert.x 主网站 一个系列博客说明如何开始用 Vert.x 开发 一些为微服务应用开发的基础组件 13. 参考文献这是一些推荐的阅读材料。并不是针对微服务或者 Vert.x 的，它们提供的概念层的讨论已经超出了这两个主题。 A. S. Tanenbaum, M Van Steam. Distributed Systems - Principles and Paradigms. 2003 L. Bass, I. Weber, L. Zhu. Devops, A software Architect’s Perspective. 2015 P. Clements, F. Bachmann, L Bass, D. Garlan, J. Ivers, R. Little, P. Merson, R. Nord, J. Stafford. Documenting Software Architecture. 2010 S. Krakowiak. Middleware Architecture with Patterns and Frameworks. 2009 (unfinished), http://lig-membres.imag.fr/krakowia/Files/MW-Book/Chapters/Preface/preface.html J. Lewis, M. Fowler. Microservices - a definition of this new architectural term, 2014, http://martinfowler.com/articles/microservices.html 14. 附录 A - 启动，终止，列表命令即便不是必须，Vert.x 提供了一个方便的启动器 Lancher 类。这个类 只 是应用程序的起始点。 它使用指定的参数初始化 Vert.x ，部署给定的竖直体，等等。 首先，这个启动器是可以扩展的。比如，在这个实验课，我们有自己的启动器(io.vertx.workshop.common.Launcher)，它扩展了 Vert.x 自带的。 另外，这个启动器包含了可以扩充的 命令(command) 集合。 运行(run) 是默认的命令来执行指定的竖直体。默认情况下，这里有一些其他可用的命令: bare 启动一个裸露的 vert.x 实例 list 列出所有 vert.x 应用 run 在自己的 vert.x 实例里执行一个叫做 &lt;main-verticale&gt; 竖直体 start 在后台启动一个 vert.x 应用程序实例 stop 终止一个 vert.x 应用程序 test 执行 Vert.x 单元测试，在自己的 vert.x 实例里调用 &lt;test-verticle&gt; 你也可以添加你的命令。 在这个小节，我们会专注于 start, stop 和 list 命令。 当我们启动了微服务，它们并不会在后台运行，会阻塞我们的终端。 start 命令可以让 Vert.x 应用在后台运行。对于报价生成器应用，我们可以使用下面的命令行： java -jar target/quote-generator-1.0-SNAPSHOT-fat.jar start -id quote-generator --redirect-output 命令(command) 名字就紧跟这 jar 文件名。id 参数给你的应用起了一个名字。默认情况下，它生成了一个 UUID。 --redirect-output 选项使 Vert.x 把输出重定向到终端。 现在，我们看一个实则列出运行中的 Vert.x 应用，你可以： java -jar target/quote-generator-1.0-SNAPSHOT-fat.jar list Listing vert.x applications... quote-generator target/quote-generator-1.0-SNAPSHOT-fat.jar 然后，stop 关闭这个应用，你可以用命令： java -jar target/quote-generator-1.0-SNAPSHOT-fat.jar stop quote-generator 基于此，你可以写一个简单的命令行脚本来启动所有的微服务 # Skip this instruction to use your version cd solution cd quote-generator mvn clean package java -jar target/quote-generator-1.0-SNAPSHOT-fat.jar start -id quote-generator --redirect-output cd .. cd portfolio-service mvn clean package java -jar target/portfolio-service-1.0-SNAPSHOT-fat.jar start -id portfolio-service --redirect-output cd .. cd compulsive-traders mvn clean package java -jar target/compulsive-traders-1.0-SNAPSHOT-fat.jar start -id compulsive-traders --redirect-output cd .. cd audit-service mvn clean package java -jar target/audit-service-1.0-SNAPSHOT-fat.jar start -id audit-service --redirect-output cd .. cd trader-dashboard mvn clean package java -jar target/trader-dashboard-1.0-SNAPSHOT-fat.jar start -id trader-dashboard --redirect-output cd .. 当每一个服务都启动以后。你可以用 list 命令查看运行的程序： java -jar target/trader-dashboard-1.0-SNAPSHOT-fat.jar list Listing vert.x applications... quote-generator target/quote-generator-1.0-SNAPSHOT-fat.jar portfolio-service target/portfolio-service-1.0-SNAPSHOT-fat.jar compulsive-traders target/compulsive-traders-1.0-SNAPSHOT-fat.jar audit-service target/audit-service-1.0-SNAPSHOT-fat.jar trader-dashboard target/trader-dashboard-1.0-SNAPSHOT-fat.jar 你可以使用任何一个胖 jar 包来执行 list 和 stop 命令 你可以写一个相反的脚本来停止每一个服务： cd quote-generator java -jar target/quote-generator-1.0-SNAPSHOT-fat.jar stop trader-dashboard java -jar target/quote-generator-1.0-SNAPSHOT-fat.jar stop quote-generator java -jar target/quote-generator-1.0-SNAPSHOT-fat.jar stop audit-service java -jar target/quote-generator-1.0-SNAPSHOT-fat.jar stop compulsive-traders java -jar target/quote-generator-1.0-SNAPSHOT-fat.jar stop portfolio-service stop 命令会发送一个信号给它要终止的进程。无论如何，这会需要一些时间。进程不会立即被终止。我们可以用 list 命令查看状态。 [1]. 反应式系统和反应式编程是两个不同的 概念 。反应式编程是使用观察和操作数据流的开发模式。而反应式系统会针对一下作出反应：请求，故障，负载高点，并且通过异步消息相互作用。[2]. 异步：调用者不会等待结果返回，但是传递一个 回调函数(callback) ，以便结果计算出来的时候调用[3]. 非阻塞：代码必须不阻塞线程，所以他必须放弃阻塞IO,长时间的处理，等等情况[4]. 这是软件工程研究所对 架构风格 的规范说明[5]. 更快更容易改变的能力，这和Agile软件工程方法无关","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"微服务","slug":"微服务","permalink":"https://blog.fenxiangz.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"Vert.x","slug":"Vert-x","permalink":"https://blog.fenxiangz.com/tags/Vert-x/"}]},{"title":"技术面试要了解的算法和数据结构知识","slug":"interview/2018-10-26_技术面试要了解的算法和数据结构知识","date":"2018-10-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.955Z","comments":true,"path":"post/interview/2018-10-26_技术面试要了解的算法和数据结构知识.html","link":"","permalink":"https://blog.fenxiangz.com/post/interview/2018-10-26_%E6%8A%80%E6%9C%AF%E9%9D%A2%E8%AF%95%E8%A6%81%E4%BA%86%E8%A7%A3%E7%9A%84%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9F%A5%E8%AF%86.html","excerpt":"","text":"在线练习 LeetCode Virtual Judge CareerCup HackerRank CodeFights Kattis HackerEarth Codility Code Forces Code Chef Sphere Online Judge – SPOJ 在线编程面试 Gainlo Refdash 数据结构链表 链表是一种由节点（Node）组成的线性数据集合，每个节点通过指针指向下一个节点。它是一种由节点组成，并能用于表示序列的数据结构。 单链表 ：每个节点仅指向下一个节点，最后一个节点指向空（null）。 双链表 ：每个节点有两个指针p，n。p指向前一个节点，n指向下一个节点；最后一个节点指向空。 循环链表 ：每个节点指向下一个节点，最后一个节点指向第一个节点。 时间复杂度： 索引：O(n) 查找：O(n) 插入：O(1) 删除：O(1) 栈 栈是一个元素集合，支持两个基本操作：push用于将元素压入栈，pop用于删除栈顶元素。 后进先出的数据结构（Last In First Out, LIFO） 时间复杂度 索引：O(n) 查找：O(n) 插入：O(1) 删除：O(1) 队列 队列是一个元素集合，支持两种基本操作：enqueue 用于添加一个元素到队列，dequeue 用于删除队列中的一个元素。 先进先出的数据结构（First In First Out, FIFO）。 时间复杂度 索引：O(n) 查找：O(n) 插入：O(1) 删除：O(1) 树 树是无向、联通的无环图。 二叉树 二叉树是一个树形数据结构，每个节点最多可以有两个子节点，称为左子节点和右子节点。 满二叉树（Full Tree） ：二叉树中的每个节点有 0 或者 2 个子节点。 完美二叉树（Perfect Binary） ：二叉树中的每个节点有两个子节点，并且所有的叶子节点的深度是一样的。 完全二叉树 ：二叉树中除最后一层外其他各层的节点数均达到最大值，最后一层的节点都连续集中在最左边。 二叉查找树 二叉查找树（BST）是一种二叉树。其任何节点的值都大于等于左子树中的值，小于等于右子树中的值。 时间复杂度 索引：O(log(n)) 查找：O(log(n)) 插入：O(log(n)) 删除：O(log(n)) 字典树 字典树，又称为基数树或前缀树，是一种用于存储键值为字符串的动态集合或关联数组的查找树。树中的节点并不直接存储关联键值，而是该节点在树中的位置决定了其关联键值。一个节点的所有子节点都有相同的前缀，根节点则是空字符串。 树状数组 树状数组，又称为二进制索引树（Binary Indexed Tree，BIT），其概念上是树，但以数组实现。数组中的下标代表树中的节点，每个节点的父节点或子节点的下标可以通过位运算获得。数组中的每个元素都包含了预计算的区间值之和，在整个树更新的过程中，这些计算的值也同样会被更新。 时间复杂度 区间求和：O(log(n)) 更新：O(log(n)) 线段树 线段树是用于存储区间和线段的树形数据结构。它允许查找一个节点在若干条线段中出现的次数。 时间复杂度 区间查找：O(log(n)) 更新：O(log(n)) 堆 堆是一种基于树的满足某些特性的数据结构：整个堆中的所有父子节点的键值都满足相同的排序条件。堆分为最大堆和最小堆。在最大堆中，父节点的键值永远大于等于所有子节点的键值，根节点的键值是最大的。最小堆中，父节点的键值永远小于等于所有子节点的键值，根节点的键值是最小的。 时间复杂度 索引：O(log(n)) 查找：O(log(n)) 插入：O(log(n)) 删除：O(log(n)) 删除最大值/最小值：O(1) 哈希 哈希用于将任意长度的数据映射到固定长度的数据。哈希函数的返回值被称为哈希值、哈希码或者哈希。如果不同的主键得到相同的哈希值，则发生了冲突。 Hash Map ： hash map 是一个存储键值间关系的数据结构。HashMap 通过哈希函数将键转化为桶或者槽中的下标，从而便于指定值的查找。 冲突解决 链地址法（ Separate Chaining ） ：在链地址法中，每个桶（bucket）是相互独立的，每一个索引对应一个元素列表。处理HashMap 的时间就是查找桶的时间（常量）与遍历列表元素的时间之和。 开放地址法（ Open Addressing ） ：在开放地址方法中，当插入新值时，会判断该值对应的哈希桶是否存在，如果存在则根据某种算法依次选择下一个可能的位置，直到找到一个未被占用的地址。开放地址即某个元素的位置并不永远由其哈希值决定。 图 图是G =（V，E）的有序对，其包括顶点或节点的集合 V 以及边或弧的集合E，其中E包括了两个来自V的元素（即边与两个顶点相关联 ，并且该关联为这两个顶点的无序对）。 无向图 ：图的邻接矩阵是对称的，因此如果存在节点 u 到节点 v 的边，那节点 v 到节点 u 的边也一定存在。 有向图 ：图的邻接矩阵不是对称的。因此如果存在节点 u 到节点 v 的边并不意味着一定存在节点 v 到节点 u 的边。 算法排序快速排序 稳定：否 时间复杂度 最优：O(nlog(n)) 最差：O(n^2) 平均：O(nlog(n)) 合并排序 合并排序是一种分治算法。这个算法不断地将一个数组分为两部分，分别对左子数组和右子数组排序，然后将两个数组合并为新的有序数组。 稳定：是 时间复杂度： 最优：O(nlog(n)) 最差：O(nlog(n)) 平均：O(nlog(n)) 桶排序 桶排序是一种将元素分到一定数量的桶中的排序算法。每个桶内部采用其他算法排序，或递归调用桶排序。 时间复杂度 最优：Ω(n + k) 最差: O(n^2) 平均：Θ(n + k) 基数排序 基数排序类似于桶排序，将元素分发到一定数目的桶中。不同的是，基数排序在分割元素之后没有让每个桶单独进行排序，而是直接做了合并操作。 时间复杂度 最优：Ω(nk) 最差: O(nk) 平均：Θ(nk) 图算法深度优先 搜索 深度优先搜索是一种先遍历子节点而不回溯的图遍历算法。 时间复杂度：O(|V| + |E|) 广度优先 搜索 广度优先搜索是一种先遍历邻居节点而不是子节点的图遍历算法。 时间复杂度：O(|V| + |E|) 拓扑排序 拓扑排序是有向图节点的线性排序。对于任何一条节点 u 到节点 v 的边，u 的下标先于 v。 时间复杂度：O(|V| + |E|) Dijkstra算法 Dijkstra 算法是一种在有向图中查找单源最短路径的算法。 时间复杂度：O(|V|^2) Bellman-Ford算法 _Bellman-Ford _ 是一种在带权图中查找单一源点到其他节点最短路径的算法。 虽然时间复杂度大于 Dijkstra 算法，但它可以处理包含了负值边的图。 时间复杂度： 最优：O(|E|) 最差：O(|V||E|) Floyd-Warshall 算法 _Floyd-Warshall _ 算法是一种在无环带权图中寻找任意节点间最短路径的算法。 该算法执行一次即可找到所有节点间的最短路径（路径权重和）。 时间复杂度： 最优：O(|V|^3) 最差：O(|V|^3) 平均：O(|V|^3) 最小生成树算法 最小生成树算法是一种在无向带权图中查找最小生成树的贪心算法。换言之，最小生成树算法能在一个图中找到连接所有节点的边的最小子集。 时间复杂度：O(|V|^2) Kruskal 算法 _Kruskal _ 算法也是一个计算最小生成树的贪心算法，但在 Kruskal 算法中，图不一定是连通的。 时间复杂度：O(|E|log|V|) 贪心算法 贪心算法总是做出在当前看来最优的选择，并希望最后整体也是最优的。 使用贪心算法可以解决的问题必须具有如下两种特性： 最优子结构 问题的最优解包含其子问题的最优解。 贪心选择 每一步的贪心选择可以得到问题的整体最优解。 实例-硬币选择问题 给定期望的硬币总和为 V 分，以及 n 种硬币，即类型是 i 的硬币共有 coinValue[i] 分，i的范围是 [0…n – 1]。假设每种类型的硬币都有无限个，求解为使和为 V 分最少需要多少硬币？ 硬币：便士（1美分），镍（5美分），一角（10美分），四分之一（25美分）。 假设总和 V 为41,。我们可以使用贪心算法查找小于或者等于 V 的面值最大的硬币，然后从 V 中减掉该硬币的值，如此重复进行。 V = 41 | 使用了0个硬币 V = 16 | 使用了1个硬币(41 – 25 = 16) V = 6 | 使用了2个硬币(16 – 10 = 6) V = 1 | 使用了3个硬币(6 – 5 = 1) V = 0 | 使用了4个硬币(1 – 1 = 0) 位 运算 位运算即在比特级别进行操作的技术。使用位运算技术可以带来更快的运行速度与更小的内存使用。 测试第 k 位：s &amp; (1 &lt;&lt; k); 设置第k位：s |= (1 &lt;&lt; k); 关闭第k位：s &amp;= ~(1 &lt;&lt; k); 切换第k位：s ^= (1 &lt;&lt; k); 乘以2n：s &lt;&lt; n; 除以2n：s &gt;&gt; n; 交集：s &amp; t; 并集：s | t; 减法：s &amp; ~t; 提取最小非0位：s &amp; (-s); 提取最小0位：~s &amp; (s + 1); 交换值：x ^= y; y ^= x; x ^= y; 运行时分析大 O 表示 大 O 表示用于表示某个算法的上界，用于描述最坏的情况。 小 O 表示 小 O 表示用于描述某个算法的渐进上界，二者逐渐趋近。 大 Ω 表示 大 Ω 表示用于描述某个算法的渐进下界。 小 ω 表示 小 ω 表示用于描述某个算法的渐进下界，二者逐渐趋近。 Theta Θ 表示 Theta Θ 表示用于描述某个算法的确界，包括最小上界和最大下界。 视频教程 数据结构 UC Berkeley Data Structures MIT Advanced Data Structures 算法 MIT Introduction to Algorithms MIT Advanced Algorithms 面试宝典 Competitive Programming 3 – Steven Halim &amp; Felix Halim Cracking The Coding Interview – Gayle Laakmann McDowell Cracking The PM Interview – Gayle Laakmann McDowell &amp; Jackie Bavaro Introduction to Algorithms – Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest &amp; Clifford Stein 技术面试要了解的算法和数据结构知识","categories":[{"name":"Java 面试","slug":"Java-面试","permalink":"https://blog.fenxiangz.com/categories/Java-%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://blog.fenxiangz.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://blog.fenxiangz.com/tags/%E9%9D%A2%E8%AF%95/"},{"name":"数据结构","slug":"数据结构","permalink":"https://blog.fenxiangz.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"Java面试整理","slug":"interview/2018-10-25_Java面试整理","date":"2018-10-25T00:00:00.000Z","updated":"2020-12-20T16:47:02.955Z","comments":true,"path":"post/interview/2018-10-25_Java面试整理.html","link":"","permalink":"https://blog.fenxiangz.com/post/interview/2018-10-25_Java%E9%9D%A2%E8%AF%95%E6%95%B4%E7%90%86.html","excerpt":"","text":"1.谈谈你对 springmvc 的理解第一印象，以为问的 spring，但是别人问的好像是 springmvc ps: spring mvc 类似于 struts 的一个 MVC 开框架，其实都是属于 spring，spring mvc需要有 spring 的架包作为支撑才能跑起来，所以 spingMVC 只是 sping 中一个小功能，小模块而已。 继续谈 spring： Spring 的两大核心 AOP 与 IOC，可以单独用于任何应用，包括与 Struts 等 MVC 框架与Hibernate 等 ORM 框架的集成。 Spring 的核心是 Bean 工厂(Bean Factory)，用以构造我们所需要的(Model)。在此基础之上，Spring 提供了 AOP（Aspect-Oriented Programming, 面向层面的编程）的实现，用它来提供非管理环境下申明方式的事务、安全等服务； 对 Bean 工厂的扩展 ApplicationContext 更加方便我们实现 J2EE 的应用； DAO/ORM 的实现方便我们进行数据库的开发； Web MVC 和 Spring Web 提供了 Java Web 应用的框架或与其他流行的 Web框架进行集成。 那么 AOP 和 IOC 又是什么呢？ IOC 控制反转，简单的理解就是将类的创建和依赖关系写在配置文件里，由配置文件注入，实现了松耦合。 形象的说明：所谓依赖，从程序的角度看，就是比如 A 要调用 B 的方法，那么 A 就依赖于B，反正 A 要用到 B，则 A 依赖于 B。所谓倒置，你必须理解如果不倒置，会怎么着，因为A 必须要有 B，才可以调用 B，如果不倒置，意思就是 A 主动获取 B 的实例：B b = new B()，这就是最简单的获取 B 实例的方法（当然还有各种设计模式可以帮助你去获得 B 的实例，比如工厂、Locator 等等），然后你就可以调用 b 对象了。 所以，不依赖倒置(控制反转)，意味着 A 要主动获取 B，才能使用 B； AOP 面向切面，简单理解：就是将安全，事务等于程序逻辑相对独立的功能抽取出来，利用 spring 的配置文件将这些功能插进去，实现了按照方面编程，提高了复用性。 形象的说明：面向切面编程的目标就是分离关注点。什么是关注点呢？就是你要做的事，就是关注点。 从 Spring 的角度看，AOP 最大的用途就在于提供了事务管理的能力。事务管理就是一个关注点，你的正事就是去访问数据库，而你不想管事务（太烦），所以，Spring 在你访问数据库之前，自动帮你开启事务，当你访问数据库结束之后，自动帮你提交/回滚事务！ 2.关于 springmvc 和 structs 的对比很常见的问题，个人理解，从两方面来说： a&gt; 设计思想上： spring 可以理解为一个 servlet 的扩展，而 structs 则更符合面向对象思想，它是一个过滤器 filter. b&gt; 实现机制上：springmvc 的拦截是方法级别的拦截，一个方法对应一个 URL 以及一个request 和 response 对象，所以 URL 参数对于方法是独享的，而 structs 则是类级别的拦截，那么每一次请求的 URL 都会实例化这个 action 类，而类属性被所有方法共享，所以一个 action 共享一个 request 和 response 对象，参数的传递是通过 get/set 来实现。 c&gt; 对 于 ajax 的支持 ： springmvc 对 于 ajax 的支持更加简单 ，只需一个注解@ResponseBody ，然后直接返回响应文本即可，而 structs 则需要 out 流输出。 3.对于常见的 POST 请求乱码，该如何处理？常用的有三种方式：a&gt; 在 web.xml 中加入 encoding 来过滤 URLb&gt; 修改 Tomcat 的配置文件，添加编码，使服务器的编码和工程的编码一致c&gt; 对方法参数进行 String 编码 4.mybatis 的生命周期是怎么样的？a&gt; 创建 SqlSessionFactoryb&gt;通过 SqlSessionFactory 创建 SqlSessionc&gt;通过 sqlsession 执行数据库操作d&gt;调用 session.commit()提交事务e&gt;调用 session.close()关闭会话 5.mybatis 的配置文件有哪些内容？properties（属性）settings（配置）typeAliases（类型别名）typeHandlers（类型处理器）objectFactory（对象工厂）plugins（插件）environments（环境集合属性对象）environment（环境子属性对象）transactionManager（事务管理）dataSource（数据源）mappers（映射器） 6.使用 mybatis 的 mapper 接口调用时有哪些要求?a&gt; Mapper 接口方法名和 mapper.xml 中定义的每个 sql 的 id 相同b&gt; Mapper 接口方法的输入参数类型和 mapper.xml 中定义的每个 sql 的parameterType 的类型相同c&gt; Mapper 接口方法的输出参数类型和mapper.xml 中定义的每个sql 的 resultType的类型相同d&gt; Mapper.xml 文件中的 namespace 即是 mapper 接口的类路径。 7.mybatis 和 hibernate 的不同a&gt; Mybatis 和 hibernate 不同，它不完全是一个 ORM 框架，因为 MyBatis 需要程序员自己编写 Sql 语句，不过 mybatis 可以通过 XML 或注解方式灵活配置要运行的 sql 语句，并将 java 对象和 sql 语句映射生成最终执行的 sql，最后将 sql 执行的结果再映射生成 java 对象。b&gt; mybatis 相对于 hibernate 很灵活，体现在 sql 语句的可控性，但是这就对不同数据库的适应就要写复杂的配置文件。 8.mybatis 的缓存？a&gt; Mybatis 首先去缓存中查询结果集，如果没有则查询数据库，如果有则从缓存取出返回结果集就不走数据库。Mybatis 内部存储缓存使用一个 HashMap， key 为 hashCode + sqlId + Sql 语句。value 为从查询出来映射生成的 java 对象b&gt; Mybatis 的二级缓存即查询缓存，它的作用域是一个 mapper 的namespace，即在同一个 namespace 中查询 sql 可以从缓存中获取数据。二级缓存是可以跨 SqlSession的。 9.为什么使用 hibernate，好处是什么？a&gt; 对 JDBC 访问数据库的代码做了封装，大大简化了数据访问层繁琐的重复性代码。b&gt; hibernate 的性能非常好，因为它是个轻量级框架。映射的灵活性很出色。它支持各种关系数据库，从一对一到多对多的各种复杂关系。 10. 什么是 hibernate 的延迟加载？延迟加载机制是为了避免一些无谓的性能开销而提出来的，所谓延迟加载就是当在真正需要数据的时候，才真正执行数据加载操作。在 Hibernate 中提供了对实体对象的延迟加载以及对集合的延迟加载，另外在 Hibernate3 中还提供了对属性的延迟加载。 11. 怎么优化 hibernate？a&gt; 使用双向一对多关联，不使用单向一对多b&gt; 灵活使用单向一对多关联c&gt; 一对多集合使用 Bag,多对多集合使用 Setd&gt; 表字段要少，表关联可以使用二级缓存 12. hibernate 中 save 和 saveorupdate？save() 方法很显然是执行保存操作的，如果是对一个新的刚 new 出来的对象进行保存，自然要使用这个方法了，数据库中没有这个对象。update() 如果是对一个已经存在的托管对象进行更新那么肯定是要使用update() 方法了，数据中有这个对象。saveOrUpdate() 这个方法是更新或者插入，有主键就执行更新，如果没有主键就执行插入。在 Hibernate 中 saveOrUpdate()方法在执行的时候，先会去 session 中去找存不存在指定的字段，如果存在直接 update，否则 save，这个时候问题就发生了 13. 缓存技术？可做缓存的技术，Ehcache, Linkedhashmap, Memcached, Redis，视需求而定。其中，LinkedHashMap 和 Ehcache 都是单机缓存技术，即只能在一个应用内实现缓存，不能实现多台机器使用相同的缓存区域（分布式缓存），LinkedHashMap 的底层是用 HashMap 实现的，特点元素的排序是按链表方式排序，按写入或输出的顺序排序，最后一次写入或读取的元素放到最后。JDK 自带 Memcached、Redis 都可以适用于分布式缓存，需要独立部署，使多台机器可以使用同一个缓存服务器，实现集群的缓存共享，其中 redis 支持的数据类型更多，使用更方便，最重要的是：Memcached 的数据只能存在内存中，重启后即消失，而 Redis 可以持久化，因此 Redis 可以作为一个 NoSql 数据库使用。 14. 什么是反射机制？场景：一个大型的软件，不可能一次就把 它设计的很完美，当这个程序编译后，发布了，当发现需要更新某些功能时，我们不可能要用户把以前的卸载，再重新安装新的版本，假如这样的话，这个软件肯定 是没有多少人用的。采用静态的话，需要把整个程序重新编译一次才可以实现功能的更新，而采用反射机制的话，它就可以不用卸载，只需要在运行时才动态的创建和编译，就可以实现该功能。反射的优点当然是体现在它的动态性上面，能运行时确定类型，绑定对象。动态编译最大限度发挥了 java 的灵活性，体现了多态的应用，降低类之间的藕合性。 一句话，反射机制的优点就是可以实现动态创建对象和编译，缺点就是会消耗一些资源。 15. hashtable 和 hashmap 的区别？Hashtable 继承自 Dictionary 类，而 HashMap 是 Java1.2 引进的 Map interface 的一个实现。从 map 接口实现来看，HashMap 是 Hashtable 的轻量级实现（非线程安全的实现），HashMap 允许将 null 作为一个 entry 的 key 或者 value，而 Hashtable 不允许。由于是轻量级实现，hash 算法是一致的，不存在谁的效率高谁的效率低。 16. 接口和抽象类的区别？从两个方面来回答： a&gt; 语法层面：接口只能存在 public abstract 方法，而抽象类可以提供成员方法的具体实现。接口的成员变量是 public static final 类型的，而抽象类是没有限制的。接口不可以保护静态代码块以及静态方法，而抽象类是可以的。接口是可以多实现的，抽象类只能单继承。 b&gt; 设计层面：抽象类是对一种事物的抽象，即对类抽象，而接口是对行为的抽象。抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（行为）进行抽象。设计层面不同，抽象类作为很多子类的父类，它是一种模板式设计。而接口是一种行为规范，它是一种辐射式设计。什么是模板式设计？最简单例子，大家都用过 ppt 里面的模板。 17. 线程安全和不安全是什么意思？线程安全就是多线程访问时，采用了加锁机制，当一个线程访问该类的某个数据时，进行保护，其他线程不能进行访问直到该线程读取完，其他线程才可使用。不会出现数据不一致或者数据污染。线程不安全就是不提供数据访问保护，有可能出现多个线程先后更改数据造成所得到的数据是脏数据。 18. 怎么实现线程？在语法上有两种，但是他们其实是一种方式， Thread 类的实例就是一个线程但是它需要调用 Runnable 接口来执行，由于线程类本身就是调用的 Runnable 接口所以你可以继承Thread 类或者直接调用 Runnable 接口来重写 run()方法实现线程。 19. Thread 类中的 start() 和 run() 方法有什么区别?当你调用 run()方法的时候，只会是在原来的线程中调用，没有新的线程启动，start()方法才会启动新线程。 20. ThreadLocal 是什么？ThreadLocal 并不是一个 Thread，而是 Thread 的局部变量，当使用 ThreadLocal 维护变量时，ThreadLocal 为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立地改变自己的副本，而不会影响其它线程所对应的副本。 21. ThreadLocal 和线程同步相比的优势？ThreadLocal 和线程同步机制都是为了解决多线程中相同变量的访问冲突问题。 在同步机制中，通过对象的锁机制保证同一时间只有一个线程访问变量。这时该变量是多个线程共享的，使用同步机制要求程序慎密地分析什么时候对变量进行读写，什么时候需要锁定某个对象，什么时候释放对象锁等繁杂的问题，程序设计和编写难度相对较大。 而 ThreadLocal 则从另一个角度来解决多线程的并发访问。ThreadLocal 会为每一个线程提供一个独立的变量副本，从而隔离了多个线程对数据的访问冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal 提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进 ThreadLocal。 简单来说：对于多线程资源共享的问题，同步机制采用了“以时间换空间”的方式，而ThreadLocal 采用了“以空间换时间”的方式。前者仅提供一份变量，让不同的线程排队访问，而后者为每一个线程都提供了一份变量，因此可以同时访问而互不影响。 22. 谈谈你业务中的分布式？这个问题，从场景来回答，为什么需要分布式？随着互联网的发展，传统单工程项目的很多性能瓶颈越发凸显，性能瓶颈可以有几个方面。a&gt; 应用服务层：随着用户量的增加，并发量增加，单项目难以承受如此大的并发请求导致的性能瓶颈。b&gt; 底层数据库层：随着业务的发展，数据库压力越来越大，导致的性能瓶颈。 针对上面两点，我觉得可以从两方面解决。 应用服务层的解决方案有几种：应用系统集群：应用系统集群最简单的就是服务器集群，比如：tomcat 集群。应用系统集群的时候，比较凸显的问题是 session 共享，session 共享我们一是可以通过服务器插件来解决。另外一种也可以通过 redis 等中间件实现。 服务化拆分：服务化拆分，是目前非常火热的一种方式。现在都在提微服务话。通过对传统项目进行服务化拆分，达到服务独立解耦，单服务又可以横向扩容。服务化拆分遇到的经典问题就是分布式事务问题。目前，比较常用的分布式事务解决方案有几种：消息最终一致性、TCC 补偿型事务、尽最大能力通知。 底层数据库层：如果系统的性能压力出现在数据库，那我们就可以读写分离、分库分表等方案进行解决。 23. 怎么解决 session 共享的问题？其实，我也没处理过，查找了一些资料：a&gt; 客户端 cookie 加密。（一般用于内网中企业级的系统中，要求用户浏览器端的 cookie不能禁用，禁用的话，该方案会失效）。b&gt; 集群中，各个应用服务器提供了 session 复制的功能，tomcat 和 jboss 都实现了这样的功能。特点：性能随着服务器增加急剧下降，容易引起广播风暴；session 数据需要序列化，影响性能。c&gt; session 的持久化，使用数据库或者 redis 来保存 session。就算服务器宕机也没事儿，数据库中的 session 照样存在。特点：每次请求 session 都要读写数据库，会带来性能开销。使用内存数据库，会提高性能，但是宕机会丢失数据(像支付宝的宕机，有同城灾备、异地灾备)。d&gt; 用共享存储来保存 session。和数据库类似，就算宕机了也没有事儿。其实就是专门搞一台服务器，全部对 session 落地。特点：频繁的进行序列化和反序列化会影响性能。 From: https://hacpai.com/article/1492737273763","categories":[{"name":"Java 面试","slug":"Java-面试","permalink":"https://blog.fenxiangz.com/categories/Java-%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://blog.fenxiangz.com/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"Linux 两个文件求交集、并集、差集","slug":"linux/2017-12-16_Linux两个文件求交集、并集、差集","date":"2017-12-16T00:00:00.000Z","updated":"2020-12-20T16:47:02.979Z","comments":true,"path":"post/linux/2017-12-16_Linux两个文件求交集、并集、差集.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-12-16_Linux%E4%B8%A4%E4%B8%AA%E6%96%87%E4%BB%B6%E6%B1%82%E4%BA%A4%E9%9B%86%E3%80%81%E5%B9%B6%E9%9B%86%E3%80%81%E5%B7%AE%E9%9B%86.html","excerpt":"","text":"一、交集sort a.txt b.txt | uniq -d 二、并集sort a.txt b.txt | uniq 三、差集a.txt-b.txt: sort a.txt b.txt b.txt | uniq -u b.txt - a.txt: sort b.txt a.txt a.txt | uniq -u 四、相关的解释使用sort可以将文件进行排序，可以使用sort后面的玲玲，例如 -n 按照数字格式排序，例如 -i 忽略大小写，例如使用-r 为逆序输出等 uniq为删除文件中重复的行，得到文件中唯一的行，后面的命令 -d 表示的是输出出现次数大于1的内容 -u表示的是输出出现次数为1的内容，那么对于上述的求交集并集差集的命令做如下的解释： sort a.txt b.txt | uniq -d：将a.txt b.txt文件进行排序，uniq使得两个文件中的内容为唯一的，使用-d输出两个文件中次数大于1的内容，即是得到交集 sort a.txt b.txt | uniq ：将a.txt b.txt文件进行排序，uniq使得两个文件中的内容为唯一的，即可得到两个文件的并集 sort a.txt b.txt b.txt | uniq -u：将两个文件排序，最后输出a.txt b.txt b.txt文件中只出现过一次的内容，因为有两个b.txt所以只会输出只在a.txt出现过一次的内容，即是a.txt-b.txt差集 对于b.txt-a.txt为同理","categories":[{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"命令","slug":"命令","permalink":"https://blog.fenxiangz.com/tags/%E5%91%BD%E4%BB%A4/"},{"name":"集合运算","slug":"集合运算","permalink":"https://blog.fenxiangz.com/tags/%E9%9B%86%E5%90%88%E8%BF%90%E7%AE%97/"}]},{"title":"硬不硬你说了算！近 40 张图解被问千百遍的 TCP 三次握手和四次挥手面试题","slug":"network/2017-12-15_近40张图解被问千百遍的TCP","date":"2017-12-14T00:00:00.000Z","updated":"2020-12-20T16:47:02.983Z","comments":true,"path":"post/network/2017-12-15_近40张图解被问千百遍的TCP.html","link":"","permalink":"https://blog.fenxiangz.com/post/network/2017-12-15_%E8%BF%9140%E5%BC%A0%E5%9B%BE%E8%A7%A3%E8%A2%AB%E9%97%AE%E5%8D%83%E7%99%BE%E9%81%8D%E7%9A%84TCP.html","excerpt":"","text":"原文：https://xie.infoq.cn/article/964c8184326daff02aea8a746 每日一句英语学习，每天进步一点点： 前言不管面试 Java 、C/C++、Python 等开发岗位， TCP 的知识点可以说是的必问的了。 任 TCP 虐我千百遍，我仍待 TCP 如初恋。 遥想小林当年校招时常因 TCP 面试题被刷，真是又爱又狠…. 过去不会没关系，今天就让我们来消除这份恐惧，微笑着勇敢的面对它吧！ 所以小林整理了关于 TCP 三次握手和四次挥手的面试题型，跟大家一起探讨探讨。 1. TCP 基本认识 2. TCP 连接建立 3. TCP 连接断开 4. Socket 编程 PS：本次文章不涉及 TCP 流量控制、拥塞控制、可靠性传输等方面知识，这些留在下篇哈！ 正文01 TCP 基本认识 瞧瞧 TCP 头格式 我们先来看看 TCP 头的格式，标注颜色的表示与本文关联比较大的字段，其他字段不做详细阐述。 序列号：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。用来解决网络包乱序问题。 确认应答号：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。用来解决不丢包的问题。 控制位： ACK：该位为 1 时，「确认应答」的字段变为有效，TCP 规定除了最初建立连接时的 SYN 包之外该位必须设置为 1 。 RST：该位为 1 时，表示 TCP 连接中出现异常必须强制断开连接。 SYN：该位为 1 时，表示希望建立连接，并在其「序列号」的字段进行序列号初始值的设定。 FIN：该位为 1 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 FIN 位置为 1 的 TCP 段。 为什么需要 TCP 协议？ TCP 工作在哪一层？ IP 层是「不可靠」的，它不保证网络包的交付、不保证网络包的按序交付、也不保证网络包中的数据的完整性。 如果需要保障网络数据包的可靠性，那么就需要由上层（传输层）的 TCP 协议来负责。 因为 TCP 是一个工作在传输层的可靠数据传输的服务，它能确保接收端接收的网络包是无损坏、无间隔、非冗余和按序的。 什么是 TCP ？ TCP 是面向连接的、可靠的、基于字节流的传输层通信协议。 面向连接：一定是「一对一」才能连接，不能像 UDP 协议 可以一个主机同时向多个主机发送消息，也就是一对多是无法做到的； 可靠的：无论的网络链路中出现了怎样的链路变化，TCP 都可以保证一个报文一定能够到达接收端； 字节流：消息是「没有边界」的，所以无论我们消息有多大都可以进行传输。并且消息是「有序的」，当「前一个」消息没有收到的时候，即使它先收到了后面的字节已经收到，那么也不能扔给应用层去处理，同时对「重复」的报文会自动丢弃。 什么是 TCP 连接？ 我们来看看 RFC 793 是如何定义「连接」的： *Connections: The reliability and flow control mechanisms described above require that TCPs initialize and maintain certain status information for each data stream. The combination of this information, including sockets, sequence numbers, and window sizes, is called a connection.* 简单来说就是，用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括Socket、序列号和窗口大小称为连接。 所以我们可以知道，建立一个 TCP 连接是需要客户端与服务器端达成上述三个信息的共识。 Socket：由 IP 地址和端口号组成 序列号：用来解决乱序问题等 窗口大小：用来做流量控制 如何唯一确定一个 TCP 连接呢？ TCP 四元组可以唯一的确定一个连接，四元组包括如下： 源地址 源端口 目的地址 目的端口 源地址和目的地址的字段（32位）是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机。 源端口和目的端口的字段（16位）是在 TCP 头部中，作用是告诉 TCP 协议应该把报文发给哪个进程。 有一个 IP 的服务器监听了一个端口，它的 TCP 的最大连接数是多少？ 服务器通常固定在某个本地端口上监听，等待客户端的连接请求。 因此，客户端 IP 和 端口是可变的，其理论值计算公式如下: 对 IPv4，客户端的 IP 数最多为 2 的 32 次方，客户端的端口数最多为 2 的 16 次方，也就是服务端单机最大 TCP 连接数，约为 2 的 48 次方。 当然，服务端最大并发 TCP 连接数远不能达到理论上限。 首先主要是文件描述符限制，Socket 都是文件，所以首先要通过 ulimit 配置文件描述符的数目； 另一个是内存限制，每个 TCP 连接都要占用一定内存，操作系统是有限的。 UDP 和 TCP 有什么区别呢？分别的应用场景是？ UDP 不提供复杂的控制机制，利用 IP 提供面向「无连接」的通信服务。 UDP 协议真的非常简，头部只有 8 个字节（ 64 位），UDP 的头部格式如下： 目标和源端口：主要是告诉 UDP 协议应该把报文发给哪个进程。 包长度：该字段保存了 UDP 首部的长度跟数据的长度之和。 校验和：校验和是为了提供可靠的 UDP 首部和数据而设计。 TCP 和 UDP 区别： 连接 TCP 是面向连接的传输层协议，传输数据前先要建立连接。 UDP 是不需要连接，即刻传输数据。 服务对象 TCP 是一对一的两点服务，即一条连接只有两个端点。 UDP 支持一对一、一对多、多对多的交互通信 可靠性 TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达。 UDP 是尽最大努力交付，不保证可靠交付数据。 拥塞控制、流量控制 TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。 UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。 首部开销 TCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 20 个字节，如果使用了「选项」字段则会变长的。 UDP 首部只有 8 个字节，并且是固定不变的，开销较小。 TCP 和 UDP 应用场景： 由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于： FTP 文件传输 HTTP / HTTPS 由于 UDP 面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于： 包总量较少的通信，如 DNS 、SNMP 等 视频、音频等多媒体通信 广播通信 为什么 UDP 头部没有「首部长度」字段，而 TCP 头部有「首部长度」字段呢？ 原因是 TCP 有可变长的「选项」字段，而 UDP 头部长度则是不会变化的，无需多一个字段去记录 UDP 的首部长度。 为什么 UDP 头部有「包长度」字段，而 TCP 头部则没有「包长度」字段呢？ 先说说 TCP 是如何计算负载数据长度： 其中 IP 总长度 和 IP 首部长度，在 IP 首部格式是已知的。TCP 首部长度，则是在 TCP 首部格式已知的，所以就可以求得 TCP 数据的长度。 大家这时就奇怪了问：“ UDP 也是基于 IP 层的呀，那 UDP 的数据长度也可以通过这个公式计算呀？ 为何还要有「包长度」呢？” 这么一问，确实感觉 UDP 「包长度」是冗余的。 因为为了网络设备硬件设计和处理方便，首部长度需要是 4 字节的整数倍。 如果去掉 UDP 「包长度」字段，那 UDP 首部长度就不是 4 字节的整数倍了，所以小林觉得这可能是为了补全 UDP 首部长度是 4 字节的整数倍，才补充了「包长度」字段。 02 TCP 连接建立 TCP 三次握手过程和状态变迁 TCP 是面向连接的协议，所以使用 TCP 前必须先建立连接，而建立连接是通过三次握手而进行的。 一开始，客户端和服务端都处于 CLOSED 状态。先是服务端主动监听某个端口，处于 LISTEN 状态 客户端会随机初始化序号（client_isn），将此序号置于 TCP 首部的「序号」字段中，同时把 SYN 标志位置为 1 ，表示 SYN 报文。接着把第一个 SYN 报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 SYN-SENT 状态。 服务端收到客户端的 SYN 报文后，首先服务端也随机初始化自己的序号（server_isn），将此序号填入 TCP 首部的「序号」字段中，其次把 TCP 首部的「确认应答号」字段填入 client_isn + 1, 接着把 SYN 和 ACK 标志位置为 1。最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 SYN-RCVD 状态。 客户端收到服务端报文后，还要向服务端回应最后一个应答报文，首先该应答报文 TCP 首部 ACK 标志位置为 1 ，其次「确认应答号」字段填入 server_isn + 1 ，最后把报文发送给服务端，这次报文可以携带客户到服务器的数据，之后客户端处于 ESTABLISHED 状态。 服务器收到客户端的应答报文后，也进入 ESTABLISHED 状态。 从上面的过程可以发现第三次握手是可以携带数据的，前两次握手是不可以携带数据的，这也是面试常问的题。 一旦完成三次握手，双方都处于 ESTABLISHED 状态，此致连接就已建立完成，客户端和服务端就可以相互发送数据了。 如何在 Linux 系统中查看 TCP 状态？ TCP 的连接状态查看，在 Linux 可以通过 netstat -napt 命令查看。 为什么是三次握手？不是两次、四次？ 相信大家比较常回答的是：“因为三次握手才能保证双方具有接收和发送的能力。” 这回答是没问题，但这回答是片面的，并没有说出主要的原因。 在前面我们知道了什么是 TCP 连接： 用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括Socket、序列号和窗口大小称为连接。 所以，重要的是为什么三次握手才可以初始化Socket、序列号和窗口大小并建立 TCP 连接。 接下来以三个方面分析三次握手的原因： 三次握手才可以阻止重复历史连接的初始化（主要原因） 三次握手才可以同步双方的初始序列号 三次握手才可以避免资源浪费 原因一：避免历史连接 我们来看看 RFC 793 指出的 TCP 连接使用三次握手的首要原因： The principle reason for the three-way handshake is to prevent old duplicate connection initiations from causing confusion. 简单来说，三次握手的首要原因是为了防止旧的重复连接初始化造成混乱。 网络环境是错综复杂的，往往并不是如我们期望的一样，先发送的数据包，就先到达目标主机，反而它很骚，可能会由于网络拥堵等乱七八糟的原因，会使得旧的数据包，先到达目标主机，那么这种情况下 TCP 三次握手是如何避免的呢？ 客户端连续发送多次 SYN 建立连接的报文，在网络拥堵等情况下： 一个「旧 SYN 报文」比「最新的 SYN 」 报文早到达了服务端； 那么此时服务端就会回一个 SYN + ACK 报文给客户端； 客户端收到后可以根据自身的上下文，判断这是一个历史连接（序列号过期或超时），那么客户端就会发送 RST 报文给服务端，表示中止这一次连接。 如果是两次握手连接，就不能判断当前连接是否是历史连接，三次握手则可以在客户端（发送方）准备发送第三次报文时，客户端因有足够的上下文来判断当前连接是否是历史连接： 如果是历史连接（序列号过期或超时），则第三次握手发送的报文是 RST 报文，以此中止历史连接； 如果不是历史连接，则第三次发送的报文是 ACK 报文，通信双方就会成功建立连接； 所以， TCP 使用三次握手建立连接的最主要原因是防止历史连接初始化了连接。 原因二：同步双方初始序列号 TCP 协议的通信双方， 都必须维护一个「序列号」， 序列号是可靠传输的一个关键因素，它的作用： 接收方可以去除重复的数据； 接收方可以根据数据包的序列号按序接收； 可以标识发送出去的数据包中， 哪些是已经被对方收到的； 可见，序列号在 TCP 连接中占据着非常重要的作用，所以当客户端发送携带「初始序列号」的 SYN 报文的时候，需要服务端回一个 ACK 应答报文，表示客户端的 SYN 报文已被服务端成功接收，那当服务端发送「初始序列号」给客户端的时候，依然也要得到客户端的应答回应，这样一来一回，才能确保双方的初始序列号能被可靠的同步。 四次握手其实也能够可靠的同步双方的初始化序号，但由于第二步和第三步可以优化成一步，所以就成了「三次握手」。 而两次握手只保证了一方的初始序列号能被对方成功接收，没办法保证双方的初始序列号都能被确认接收。 原因三：避免资源浪费 如果只有「两次握手」，当客户端的 SYN 请求连接在网络中阻塞，客户端没有接收到 ACK 报文，就会重新发送 SYN ，由于没有第三次握手，服务器不清楚客户端是否收到了自己发送的建立连接的 ACK 确认信号，所以每收到一个 SYN 就只能先主动建立一个连接，这会造成什么情况呢？ 如果客户端的 SYN 阻塞了，重复发送多次 SYN 报文，那么服务器在收到请求后就会建立多个冗余的无效链接，造成不必要的资源浪费。 即两次握手会造成消息滞留情况下，服务器重复接受无用的连接请求 SYN 报文，而造成重复分配资源。 小结 TCP 建立连接时，通过三次握手能防止历史连接的建立，能减少双方不必要的资源开销，能帮助双方同步初始化序列号。序列号能够保证数据包不重复、不丢弃和按序传输。 不使用「两次握手」和「四次握手」的原因： 「两次握手」：无法防止历史连接的建立，会造成双方资源的浪费，也无法可靠的同步双方序列号； 「四次握手」：三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数。 为什么客户端和服务端的初始序列号 ISN 是不相同的？ 因为网络中的报文会延迟、会复制重发、也有可能丢失，这样会造成的不同连接之间产生互相影响，所以为了避免互相影响，客户端和服务端的初始序列号是随机且不同的。 初始序列号 ISN 是如何随机产生的？ 起始 ISN 是基于时钟的，每 4 毫秒 + 1，转一圈要 4.55 个小时。 RFC1948 中提出了一个较好的初始化序列号 ISN 随机生成算法。 ISN = M + F (localhost, localport, remotehost, remoteport) M 是一个计时器，这个计时器每隔 4 毫秒加 1。 F 是一个 Hash 算法，根据源 IP、目的 IP、源端口、目的端口生成一个随机数值。要保证 Hash 算法不能被外部轻易推算得出，用 MD5 算法是一个比较好的选择。 既然 IP 层会分片，为什么 TCP 层还需要 MSS 呢？ 我们先来认识下 MTU 和 MSS MTU：一个网络包的最大长度，以太网中一般为 1500 字节； MSS：除去 IP 和 TCP 头部之后，一个网络包所能容纳的 TCP 数据的最大长度； 如果在 TCP 的整个报文（头部 + 数据）交给 IP 层进行分片，会有什么异常呢？ 当 IP 层有一个超过 MTU 大小的数据（TCP 头部 + TCP 数据）要发送，那么 IP 层就要进行分片，把数据分片成若干片，保证每一个分片都小于 MTU。把一份 IP 数据报进行分片以后，由目标主机的 IP 层来进行重新组装后，在交给上一层 TCP 传输层。 这看起来井然有序，但这存在隐患的，那么当如果一个 IP 分片丢失，整个 IP 报文的所有分片都得重传。 因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。 当接收方发现 TCP 报文（头部 + 数据）的某一片丢失后，则不会响应 ACK 给对方，那么发送方的 TCP 在超时后，就会重发「整个 TCP 报文（头部 + 数据）」。 因此，可以得知由 IP 层进行分片传输，是非常没有效率的。 所以，为了达到最佳的传输效能 TCP 协议在建立连接的时候通常要协商双方的 MSS 值，当 TCP 层发现数据超过 MSS 时，则就先会进行分片，当然由它形成的 IP 包的长度也就不会大于 MTU ，自然也就不用 IP 分片了。 经过 TCP 层分片后，如果一个 TCP 分片丢失后，进行重发时也是以 MSS 为单位，而不用重传所有的分片，大大增加了重传的效率。 什么是 SYN 攻击？如何避免 SYN 攻击？ SYN 攻击 我们都知道 TCP 连接建立是需要三次握手，假设攻击者短时间伪造不同 IP 地址的 SYN 报文，服务端每接收到一个 SYN 报文，就进入SYN_RCVD 状态，但服务端发送出去的 ACK + SYN 报文，无法得到未知 IP 主机的 ACK 应答，久而久之就会占满服务端的 SYN 接收队列（未连接队列），使得服务器不能为正常用户服务。 避免 SYN 攻击方式一 其中一种解决方式是通过修改 Linux 内核参数，控制队列大小和当队列满时应做什么处理。 当网卡接收数据包的速度大于内核处理的速度时，会有一个队列保存这些数据包。控制该队列的最大值如下参数： net.core.netdev_max_backlog SYN_RCVD 状态连接的最大个数： net.ipv4.tcp_max_syn_backlog 超出处理能时，对新的 SYN 直接回报 RST，丢弃连接： net.ipv4.tcp_abort_on_overflow 避免 SYN 攻击方式二 我们先来看下 Linux 内核的 SYN （未完成连接建立）队列与 Accpet （已完成连接建立）队列是如何工作的？ 正常流程： 当服务端接收到客户端的 SYN 报文时，会将其加入到内核的「 SYN 队列」； 接着发送 SYN + ACK 给客户端，等待客户端回应 ACK 报文； 服务端接收到 ACK 报文后，从「 SYN 队列」移除放入到「 Accept 队列」； 应用通过调用 accpet() socket 接口，从「 Accept 队列」取出的连接。 应用程序过慢： 如果应用程序过慢时，就会导致「 Accept 队列」被占满。 受到 SYN 攻击： 如果不断受到 SYN 攻击，就会导致「 SYN 队列」被占满。 tcp_syncookies 的方式可以应对 SYN 攻击的方法： net.ipv4.tcp_syncookies = 1 当 「 SYN 队列」满之后，后续服务器收到 SYN 包，不进入「 SYN 队列」； 计算出一个 cookie 值，再以 SYN + ACK 中的「序列号」返回客户端， 服务端接收到客户端的应答报文时，服务器会检查这个 ACK 包的合法性。如果合法，直接放入到「 Accept 队列」。 最后应用通过调用 accpet() socket 接口，从「 Accept 队列」取出的连接。 03 TCP 连接断开 TCP 四次挥手过程和状态变迁 天下没有不散的宴席，对于 TCP 连接也是这样， TCP 断开连接是通过四次挥手方式。 双方都可以主动断开连接，断开连接后主机中的「资源」将被释放。 客户端打算关闭连接，此时会发送一个 TCP 首部 FIN 标志位被置为 1 的报文，也即 FIN 报文，之后客户端进入 FINWAIT1 状态。 服务端收到该报文后，就向客户端发送 ACK 应答报文，接着服务端进入 CLOSED_WAIT 状态。 客户端收到服务端的 ACK 应答报文后，之后进入 FIN_WAIT_2 状态。 等待服务端处理完数据后，也向客户端发送 FIN 报文，之后服务端进入 LAST_ACK 状态。 客户端收到服务端的 FIN 报文后，回一个 ACK 应答报文，之后进入 TIME_WAIT 状态 服务器收到了 ACK 应答报文后，就进入了 CLOSE 状态，至此服务端已经完成连接的关闭。 客户端在经过 2MSL 一段时间后，自动进入 CLOSE 状态，至此客户端也完成连接的关闭。 你可以看到，每个方向都需要一个 FIN 和一个 ACK，因此通常被称为四次挥手。 这里一点需要注意是：主动关闭连接的，才有 TIME_WAIT 状态。 为什么挥手需要四次？ 再来回顾下四次挥手双方发 FIN 包的过程，就能理解为什么需要四次了。 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。 服务器收到客户端的 FIN 报文时，先回一个 ACK 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 FIN 报文给客户端来表示同意现在关闭连接。 从上面过程可知，服务端通常需要等待完成数据的发送和处理，所以服务端的 ACK 和 FIN 一般都会分开发送，从而比三次握手导致多了一次。 为什么 TIME_WAIT 等待的时间是 2MSL？ MSL 是 Maximum Segment Lifetime，报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 IP 协议的，而 IP 头中有一个 TTL 字段，是 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。 MSL 与 TTL 的区别： MSL 的单位是时间，而 TTL 是经过路由跳数。所以 MSL 应该要大于等于 TTL 消耗为 0 的时间，以确保报文已被自然消亡。 TIME_WAIT 等待 2 倍的 MSL，比较合理的解释是： 网络中可能存在来自发送方的数据包，当这些发送方的数据包被接收方处理后又会向对方发送响应，所以一来一回需要等待 2 倍的时间。 比如如果被动关闭方没有收到断开连接的最后的 ACK 报文，就会触发超时重发 Fin 报文，另一方接收到 FIN 后，会重发 ACK 给被动关闭方， 一来一去正好 2 个 MSL。 2MSL 的时间是从客户端接收到 FIN 后发送 ACK 开始计时的。如果在 TIME-WAIT 时间内，因为客户端的 ACK 没有传输到服务端，客户端又接收到了服务端重发的 FIN 报文，那么 2MSL 时间将重新计时。 在 Linux 系统里 2MSL 默认是 60 秒，那么一个 MSL 也就是 30 秒。Linux 系统停留在 TIME_WAIT 的时间为固定的 60 秒。 其定义在 Linux 内核代码里的名称为 TCPTIMEWAITLEN： #define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT state, about 60 seconds */ 如果要修改 TIMEWAIT 的时间长度，只能修改 Linux 内核代码里 TCPTIMEWAIT_LEN 的值，并重新编译 Linux 内核。 **为什么需要 TIME_WAIT 状态？ ** 主动发起关闭连接的一方，才会有 TIME-WAIT 状态。 需要 TIME-WAIT 状态，主要是两个原因： 防止具有相同「四元组」的「旧」数据包被收到； 保证「被动关闭连接」的一方能被正确的关闭，即保证最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭； 原因一：防止旧连接的数据包 假设 TIME-WAIT 没有等待时间或时间过短，被延迟的数据包抵达后会发生什么呢？ 如上图黄色框框服务端在关闭连接之前发送的 SEQ = 301 报文，被网络延迟了。 这时有相同端口的 TCP 连接被复用后，被延迟的 SEQ = 301 抵达了客户端，那么客户端是有可能正常接收这个过期的报文，这就会产生数据错乱等严重的问题。 所以，TCP 就设计出了这么一个机制，经过 2MSL 这个时间，足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。 原因二：保证连接正确关闭 在 RFC 793 指出 TIME-WAIT 另一个重要的作用是： TIME-WAIT - represents waiting for enough time to pass to be sure the remote TCP received the acknowledgment of its connection termination request. 也就是说，TIME-WAIT 作用是等待足够的时间以确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。 假设 TIME-WAIT 没有等待时间或时间过短，断开连接会造成什么问题呢？ 如上图红色框框客户端四次挥手的最后一个 ACK 报文如果在网络中被丢失了，此时如果客户端 TIME-WAIT 过短或没有，则就直接进入了 CLOSE 状态了，那么服务端则会一直处在 LASE-ACK 状态。 当客户端发起建立连接的 SYN 请求报文后，服务端会发送 RST 报文给客户端，连接建立的过程就会被终止。 如果 TIME-WAIT 等待足够长的情况就会遇到两种情况： 服务端正常收到四次挥手的最后一个 ACK 报文，则服务端正常关闭连接。 服务端没有收到四次挥手的最后一个 ACK 报文时，则会重发 FIN 关闭连接报文并等待新的 ACK 报文。 所以客户端在 TIME-WAIT 状态等待 2MSL 时间后，就可以保证双方的连接都可以正常的关闭。 TIME_WAIT 过多有什么危害？ 如果服务器有处于 TIME-WAIT 状态的 TCP，则说明是由服务器方主动发起的断开请求。 过多的 TIME-WAIT 状态主要的危害有两种： 第一是内存资源占用； 第二是对端口资源的占用，一个 TCP 连接至少消耗一个本地端口； 第二个危害是会造成严重的后果的，要知道，端口资源也是有限的，一般可以开启的端口为 32768～61000，也可以通过如下参数设置指定 net.ipv4.ip_local_port_range 如果发起连接一方的 TIME_WAIT 状态过多，占满了所有端口资源，则会导致无法创建新连接。 客户端受端口资源限制： 客户端TIME_WAIT过多，就会导致端口资源被占用，因为端口就65536个，被占满就会导致无法创建新的连接。 服务端受系统资源限制： 由于一个TCP四元组表示TCP连接，理论上服务端可以建立很多连接，服务端确实只监听一个端口 但是会把连接扔给处理线程，所以理论上监听的端口可以继续监听。但是线程池处理不了那么多一直不断的连接了。所以当服务端出现大量TIMEWAIT时，系统资源被占满时，会导致处理不过来新的连接。 如何优化 TIME_WAIT？ 这里给出优化 TIME-WAIT 的几个方式，都是有利有弊： 打开 net.ipv4.tcptwreuse 和 net.ipv4.tcp_timestamps 选项； net.ipv4.tcpmaxtw_buckets 程序中使用 SO_LINGER ，应用强制使用 RST 关闭。 方式一：net.ipv4.tcp_tw_reuse 和 tcp_timestamps 如下的 Linux 内核参数开启后，则可以复用处于 TIME_WAIT 的 socket 为新的连接所用。 net.ipv4.tcp_tw_reuse = 1 使用这个选项，还有一个前提，需要打开对 TCP 时间戳的支持，即 net.ipv4.tcp_timestamps=1（默认即为 1） 这个时间戳的字段是在 TCP 头部的「选项」里，用于记录 TCP 发送方的当前时间戳和从对端接收到的最新时间戳。 由于引入了时间戳，我们在前面提到的 2MSL 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃。 方式二：net.ipv4.tcp_max_tw_buckets 这个值默认为 18000，当系统中处于 TIME_WAIT 的连接一旦超过这个值时，系统就会将所有的 TIME_WAIT 连接状态重置。 这个方法过于暴力，而且治标不治本，带来的问题远比解决的问题多，不推荐使用。 方式三：程序中使用 SO_LINGER 我们可以通过设置 socket 选项，来设置调用 close 关闭连接行为。 structlingerso_linger; so_linger.l_onoff = 1; so_linger.l_linger = 0; setsockopt(s, SOL_SOCKET, SO_LINGER, &amp;so_linger,sizeof(so_linger)); 如果l_onoff为非 0， 且l_linger值为 0，那么调用close后，会立该发送一个RST标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了TIME_WAIT状态，直接关闭。 但这为跨越TIME_WAIT状态提供了一个可能，不过是一个非常危险的行为，不值得提倡。 #### 如果已经建立了连接，但是客户端突然出现故障了怎么办？ TCP 有一个机制是保活机制。这个机制的原理是这样的： 定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。 在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值： net.ipv4.tcp_keepalive_time=7200 net.ipv4.tcp_keepalive_intvl=75 net.ipv4.tcp_keepalive_probes=9 tcpkeepalivetime=7200：表示保活时间是 7200 秒（2小时），也就 2 小时内如果没有任何连接相关的活动，则会启动保活机制 tcpkeepaliveintvl=75：表示每次检测间隔 75 秒； tcpkeepaliveprobes=9：表示检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。 也就是说在 Linux 系统中，最少需要经过 2 小时 11 分 15 秒才可以发现一个「死亡」连接。 这个时间是有点长的，我们也可以根据实际的需求，对以上的保活相关的参数进行设置。 如果开启了 TCP 保活，需要考虑以下几种情况： 第一种，对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 TCP 保活时间会被重置，等待下一个 TCP 保活时间的到来。 第二种，对端程序崩溃并重启。当 TCP 保活的探测报文发送给对端后，对端是可以响应的，但由于没有该连接的有效信息，会产生一个 RST 报文，这样很快就会发现 TCP 连接已经被重置。 第三种，是对端程序崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，TCP 会报告该 TCP 连接已经死亡。 03 Socket 编程 针对 TCP 应该如何 Socket 编程？ 服务端和客户端初始化 socket，得到文件描述符； 服务端调用 bind，将绑定在 IP 地址和端口; 服务端调用 listen，进行监听； 服务端调用 accept，等待客户端连接； 客户端调用 connect，向服务器端的地址和端口发起连接请求； 服务端 accept 返回用于传输的 socket 的文件描述符； 客户端调用 write 写入数据；服务端调用 read 读取数据； 客户端断开连接时，会调用 close，那么服务端 read 读取数据的时候，就会读取到了 EOF，待处理完数据后，服务端调用 close，表示连接关闭。 这里需要注意的是，服务端调用 accept 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。 所以，监听的 socket 和真正用来传送数据的 socket，是「两个」 socket，一个叫作监听 socket，一个叫作已完成连接 socket。 成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。 listen 时候参数 backlog 的意义？ Linux内核中会维护两个队列： 未完成连接队列（SYN 队列）：接收到一个 SYN 建立连接请求，处于 SYN_RCVD 状态； 已完成连接队列（Accpet 队列）：已完成 TCP 三次握手过程，处于 ESTABLISHED 状态； intlisten(int socketfd, int backlog) 参数一 socketfd 为 socketfd 文件描述符 参数二 backlog，这参数在历史有一定的变化 在 早期 Linux 内核 backlog 是 SYN 队列大小，也就是未完成的队列大小。 在 Linux 内核 2.2 之后，backlog 变成 accept 队列，也就是已完成连接建立的队列长度，所以现在通常认为 backlog 是 accept 队列。 accept 发送在三次握手的哪一步？ 我们先看看客户端连接服务端时，发送了什么？ 客户端的协议栈向服务器端发送了 SYN 包，并告诉服务器端当前发送序列号 clientisn，客户端进入 SYNCSENT 状态； 服务器端的协议栈收到这个包之后，和客户端进行 ACK 应答，应答的值为 clientisn+1，表示对 SYN 包 clientisn 的确认，同时服务器也发送一个 SYN 包，告诉客户端当前我的发送序列号为 serverisn，服务器端进入 SYNCRCVD 状态； 客户端协议栈收到 ACK 之后，使得应用程序从 connect 调用返回，表示客户端到服务器端的单向连接建立成功，客户端的状态为 ESTABLISHED，同时客户端协议栈也会对服务器端的 SYN 包进行应答，应答数据为 server_isn+1； 应答包到达服务器端后，服务器端协议栈使得 accept 阻塞调用返回，这个时候服务器端到客户端的单向连接也建立成功，服务器端也进入 ESTABLISHED 状态。 从上面的描述过程，我们可以得知客户端 connect 成功返回是在第二次握手，服务端 accept 成功返回是在三次握手成功之后。 客户端调用 close 了，连接是断开的流程是什么？ 我们看看客户端主动调用了 close，会发生什么？ 客户端调用 close，表明客户端没有数据需要发送了，则此时会向服务端发送 FIN 报文，进入 FINWAIT1 状态； 服务端接收到了 FIN 报文，TCP 协议栈会为 FIN 包插入一个文件结束符 EOF 到接收缓冲区中，应用程序可以通过 read 调用来感知这个 FIN 包。这个 EOF 会被放在已排队等候的其他已接收的数据之后，这就意味着服务端需要处理这种异常情况，因为 EOF 表示在该连接上再无额外数据到达。此时，服务端进入 CLOSE_WAIT 状态； 接着，当处理完数据后，自然就会读到 EOF，于是也调用 close 关闭它的套接字，这会使得会发出一个 FIN 包，之后处于 LAST_ACK 状态； 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进入 TIME_WAIT 状态； 服务端收到 ACK 确认包后，就进入了最后的 CLOSE 状态； 客户端进过 2MSL 时间之后，也进入 CLOSED 状态； [1] 趣谈网络协议专栏.刘超.极客时间. [2] 网络编程实战专栏.盛延敏.极客时间. [3] 计算机网络-自顶向下方法.陈鸣 译.机械工业出版社 [4] TCP/IP详解 卷1：协议.范建华 译.机械工业出版社 [5] 图解TCP/IP.竹下隆史.人民邮电出版社 [6] https://www.rfc-editor.org/rfc/rfc793.html [7] https://draveness.me/whys-the-design-tcp-three-way-handshake [9] https://draveness.me/whys-the-design-tcp-time-wait/","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"https://blog.fenxiangz.com/categories/TCP-IP/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://blog.fenxiangz.com/tags/%E9%9D%A2%E8%AF%95/"},{"name":"协议","slug":"协议","permalink":"https://blog.fenxiangz.com/tags/%E5%8D%8F%E8%AE%AE/"},{"name":"TCP","slug":"TCP","permalink":"https://blog.fenxiangz.com/tags/TCP/"}]},{"title":"帧、报文、报文段、分组、包、数据报的概念区别","slug":"network/2017-12-13_网络传输术语概念","date":"2017-12-13T00:00:00.000Z","updated":"2020-12-20T16:47:02.982Z","comments":true,"path":"post/network/2017-12-13_网络传输术语概念.html","link":"","permalink":"https://blog.fenxiangz.com/post/network/2017-12-13_%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93%E6%9C%AF%E8%AF%AD%E6%A6%82%E5%BF%B5.html","excerpt":"","text":"分组、包，packet，信息在互联网当中传输的单元，网络层实现分组交付。用抓包工具抓到的一条条记录就是包。 帧，frame，数据链路层的协议数据单元。我们将链路层分组称为帧。 数据报，Datagram，通过网络传输的数据的基本单元，包含一个报头（header）和数据本身，其中报头描述了数据的目的地以及和其它数据之间的关系。可以理解为传输数据的分组。我们将通过网络传输的数据的基本单元称为数据报。 报文段，segment，组成报文的每个分组。我们将运输层分组称为报文段。 报文，message，一般指完整的信息，传输层实现报文交付。我们将位于应用层的信息分组称为报文。 由此可见，抓包抓到的是传输层的包，所以packet，frame，Datagram，segment是存在于同条记录中的，而frame，Datagram，segment是基于所在协议层不同而取了不同的名字。 1. 报文(message)报文是网络中交换与传输的数据单元，也是网络传输的单元。报文包含了将要发送的完整的数据信息，其长短不需一致。报文在传输过程中会不断地封装成分组、包、帧来传输，封装的方式就是添加一些控制信息组成的首部，那些就是报文头。 2. 分组(packet)分组是在网络中传输的二进制格式的单元，为了提供通信性能和可靠性，每个用户发送的数据会被分成多个更小的部分。在每个部分的前面加上一些必要的控制信息组成的首部，有时也会加上尾部，就构成了一个分组。 3. 数据包(data packet)数据包是TCP/IP协议通信传输中的数据单元，也称为“包”。是指自包含的，带有足够寻址信息，可独立地从源主机传输到目的主机，而不需要依赖早期的源主机和目的主机之间交换信息以及传输网络的数据包。 4. 数据报(datagram)面向无连接的数据传输，其工作过程类似于报文交换。采用数据报方式传输时，被传输的分组称为数据报。 5. 帧(frame)帧是数据链路层的传输单元。它将上层传入的数据添加一个头部和尾部，组成了帧。 应用层——消息传输层——数据段(segment)网络层——分组、数据包（packet）链路层——帧（frame）物理层——P-PDU（bit）（《计算机网络——自顶向下方法》）","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"https://blog.fenxiangz.com/categories/TCP-IP/"}],"tags":[{"name":"概念","slug":"概念","permalink":"https://blog.fenxiangz.com/tags/%E6%A6%82%E5%BF%B5/"},{"name":"网络术语","slug":"网络术语","permalink":"https://blog.fenxiangz.com/tags/%E7%BD%91%E7%BB%9C%E6%9C%AF%E8%AF%AD/"}]},{"title":"TCP协议中的seq/ack序号是如何变化的？","slug":"network/2017-12-14_TCP协议中的seq_ack序号是如何变化的？","date":"2017-12-13T00:00:00.000Z","updated":"2020-12-20T16:47:02.982Z","comments":true,"path":"post/network/2017-12-14_TCP协议中的seq_ack序号是如何变化的？.html","link":"","permalink":"https://blog.fenxiangz.com/post/network/2017-12-14_TCP%E5%8D%8F%E8%AE%AE%E4%B8%AD%E7%9A%84seq_ack%E5%BA%8F%E5%8F%B7%E6%98%AF%E5%A6%82%E4%BD%95%E5%8F%98%E5%8C%96%E7%9A%84%EF%BC%9F.html","excerpt":"","text":"这里提供了截取出来的一次client端和server端TCP包的交互过程。建议将图单独放到一台设备、或者打印出来查看，以便不断核对下述内容。 图：TCP数据包交换过程 再开始分析之前，还需要论述一下seq、ack表示什么意思，应该以什么样的角度去理解这两个序列号。 sequence number：表示的是我方（发送方）这边，这个packet的数据部分的第一位应该在整个data stream中所在的位置。（注意这里使用的是“应该”。因为对于没有数据的传输，如ACK，虽然它有一个seq，但是这次传输在整个data stream中是不占位置的。所以下一个实际有数据的传输，会依旧从上一次发送ACK的数据包的seq开始） acknowledge number：表示的是期望的对方（接收方）的下一次sequence number是多少。 注意，SYN/FIN的传输虽然没有data，但是会让下一次传输的packet seq增加一，但是，ACK的传输，不会让下一次的传输packet加一。 上面这几条原则第一次读会有些抽象，可以先继续往下读分析过程，再回过头来查看这个三个原则。 1、 seq：client端第一次发送packet，即：first-way handshake。所以按照上面的准则，它的数据应该从第一个开始，也即是第0位开始，所以seq为0。 ack：而server端之前并未发送过数据，所以期望的是server端回传时的packet的seq应该从第一个开始，即是第0位开始，所以ack为0。 2、 seq：server端第一次发送packet，即：second-way handshake。所以，这个packet的seq为0。 ack：由于在【no.1】中接收到的是client端的SYN数据包，且它的seq为0，所以client端会让它自己的seq增加1。由此可预计（expect），client端的下一次packet传输时，它的seq是1（0增加1）。所以，ACK为1。 3、 seq：third-way handshake。上一次发送时为【no.1】，【no.1】中seq为0且为SYN数据包，所以这一次的seq为1（0增加1）。 ack：上次接收到时为【no.2】，【no.2】中seq为0，且为SYN数据包（虽然在flag上同时设定为SYN/ACK，但只要flag是SYN，就会驱使seq加一），所以可预计，server端下一次seq为1（0增加1）。 4、 seq：上一次发送时为【no.1】，【no.1】中seq为0且为SYN数据包，所以这一次的seq为1（0增加1）。 ack：上次接收到时为【no.2】，【no.2】中seq为0，且为SYN数据包，所以可预计，server端下一次seq为1（0增加1）。 5、 seq：上一次发送时为【no.2】，【no.2】中seq为0，且为SYN数据包，所以这一次的seq为1（0增加1）。 ack：上一次接收时为【no.4】，【no.4】中的seq为1，数据包的长度为725，所以可以预计，下一次client端的seq为726（1+725）。 6、 seq：上一次发送时为【no.5】，【no.5】中seq为1，但【no.5】为ACK数据包，所以数据长度为0且不会驱使seq加1，所以这一次的seq为1（1+0）。 ack：上一次接收时为【no.4】，【no.4】中的seq为1，数据包的长度为725，所以可以预计，下一次client端的seq为726（1+725）。 7、 seq：上一次发送时为【no.4】，【no.4】中seq为1，数据包长度为725，所以这一次的seq为726（1+725）。 ack：上一次接收时为【no.6】，【no.6】中seq为1，且数据长度为1448，所以可以预计，下一次server端的seq为1449（1+1448）。 8、 seq：上一次发送时为【no.6】，【no.6】中seq为1，数据包长度为1448，所以这一次的seq为1449（1+1448）。 ack：上一次接收时为【no.7】，【no.7】中seq为726，数据包为ACK、即数据为0，所以可以预计，下一次client端的seq为726（726+0）。 9、 seq：上一次发送时为【no.7】，【no.7】中seq为726，数据包为ACK、即长度为0， 所以这一次seq为726（726+0）。 ack：上一次接收时为【no.8】，【no.8】中seq为1449，数据包长度为1448，所以可以预计，下一次server端的seq为2897（1449+1448）。 10、 seq：上一次发送时为【no.8】，【no.8】中seq为1449，且数据包长度为1448，所以这一次seq为2897（1449+1448）。 ack：上一次接收时为【no.9】，【no.9】中seq为726，数据包为ACK、即数据为0，所以可以预计，下一次client端的seq为726（726+0）。 剩下的7个packet可以留作练习题自己分析。可以看到的是，从【no.7】开始，client端这边就只负责做响应，发送ACK数据包，而并没有实际的数据发送到server端。所以，从【no.7】开始，所有的ACK数据包的seq都是相同的726，因为ACK不像SYN/FIN可以让seq增加，所以发送再多的ACK包都只能让seq原地踏步。 丢包验证由此可以看到，无论对于client端还是server端，这一次刚收到的对方的packet的seq，一定要和最后一次发送时的packet的ack相等。 因为最后一次发送时的packet的ack，是对下一次接收的packet的seq做的预测。如果两者不等，则表明中途有数据包丢失了！","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"https://blog.fenxiangz.com/categories/TCP-IP/"}],"tags":[{"name":"概念","slug":"概念","permalink":"https://blog.fenxiangz.com/tags/%E6%A6%82%E5%BF%B5/"},{"name":"网络术语","slug":"网络术语","permalink":"https://blog.fenxiangz.com/tags/%E7%BD%91%E7%BB%9C%E6%9C%AF%E8%AF%AD/"}]},{"title":"Ubuntu 20.04 LTS 安装搜狗输入法","slug":"linux/2017-12-12_Ubuntu20.04LTS安装搜狗输入法","date":"2017-12-12T00:00:00.000Z","updated":"2020-12-20T16:47:02.979Z","comments":true,"path":"post/linux/2017-12-12_Ubuntu20.04LTS安装搜狗输入法.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-12-12_Ubuntu20.04LTS%E5%AE%89%E8%A3%85%E6%90%9C%E7%8B%97%E8%BE%93%E5%85%A5%E6%B3%95.html","excerpt":"","text":"Ubuntu20.04LTS安装搜狗输入法 舟公 Ubuntu 20.04安装后，一直想安装搜狗输入法，尝试过官网下载安装搜狗输入法Linux版，但由于依赖（qt4相关，在20.04版本中被去掉了）的问题无法修复，导致一直无法成功安装。用了一段时间统自带的输入法，使用起来相对还可以，不过相比搜狗输入法（拼音纠错，模糊音，候选词等），还是远远比不上的。最近搜索了一下，发现已经有方法安装，并可以正常使用，而且方法并不复杂，帮助大家整理一下，并附上原文链接！ 1.添加ubuntukylin源根据官方教程添加ukui的官方源，为了安装及后续更新。此源中包含了麒麟版搜狗输入法。 $ curl -sL &#39;https://keyserver.ubuntu.com/pks/lookup?&amp;op=get&amp;search=0x73BC8FBCF5DE40C6ADFCFFFA9C949F2093F565FF&#39; | sudo apt-key add $ sudo apt-add-repository &#39;deb http://archive.ubuntukylin.com/ukui focal main&#39; $ sudo apt upgrade 2.安装搜狗输入法直接安装软件包，注意包名是sogouimebs而不是原来的sogoupinyin。这个版本在其他地方暂时是找不到的，因为是麒麟社区定制的版本。由于ubuntu和优麒麟一脉相承的关系，我们可以直接拿来用。 $ sudo apt install sogouimebs 3.输入法设置根据自身需求针对输入法属性进行设置 $ sogouIme-configtool 4.设置默认输入法进入设置，选择“区域与语言”，点击管理已安装的语言，把默认输入法设置为fcitx，重启电脑就可以使用搜狗输入法了。切换输入法按键Ctrl+Space。 禁用IBus(可选)经过使用，我发现偶尔会遇到输入卡住的情况，ibus和fcitx发生冲突不是新鲜问题了，如果有相同的问题不妨卸载ibus试一试。 如果无法卸载ibus可以采用禁用ibus的方法。 $ sudo dpkg-divert --package im-config --rename /usr/bin/ibus-daemon Adding &#39;diversion of /usr/bin/ibus-daemon to /usr/bin/ibus-daemon.distrib by im-config&#39; 附上启用ibus方法。 $ sudo dpkg-divert --package im-config --rename --remove /usr/bin/ibus-daemon Removing &#39;diversion of /usr/bin/ibus-daemon to /usr/bin/ibus-daemon.distrib by im-config&#39; 原文链接： Ubuntu 20.04 LTS安装搜狗输入法，只需三条命令，还能自动更新​www.cnblogs.com![图标](https://pic1.zhimg.com/v2-b4a895150168f683f1e800179334bcfd_180x120.jpg) 麒麟社区软件： 应用下载-Ubuntu Kylin,优麒麟,ubuntu,麒麟操作系统,优客-优麒麟操作系统​www.ubuntukylin.com Ubuntu kylin官方源： UKUI Desktop Environment​www.ukui.org禁用ibus方法：InputMethodBuster - Debian Wiki​wiki.debian.org","categories":[{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"输入法","slug":"输入法","permalink":"https://blog.fenxiangz.com/tags/%E8%BE%93%E5%85%A5%E6%B3%95/"}]},{"title":"未来已来：云原生 Cloud Native","slug":"linux/2017-12-11_未来已来：云原生Cloud_Native","date":"2017-12-11T00:00:00.000Z","updated":"2020-12-20T16:47:02.979Z","comments":true,"path":"post/linux/2017-12-11_未来已来：云原生Cloud_Native.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-12-11_%E6%9C%AA%E6%9D%A5%E5%B7%B2%E6%9D%A5%EF%BC%9A%E4%BA%91%E5%8E%9F%E7%94%9FCloud_Native.html","excerpt":"","text":"前言自 2013 年容器（虚拟）技术（Docker）成熟后，后端的架构方式进入快速迭代的阶段，出现了很多新兴概念： 微服务 k8s Serverless IaaS：基础设施服务，Infrastructure-as-a-service PaaS：平台服务，Platform-as-a-service SaaS：软件服务，Software-as-a-service Cloud Native： 云原生 Service Mesh 后端架构的变迁和云计算的发展密切相关，架构其实在不断地适应云计算，特别是云原生，被誉为未来架构，在 2019 年，云原生落地方案 Service Mesh 在国内外全面开花，我认为，未来已来。 接下来，我们将： 梳理后端架构演化史，回顾后端架构发展历程； 回顾云服务发展历程，探讨云原生概念； 梳理云原生实现方案 Service Mesh 的发展历程； 介绍 Service Mesh 的代表 Istio 的亮眼功能； 后端架构演化史集中式架构集中式架构又叫单体式架构，在Web2.0模式并未大规模兴起时十分流行。后来，基于Web应用的B/S（Browser/Server）架构逐渐取代了基于桌面应用的C/S（Client/Server）架构。B/S架构的后端系统大都采用集中式架构，它当时以优雅的分层设计，统一了服务器后端的开发领域。 集中式应用分为标准的3层架构模型：数据访问层M、服务层V和逻辑控制层C。每个层之间既可以共享领域模型对象，也可以进行更加细致的拆分。 其缺点是 编译时间过长; 回归测试周期过长; 开发效率降低等； 不利于更新技术框架 分布式系统架构对于互联网应用规模的迅速增长，集中式架构并无法做到无限制的提升系统的吞吐量，而分布式系统架构在理论上为吞吐量的上升提供了无限扩展的可能。因此，用于搭建互联网应用的服务器也渐渐地放弃了昂贵的小型机，转而采用大量的廉价PC服务器。 容器技术新纪元 Docker分布式架构的概念很早就出现，阻碍其落地的最大问题是容器技术不成熟，应用程序在云平台运行，仍然需要为不同的开发语言安装相应的运行时环境。虽然自动化运维工具可以降低环境搭建的复杂度，但仍然不能从根本上解决环境的问题。 Docker的出现成为了软件开发行业新的分水岭；容器技术的成熟，也标志技术新纪元的开启。Docker让开发工程师可以将他们的应用和依赖封装到一个可移植的容器中。就像当年智能手机的出现改变了整个手机行业的游戏规则一样，Docker也大有席卷整个软件行业，并且进而改变行业游戏规则的趋势。通过集装箱式的封装，开发和运维都以标准化的方式发布的应用，异构语言不再是桎梏团队的枷锁。 在 Docker 之后，微服务得以流行开来 微服务架构微服务架构风格是一种将一个单一应用程序开发为一组小型服务的方法，每个服务运行在自己的进程中，服务间通信采用轻量级通信机制(通常用HTTP资源API)。这些服务围绕业务能力构建并且可通过全自动部署机制独立部署。这些服务共用一个最小型的集中式的管理，服务可用不同的语言开发，使用不同的数据存储技术。 微服务优势 可扩展 可升级 易维护 故障和资源的隔离 微服务的问题 但是，世界上没有完美无缺的事物，微服务也是一样。著名软件大师，被认为是十大软件架构师之一的 Chris Richardson 曾一针见血地指出：“微服务应用是分布式系统，由此会带来固有的复杂性。开发者需要在 RPC 或者消息传递之间选择并完成进程间通讯机制。此外，他们必须写代码来处理消息传递中速度过慢或者不可用等局部失效问题。” 在微服务架构中，一般要处理以下几类问题： 服务治理：弹性伸缩，故障隔离 流量控制：路由，熔断，限速 应用观测：指标度量、链式追踪 解决方案 Spring Cloud（Netflix OSS） 这是一个典型的微服务架构图 Spring Cloud 体系提供了服务发现、负载均衡、失效转移、动态扩容、数据分片、调用链路监控等分布式系统的核心功能，一度成为微服务的最佳实践。 Spring Cloud 的问题 如果开始构建微服务的方法，肯定容易被 Netflix OSS/Java/Spring/SpringCloud 所吸引。但是要知道你不是Netflix，也不需要直接使用 AWS EC2，使得应用程序变得很复杂。如今使用 docker 和采用 memos/kubernetes 是明智之举，它们已经具备大量的分布式系统特性。在应用层进行分层，是因为 netflix 5年前面临的问题，而不得不这样做（可以说如果那时有了kubernetes，netflix OSS栈会大不相同）。 因此，建议谨慎选择，按需选择，避免给应用程序带来不必要的复杂度。 的确 SpringCloud 方案看起来很美好，但是它具有非常强的侵入性，应用代码中会包含大量的 SpringCloud 模块，而且对其他编程语言也不友好。 KubernetesKubernetes 出现就是为了解决 SpringCloud 的问题，不侵入应用层，在容器层解决问题。 Kubernetes 起源 Kubernetes最初源于谷歌内部的Borg，提供了面向应用的容器集群部署和管理系统。 Kubernetes的目标旨在消除编排物理/虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。 Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的 workflows 和更高级的自动化任务。 Kubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。 Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。 Service MeshService Mesh 是对 Kubernetes 的增强，提供了更多的能力。 2018年9月1日，Bilgin Ibryam 在 InfoQ 发表了一篇文章 Microservices in a Post-Kubernetes Era，中文版见后 Kubernetes 时代的微服务（译文有些错误，仅供参考）。 文中作者的观点是：在后 Kubernetes 时代，服务网格（Service Mesh）技术已完全取代了使用软件库实现网络运维（例如 Hystrix 断路器）的方式。 如果说 Kubernetes 对 Spring Cloud 开了第一枪，那么 Service Mesh 就是 Spring Cloud 的终结者。 总结最后我们用一个流程图来描述后端架构的发展历程 每个关键节点的大概时间表 架构/技术 时间/年份 集中式架构 ~ 分布式架构 ~ Docker 2013 微服务 2014 Spring Cloud 2014 Kubernetes 成熟 2017 Service Mesh 2017 可以看出，微服务生态这里，Spring Cloud 为代表的这条路已经后继无人了，未来属于 Service Mesh 。Service Mesh 经过2年的发展，目前 Service Mesh 已经足够成熟，已经有生产落地的案例，我们接下来就看看 Service Mesh，在此之前，我们要先理解一个概念，云原生。 云原生 Cloud Native如何理解“云原生”？之所以将这个话题放在前面，是因为，这是对云原生概念的最基本的理解，而这会直接影响到后续的所有认知。 注意：以下云原生的内容将全部引用敖小剑的 畅谈云原生（上）：云原生应用应该是什么样子？ 这篇文章，图画得太好了。 云原生的定义一直在发展，每个人对云原生的理解都可能不同，就如莎士比亚所说：一千个人眼中有一千个哈姆雷特。 2018 年 CNCF (Cloud Native Computing Foundation)更新了云原生的定义。 这是新定义中描述的代表技术，其中容器和微服务两项在不同时期的不同定义中都有出现，而服务网格这个在 2017 年才开始被社区接纳的新热点技术被非常醒目的列出来，和微服务并列，而不是我们通常认为的服务网格只是微服务在实施时的一种新的方式。 那我们该如何理解云原生呢？我们尝试一下，将 Cloud Native 这个词汇拆开来理解，先看看什么是 Cloud。 什么是云 Cloud快速回顾一下云计算的历史，来帮助我们对云有个更感性的认识。 云计算的出现和虚拟化技术的发展和成熟密切相关，2000 年前后 x86 的虚拟机技术成熟后，云计算逐渐发展起来。 基于虚拟机技术，陆续出现了 IaaS/PaaS/FaaS 等形态，以及他们的开源版本。 2013 年 docker 出现，容器技术成熟，然后围绕容器编排一场大战，最后在 2017 年底，kubernetes 胜出。2015 年 CNCF 成立，并在近年形成了 cloud native 生态。 在这个过程中，云的形态一直变化，可以看到：供应商提供的功能越来越多，而客户或者说应用需要自己管理的功能越来越少。 架构也在一直适应云计算的变化 什么是原生 Native在回顾完云计算的历史之后，我们对 Cloud 有更深的认识，接着继续看一下：什么是 Native？字典的解释是：与生俱来的。那 Cloud 和 native 和在一起，又该如何理解？ 这里我们抛出一个我们自己的理解：云原生代表着原生为云设计。详细的解释是：应用原生被设计为在云上以最佳方式运行，充分发挥云的优势。 这个理解有点空泛，但是考虑到云原生的定义和特征在这些年间不停的变化，以及完全可以预料到的在未来的必然变化，我觉得，对云原生的理解似乎也只能回到云原生的出发点，而不是如何具体实现。 Cloud Native 是道，Service Mesh 是术那在这么一个云原生理解的背景下，我再来介绍一下我对云原生应用的设想，也就是我觉得云原生应用应该是什么样子。 在云原生之前，底层平台负责向上提供基本运行资源。而应用需要满足业务需求和非业务需求，为了更好的代码复用，通用型好的非业务需求的实现往往会以类库和开发框架的方式提供，另外在 SOA/ 微服务时代部分功能会以后端服务的方式存在，这样在应用中就被简化为对其客户端的调用代码。 然后应用将这些功能，连同自身的业务实现代码，一起打包。 而云的出现，可以在提供各种资源之外，还提供各种能力，从而帮助应用，使得应用可以专注于业务需求的实现。非业务需求相关的功能都被移到云，或者说基础设施中去了，以及下沉到基础设施的中间件。 以服务间通讯为例：需要实现上面列举的各种功能。 SDK 的思路：在应用层添加一个胖客户端，在这个客户端中实现各种功能。 Service Mesh 的思路，体现在将 SDK 客户端的功能剥离出来，放到 Sidecar 中。就是把更多的事情下沉，下沉到基础设施中。 在用户看来，应用长这样： 云原生是我们的目标，Service Mesh 交出了自己的答卷，接下来我们可以回到 Service Mesh 这里了。 Service Mesh其中文译名是服务网格，这个词最早使用由开发Linkerd的Buoyant公司提出，并在内部使用。 定义 服务网格的基本构成 纷争 20172017 年年底，当非侵入式的 Service Mesh 技术终于从萌芽到走向了成熟，当 Istio/Conduit 横空出世，人们才惊觉：微服务并非只有侵入式一种玩法，更不是 Spring Cloud 的独角戏！ 解读 2017 之 Service Mesh：群雄逐鹿烽烟起 文章总结一下：创业公司 Buoyant 的产品 Linkerd 开局拿下一血；Envoy 默默耕耘；从 Google 和 IBM 联手推出 Istio，Linkerd 急转直下；2017 年底 Buoyant 推出 Conduit 背水一战；Nginmesh 与 Kong 低调参与； 百家争鸣 20182018 年，Service Mesh 又多了哪些内容呢？在 2018 年，Service Mesh 在国内大热，有多家公司推出自己的 Service Mesh 产品和方案，Service Mesh 更加热闹了。详细见这篇文章 下一代微服务！Service Mesh 2018 年度总结 文章总结一下：Service Mesh 在国内大热，有多家公司加入战场；Istio 发布1.0，成为最受欢迎的 Service Mesh 项目，获得多方支持；Envoy 继续稳扎稳打，Envoy 被 Istio 直接采用为数据平面，有望成为数据平面标准；Linkerd1.x 陷入困境，Conduit 小步快跑，但响应平平，Buoyant 公司决定合并产品线，Linkerd1.x + Conduit = Linkerd2.0；更多的公司参与 Service Mesh，国外有 Nginx、Consul、Kong、AWS等，国内有蚂蚁金服、新浪微博、华为，阿里 Dubbo，腾讯等； 持续发展 20192019 将会听到更多 Service Mesh 的声音，请关注Service Mesh 中文社区 Istio前文讲到 Istio 是当前最受欢迎的 Service Mesh 框架，一句话定义 Istio：一个用来连接、管理和保护微服务的开放平台。它能给我们的微服务提供哪些功能呢？ 连接 动态路由 超时重试 熔断 故障注入 详细见官网介绍 保护安全问题一开始就要做好，在 Istio 实现安全通讯是非常方便的。 Istio 支持双向 TLS 加密 见官方文档 控制速率限制黑白名单 见官方文档 观测 指标度量：每秒请求数，Prometheus 与 Grafana使用 Grafana 观测流量情况 分布式追踪：Jaeger 或 Zipkin快速观测调用链路 日志：非应用日志 网格可视化快速理清服务的关系 总结虚拟化技术推动这云计算技术的变革，顺带也影响了后端架构的演进，目前我们身处云时代，将会有更多的元原生应用出现，Istio 作为其中的佼佼者，值得你投入一份精力了解一下。 学习资料/指引Service Mesh 中文社区 上面提供了丰富的学习资料。 搭建 Kubernetes 集群会比较麻烦，推荐几种方式。主要原因是很多镜像需要翻墙才能下载。 Docker Desktop 自带的 Kubernetes 集群 使用 Rancher2.0 搭建 Kubernetes 集群 在 Google Cloud 上直接开集群，可以领 300 美金的体验金，需要翻墙 不推荐 MiniKube,翻墙和代理问题非常难搞。再附上 Docker 设置代理的方式 在线体验 Istio 参考资料kubernetes-handbook istio-handbook 微服务学习笔记 畅谈云原生（上）：云原生应用应该是什么样子？ Service Mesh：下一代微服务？ 从架构到组件，深挖istio如何连接、管理和保护微服务2.0？ 原文链接作者: 天如","categories":[{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"云原生","slug":"云原生","permalink":"https://blog.fenxiangz.com/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"Cloud","slug":"Cloud","permalink":"https://blog.fenxiangz.com/tags/Cloud/"}]},{"title":"浅析Linux中的零拷贝技术","slug":"linux/2017-12-10_浅析Linux中的零拷贝技术","date":"2017-12-10T00:00:00.000Z","updated":"2020-12-20T16:47:02.978Z","comments":true,"path":"post/linux/2017-12-10_浅析Linux中的零拷贝技术.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-12-10_%E6%B5%85%E6%9E%90Linux%E4%B8%AD%E7%9A%84%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AF.html","excerpt":"","text":"本文探讨Linux中主要的几种零拷贝技术以及零拷贝技术适用的场景。为了迅速建立起零拷贝的概念，我们拿一个常用的场景进行引入： 引文在写一个服务端程序时（Web Server或者文件服务器），文件下载是一个基本功能。这时候服务端的任务是：将服务端主机磁盘中的文件不做修改地从已连接的socket发出去，我们通常用下面的代码完成： 123while((n &#x3D; read(diskfd, buf, BUF_SIZE)) &gt; 0) write(sockfd, buf , n); 基本操作就是循环的从磁盘读入文件内容到缓冲区，再将缓冲区的内容发送到socket。但是由于Linux的I/O操作默认是缓冲I/O。这里面主要使用的也就是read和write两个系统调用，我们并不知道操作系统在其中做了什么。实际上在以上I/O操作中，发生了多次的数据拷贝。 当应用程序访问某块数据时，操作系统首先会检查，是不是最近访问过此文件，文件内容是否缓存在内核缓冲区，如果是，操作系统则直接根据read系统调用提供的buf地址，将内核缓冲区的内容拷贝到buf所指定的用户空间缓冲区中去。如果不是，操作系统则首先将磁盘上的数据拷贝的内核缓冲区，这一步目前主要依靠DMA来传输，然后再把内核缓冲区上的内容拷贝到用户缓冲区中。 接下来，write系统调用再把用户缓冲区的内容拷贝到网络堆栈相关的内核缓冲区中，最后socket再把内核缓冲区的内容发送到网卡上。 说了这么多，不如看图清楚： 从上图中可以看出，共产生了四次数据拷贝，即使使用了DMA来处理了与硬件的通讯，CPU仍然需要处理两次数据拷贝，与此同时，在用户态与内核态也发生了多次上下文切换，无疑也加重了CPU负担。 在此过程中，我们没有对文件内容做任何修改，那么在内核空间和用户空间来回拷贝数据无疑就是一种浪费，而零拷贝主要就是为了解决这种低效性。 什么是零拷贝技术（zero-copy）？零拷贝主要的任务就是避免CPU将数据从一块存储拷贝到另外一块存储，主要就是利用各种零拷贝技术，避免让CPU做大量的数据拷贝任务，减少不必要的拷贝，或者让别的组件来做这一类简单的数据传输任务，让CPU解脱出来专注于别的任务。这样就可以让系统资源的利用更加有效。 我们继续回到引文中的例子，我们如何减少数据拷贝的次数呢？一个很明显的着力点就是减少数据在内核空间和用户空间来回拷贝，这也引入了零拷贝的一个类型： 让数据传输不需要经过user space使用mmap我们减少拷贝次数的一种方法是调用mmap()来代替read调用： 123buf &#x3D; mmap(diskfd, len);write(sockfd, buf, len); 应用程序调用mmap()，磁盘上的数据会通过DMA被拷贝的内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用write(),操作系统直接将内核缓冲区的内容拷贝到socket缓冲区中，这一切都发生在内核态，最后，socket缓冲区再把数据发到网卡去。 同样的，看图很简单： 使用mmap替代read很明显减少了一次拷贝，当拷贝数据量很大时，无疑提升了效率。但是使用mmap是有代价的。当你使用mmap时，你可能会遇到一些隐藏的陷阱。例如，当你的程序map了一个文件，但是当这个文件被另一个进程截断(truncate)时, write系统调用会因为访问非法地址而被SIGBUS信号终止。SIGBUS信号默认会杀死你的进程并产生一个coredump,如果你的服务器这样被中止了，那会产生一笔损失。 通常我们使用以下解决方案避免这种问题： 1. 为SIGBUS信号建立信号处理程序 当遇到SIGBUS信号时，信号处理程序简单地返回，write系统调用在被中断之前会返回已经写入的字节数，并且errno会被设置成success,但是这是一种糟糕的处理办法，因为你并没有解决问题的实质核心。 2. 使用文件租借锁 通常我们使用这种方法，在文件描述符上使用租借锁，我们为文件向内核申请一个租借锁，当其它进程想要截断这个文件时，内核会向我们发送一个实时的RT_SIGNAL_LEASE信号，告诉我们内核正在破坏你加持在文件上的读写锁。这样在程序访问非法内存并且被SIGBUS杀死之前，你的write系统调用会被中断。write会返回已经写入的字节数，并且置errno为success。 我们应该在mmap文件之前加锁，并且在操作完文件后解锁： 1234567891011if(fcntl(diskfd, F_SETSIG, RT_SIGNAL_LEASE) &#x3D;&#x3D; -1) &#123; perror(&quot;kernel lease set signal&quot;); return -1;&#125;&#x2F;* l_type can be F_RDLCK F_WRLCK 加锁*&#x2F;&#x2F;* l_type can be F_UNLCK 解锁*&#x2F;if(fcntl(diskfd, F_SETLEASE, l_type))&#123; perror(&quot;kernel lease set type&quot;); return -1;&#125; 使用sendfile从2.1版内核开始，Linux引入了sendfile来简化操作: 123#include&lt;sys&#x2F;sendfile.h&gt;ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 系统调用sendfile()在代表输入文件的描述符in_fd和代表输出文件的描述符out_fd之间传送文件内容（字节）。描述符out_fd必须指向一个套接字，而in_fd指向的文件必须是可以mmap的。这些局限限制了sendfile的使用，使sendfile只能将数据从文件传递到套接字上，反之则不行。 使用sendfile不仅减少了数据拷贝的次数，还减少了上下文切换，数据传送始终只发生在kernel space。 在我们调用sendfile时，如果有其它进程截断了文件会发生什么呢？假设我们没有设置任何信号处理程序，sendfile调用仅仅返回它在被中断之前已经传输的字节数，errno会被置为success。如果我们在调用sendfile之前给文件加了锁，sendfile的行为仍然和之前相同，我们还会收到RT_SIGNAL_LEASE的信号。 目前为止，我们已经减少了数据拷贝的次数了，但是仍然存在一次拷贝，就是页缓存到socket缓存的拷贝。那么能不能把这个拷贝也省略呢？ 借助于硬件上的帮助，我们是可以办到的。之前我们是把页缓存的数据拷贝到socket缓存中，实际上，我们仅仅需要把缓冲区描述符传到socket缓冲区，再把数据长度传过去，这样DMA控制器直接将页缓存中的数据打包发送到网络中就可以了。 总结一下，sendfile系统调用利用DMA引擎将文件内容拷贝到内核缓冲区去，然后将带有文件位置和长度信息的缓冲区描述符添加socket缓冲区去，这一步不会将内核中的数据拷贝到socket缓冲区中，DMA引擎会将内核缓冲区的数据拷贝到协议引擎中去，避免了最后一次拷贝。 不过这一种收集拷贝功能是需要硬件以及驱动程序支持的。 使用splicesendfile只适用于将数据从文件拷贝到套接字上，限定了它的使用范围。Linux在2.6.17版本引入splice系统调用，用于在两个文件描述符中移动数据： 1234#define _GNU_SOURCE &#x2F;* See feature_test_macros(7) *&#x2F;#include &lt;fcntl.h&gt;ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags); splice调用在两个文件描述符之间移动数据，而不需要数据在内核空间和用户空间来回拷贝。他从fd_in拷贝len长度的数据到fd_out，但是有一方必须是管道设备，这也是目前splice的一些局限性。flags参数有以下几种取值： SPLICE_F_MOVE ：尝试去移动数据而不是拷贝数据。这仅仅是对内核的一个小提示：如果内核不能从pipe移动数据或者pipe的缓存不是一个整页面，仍然需要拷贝数据。Linux最初的实现有些问题，所以从2.6.21开始这个选项不起作用，后面的Linux版本应该会实现。 ** SPLICE_F_NONBLOCK** ：splice 操作不会被阻塞。然而，如果文件描述符没有被设置为不可被阻塞方式的 I/O ，那么调用 splice 有可能仍然被阻塞。 ** SPLICE_F_MORE**： 后面的splice调用会有更多的数据。 splice调用利用了Linux提出的管道缓冲区机制， 所以至少一个描述符要为管道。 以上几种零拷贝技术都是减少数据在用户空间和内核空间拷贝技术实现的，但是有些时候，数据必须在用户空间和内核空间之间拷贝。这时候，我们只能针对数据在用户空间和内核空间拷贝的时机上下功夫了。Linux通常利用**写时复制(copy on write)**来减少系统开销，这个技术又时常称作COW。 由于篇幅原因，本文不详细介绍写时复制。大概描述下就是：如果多个程序同时访问同一块数据，那么每个程序都拥有指向这块数据的指针，在每个程序看来，自己都是独立拥有这块数据的，只有当程序需要对数据内容进行修改时，才会把数据内容拷贝到程序自己的应用空间里去，这时候，数据才成为该程序的私有数据。如果程序不需要对数据进行修改，那么永远都不需要拷贝数据到自己的应用空间里。这样就减少了数据的拷贝。写时复制的内容可以再写一篇文章了。。。 除此之外，还有一些零拷贝技术，比如传统的Linux I/O中加上O_DIRECT标记可以直接I/O，避免了自动缓存，还有尚未成熟的fbufs技术，本文尚未覆盖所有零拷贝技术，只是介绍常见的一些，如有兴趣，可以自行研究，一般成熟的服务端项目也会自己改造内核中有关I/O的部分，提高自己的数据传输速率。 原文：https://www.jianshu.com/p/fad3339e3448","categories":[{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/categories/IO/"}],"tags":[{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"},{"name":"内存","slug":"内存","permalink":"https://blog.fenxiangz.com/tags/%E5%86%85%E5%AD%98/"},{"name":"零拷贝","slug":"零拷贝","permalink":"https://blog.fenxiangz.com/tags/%E9%9B%B6%E6%8B%B7%E8%B4%9D/"}]},{"title":"SVN 自动删除 “被手动删除” 的文件","slug":"util/svn/2017-12-10_SVN自动删除“被手动删除”的文件","date":"2017-12-10T00:00:00.000Z","updated":"2020-12-20T16:47:02.988Z","comments":true,"path":"post/util/svn/2017-12-10_SVN自动删除“被手动删除”的文件.html","link":"","permalink":"https://blog.fenxiangz.com/post/util/svn/2017-12-10_SVN%E8%87%AA%E5%8A%A8%E5%88%A0%E9%99%A4%E2%80%9C%E8%A2%AB%E6%89%8B%E5%8A%A8%E5%88%A0%E9%99%A4%E2%80%9D%E7%9A%84%E6%96%87%E4%BB%B6.html","excerpt":"","text":"删除SVN存储库工作副本中的文件时，应该在命令行上执行： svn rm [filename] 但是，如果您不这样做(例如通过gui删除，或者只执行“rm”而不执行“svn”)，那么svn就会感到困惑，并将一个“!”在所有被删除的文件之前的状态。 如果您进行svn更新，所有的文件都将被恢复，显然您花费在删除它们上的所有时间都将被浪费了。 你真的应该使用svn rm，但如果已经太晚了，你可以使用这个bash脚本来删除svn中的文件： #从svn中删除你已经删除的文件 svn status | grep &quot;^\\!&quot; | sed &#39;s/^\\! *//g&#39; | xargs svn rm 这个命令执行一个status命令，查找所有以“!”开头的行。然后提取文件名，并运行它通过“ svn rm ” 真正删除文件。 警告：确保您确实想删除所有这些文件！ 使用方法自负风险。代码足够简单，所以您应该能够了解它的功能。","categories":[{"name":"SVN","slug":"SVN","permalink":"https://blog.fenxiangz.com/categories/SVN/"}],"tags":[{"name":"SVN","slug":"SVN","permalink":"https://blog.fenxiangz.com/tags/SVN/"}]},{"title":"Linux内存映射mmap原理分析","slug":"linux/2017-12-09_Linux内存映射mmap原理分析","date":"2017-12-09T00:00:00.000Z","updated":"2020-12-20T16:47:02.978Z","comments":true,"path":"post/linux/2017-12-09_Linux内存映射mmap原理分析.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-12-09_Linux%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%84mmap%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.html","excerpt":"","text":"内存映射，简而言之就是将用户空间的一段内存区域映射到内核空间，映射成功后，用户对这段内存区域的修改可以直接反映到内核空间，同样，内核空间对这段区域的修改也直接反映用户空间。那么对于内核空间&lt;—-&gt;用户空间两者之间需要大量数据传输等操作的话效率是非常高的。 二、基本函数mmap函数是unix/linux下的系统调用，详细内容可参考《Unix Netword programming》卷二12.2节。 mmap系统调用并不是完全为了用于共享内存而设计的。它本身提供了不同于一般对普通文件的访问方式，进程可以像读写内存一样对普通文件的操作。而Posix或系统V的共享内存IPC则纯粹用于共享目的，当然mmap()实现共享内存也是其主要应用之一。 mmap系统调用使得进程之间通过映射同一个普通文件实现共享内存。普通文件被映射到进程地址空间后，进程可以像访问普通内存一样对文件进行访问，不必再调用read()，write（）等操作。mmap并不分配空间, 只是将文件映射到调用进程的地址空间里（但是会占掉你的 virutal memory）, 然后你就可以用memcpy等操作写文件, 而不用write()了.写完后，内存中的内容并不会立即更新到文件中，而是有一段时间的延迟，你可以调用msync()来显式同步一下, 这样你所写的内容就能立即保存到文件里了.这点应该和驱动相关。 不过通过mmap来写文件这种方式没办法增加文件的长度, 因为要映射的长度在调用mmap()的时候就决定了.如果想取消内存映射，可以调用munmap()来取消内存映射。 void * mmap(void *start, size_t length, int prot , int flags, int fd, off_t offset) mmap用于把文件映射到内存空间中，简单说mmap就是把一个文件的内容在内存里面做一个映像。映射成功后，用户对这段内存区域的修改可以直接反映到内核空间，同样，内核空间对这段区域的修改也直接反映用户空间。那么对于内核空间&lt;—-&gt;用户空间两者之间需要大量数据传输等操作的话效率是非常高的。 原理首先，“映射”这个词，就和数学课上说的“一一映射”是一个意思，就是建立一种一一对应关系，在这里主要是只 硬盘上文件 的位置与进程 逻辑地址空间 中一块大小相同的区域之间的一一对应，如图1中过程1所示。这种对应关系纯属是逻辑上的概念，物理上是不存在的，原因是进程的逻辑地址空间本身就是不存在的。在内存映射的过程中，并没有实际的数据拷贝，文件没有被载入内存，只是逻辑上被放入了内存，具体到代码，就是建立并初始化了相关的数据结构（struct address_space），这个过程有系统调用mmap()实现，所以建立内存映射的效率很高。 |-图1.内存映射原理-| 既然建立内存映射没有进行实际的数据拷贝，那么进程又怎么能最终直接通过内存操作访问到硬盘上的文件呢？那就要看内存映射之后的几个相关的过程了。 mmap()会返回一个指针ptr，它指向进程逻辑地址空间中的一个地址，这样以后，进程无需再调用read或write对文件进行读写，而只需要通过ptr就能够操作文件。但是ptr所指向的是一个逻辑地址，要操作其中的数据，必须通过MMU将逻辑地址转换成物理地址，如图1中过程2所示。这个过程与内存映射无关。 前面讲过，建立内存映射并没有实际拷贝数据，这时，MMU在地址映射表中是无法找到与ptr相对应的物理地址的，也就是MMU失败，将产生一个缺页中断，缺页中断的中断响应函数会在swap中寻找相对应的页面，如果找不到（也就是该文件从来没有被读入内存的情况），则会通过mmap()建立的映射关系，从硬盘上将文件读取到物理内存中，如图1中过程3所示。这个过程与内存映射无关。 如果在拷贝数据时，发现物理内存不够用，则会通过虚拟内存机制（swap）将暂时不用的物理页面交换到硬盘上，如图1中过程4所示。这个过程也与内存映射无关。 效率从代码层面上看，从硬盘上将文件读入内存，都要经过文件系统进行数据拷贝，并且数据拷贝操作是由文件系统和硬件驱动实现的，理论上来说，拷贝数据的效率是一样的。但是通过内存映射的方法访问硬盘上的文件，效率要比read和write系统调用高，这是为什么呢？原因是read()是系统调用，其中进行了数据拷贝，它首先将文件内容从硬盘拷贝到内核空间的一个缓冲区，如图2中过程1，然后再将这些数据拷贝到用户空间，如图2中过程2，在这个过程中，实际上完成了 两次数据拷贝 ；而mmap()也是系统调用，如前所述，mmap()中没有进行数据拷贝，真正的数据拷贝是在缺页中断处理时进行的，由于mmap()将文件直接映射到用户空间，所以中断处理函数根据这个映射关系，直接将文件从硬盘拷贝到用户空间，只进行了 一次数据拷贝 。因此，内存映射的效率要比read/write效率高。 |-图2.read系统调用原理-| 下面这个程序，通过read和mmap两种方法分别对硬盘上一个名为“mmap_test”的文件进行操作，文件中存有10000个整数，程序两次使用不同的方法将它们读出，加1，再写回硬盘。通过对比可以看出，read消耗的时间将近是mmap的两到三倍。 #include&lt;unistd.h&gt; #include&lt;stdio.h&gt; #include&lt;stdlib.h&gt; #include&lt;string.h&gt; #include&lt;sys/types.h&gt; #include&lt;sys/stat.h&gt; #include&lt;sys/time.h&gt; #include&lt;fcntl.h&gt; #include&lt;sys/mman.h&gt; #define MAX 10000 int main() &#123; int i=0; int count=0, fd=0; struct timeval tv1, tv2; int *array = (int *)malloc( sizeof(int)*MAX ); /*read*/ gettimeofday( &amp;tv1, NULL ); fd = open( &quot;mmap_test&quot;, O_RDWR ); if( sizeof(int)*MAX != read( fd, (void *)array, sizeof(int)*MAX ) ) &#123; printf( &quot;Reading data failed.../n&quot; ); return -1; &#125; for( i=0; i&lt;MAX; ++i ) ++array[ i ]; if( sizeof(int)*MAX != write( fd, (void *)array, sizeof(int)*MAX ) ) &#123; printf( &quot;Writing data failed.../n&quot; ); return -1; &#125; free( array ); close( fd ); gettimeofday( &amp;tv2, NULL ); printf( &quot;Time of read/write: %dms/n&quot;, tv2.tv_usec-tv1.tv_usec ); /*mmap*/ gettimeofday( &amp;tv1, NULL ); fd = open( &quot;mmap_test&quot;, O_RDWR ); array = mmap( NULL, sizeof(int)*MAX, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0 ); for( i=0; i&lt;MAX; ++i ) ++array[ i ]; munmap( array, sizeof(int)*MAX ); msync( array, sizeof(int)*MAX, MS_SYNC ); free( array ); close( fd ); gettimeofday( &amp;tv2, NULL ); printf( &quot;Time of mmap: %dms/n&quot;, tv2.tv_usec-tv1.tv_usec ); return 0; &#125; 输出结果： Time of read/write: 154ms Time of mmap: 68ms 原文：https://blog.csdn.net/mg0832058/article/details/5890688","categories":[{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/categories/IO/"}],"tags":[{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"},{"name":"MMAP","slug":"MMAP","permalink":"https://blog.fenxiangz.com/tags/MMAP/"},{"name":"内存映射","slug":"内存映射","permalink":"https://blog.fenxiangz.com/tags/%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%84/"}]},{"title":"从零开始搭建Prometheus自动监控报警系统（企业级）","slug":"util/monitor/2017-12-10_从零开始搭建Prometheus自动监控报警系统","date":"2017-12-09T00:00:00.000Z","updated":"2020-12-20T16:47:02.987Z","comments":true,"path":"post/util/monitor/2017-12-10_从零开始搭建Prometheus自动监控报警系统.html","link":"","permalink":"https://blog.fenxiangz.com/post/util/monitor/2017-12-10_%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BAPrometheus%E8%87%AA%E5%8A%A8%E7%9B%91%E6%8E%A7%E6%8A%A5%E8%AD%A6%E7%B3%BB%E7%BB%9F.html","excerpt":"","text":"什么是Prometheus?Prometheus是由SoundCloud开发的开源监控报警系统和时序列数据库(TSDB)。Prometheus使用Go语言开发，是Google BorgMon监控系统的开源版本。 2016年由Google发起Linux基金会旗下的原生云基金会(Cloud Native Computing Foundation), 将Prometheus纳入其下第二大开源项目。 Prometheus目前在开源社区相当活跃。 Prometheus和Heapster(Heapster是K8S的一个子项目，用于获取集群的性能数据。)相比功能更完善、更全面。Prometheus性能也足够支撑上万台规模的集群。 Prometheus的特点 多维度数据模型。 灵活的查询语言。 不依赖分布式存储，单个服务器节点是自主的。 通过基于HTTP的pull方式采集时序数据。 可以通过中间网关进行时序列数据推送。 通过服务发现或者静态配置来发现目标服务对象。 支持多种多样的图表和界面展示，比如Grafana等。 官网地址：https://prometheus.io/ 架构图 基本原理Prometheus的基本原理是通过HTTP协议周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以接入监控。不需要任何SDK或者其他的集成过程。这样做非常适合做虚拟化环境监控系统，比如VM、Docker、Kubernetes等。输出被监控组件信息的HTTP接口被叫做exporter 。目前互联网公司常用的组件大部分都有exporter可以直接使用，比如Varnish、Haproxy、Nginx、MySQL、Linux系统信息(包括磁盘、内存、CPU、网络等等)。 服务过程 Prometheus Daemon负责定时去目标上抓取metrics(指标)数据，每个抓取目标需要暴露一个http服务的接口给它定时抓取。Prometheus支持通过配置文件、文本文件、Zookeeper、Consul、DNS SRV Lookup等方式指定抓取目标。Prometheus采用PULL的方式进行监控，即服务器可以直接通过目标PULL数据或者间接地通过中间网关来Push数据。 Prometheus在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中。 Prometheus通过PromQL和其他API可视化地展示收集的数据。Prometheus支持很多方式的图表可视化，例如Grafana、自带的Promdash以及自身提供的模版引擎等等。Prometheus还提供HTTP API的查询方式，自定义所需要的输出。 PushGateway支持Client主动推送metrics到PushGateway，而Prometheus只是定时去Gateway上抓取数据。 Alertmanager是独立于Prometheus的一个组件，可以支持Prometheus的查询语句，提供十分灵活的报警方式。 三大套件 Server 主要负责数据采集和存储，提供PromQL查询语言的支持。 Alertmanager 警告管理器，用来进行报警。 Push Gateway 支持临时性Job主动推送指标的中间网关。 本飞猪教程内容简介 1.演示安装Prometheus Server 2.演示通过golang和node-exporter提供metrics接口 3.演示pushgateway的使用 4.演示grafana的使用 5.演示alertmanager的使用 安装准备这里我的IP是10.211.55.25，登入，建立相应文件夹 12345mkdir -p &#x2F;home&#x2F;chenqionghe&#x2F;promethuesmkdir -p &#x2F;home&#x2F;chenqionghe&#x2F;promethues&#x2F;servermkdir -p &#x2F;home&#x2F;chenqionghe&#x2F;promethues&#x2F;clienttouch &#x2F;home&#x2F;chenqionghe&#x2F;promethues&#x2F;server&#x2F;rules.ymlchmod 777 &#x2F;home&#x2F;chenqionghe&#x2F;promethues&#x2F;server&#x2F;rules.yml 下面开始三大套件的学习 一.安装Prometheus Server通过docker方式 首先创建一个配置文件/home/chenqionghe/test/prometheus/prometheus.yml 挂载之前需要改变文件权限为777，要不会引起修改宿主机上的文件 会引起内容不同步的问题 12345678910global: scrape_interval: 15s # 默认抓取间隔, 15秒向目标抓取一次数据。 external_labels: monitor: &#39;codelab-monitor&#39;# 这里表示抓取对象的配置scrape_configs: #这个配置是表示在这个配置内的时间序例，每一条都会自动添加上这个&#123;job_name:&quot;prometheus&quot;&#125;的标签 - job_name: &#39;prometheus&#39; scrape_interval: 5s # 重写了全局抓取间隔时间，由15秒重写成5秒 static_configs: - targets: [&#39;localhost:9090&#39;] 12345678docker rm -f prometheusdocker run --name&#x3D;prometheus -d \\-p 9090:9090 \\-v &#x2F;home&#x2F;chenqionghe&#x2F;promethues&#x2F;server&#x2F;prometheus.yml:&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml \\-v &#x2F;home&#x2F;chenqionghe&#x2F;promethues&#x2F;server&#x2F;rules.yml:&#x2F;etc&#x2F;prometheus&#x2F;rules.yml \\prom&#x2F;prometheus:v2.7.2 \\--config.file&#x3D;&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml \\--web.enable-lifecycle 启动时加上–web.enable-lifecycle启用远程热加载配置文件 调用指令是curl -X POST http://localhost:9090/-/reload 访问http://10.211.55.25:9090 我们会看到如下l界面 访问http://10.211.55.25:9090/metrics 我们配置了9090端口，默认prometheus会抓取自己的/metrics接口 在Graph选项已经可以看到监控的数据 二.安装客户端提供metrics接口1.通过golang客户端提供metrics12345678910111213141516mkdir -p &#x2F;home&#x2F;chenqionghe&#x2F;promethues&#x2F;client&#x2F;golang&#x2F;srccd !$export GOPATH&#x3D;&#x2F;home&#x2F;chenqionghe&#x2F;promethues&#x2F;client&#x2F;golang&#x2F;#克隆项目git clone https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;client_golang.git#安装需要FQ的第三方包mkdir -p $GOPATH&#x2F;src&#x2F;golang.org&#x2F;x&#x2F;cd !$git clone https:&#x2F;&#x2F;github.com&#x2F;golang&#x2F;net.gitgit clone https:&#x2F;&#x2F;github.com&#x2F;golang&#x2F;sys.gitgit clone https:&#x2F;&#x2F;github.com&#x2F;golang&#x2F;tools.git#安装必要软件包go get -u -v github.com&#x2F;prometheus&#x2F;client_golang&#x2F;prometheus#编译cd $GOPATH&#x2F;src&#x2F;client_golang&#x2F;examples&#x2F;randomgo build -o random main.go 运行3个示例metrics接口 123.&#x2F;random -listen-address&#x3D;:8080 &amp;.&#x2F;random -listen-address&#x3D;:8081 &amp;.&#x2F;random -listen-address&#x3D;:8082 &amp; 2.通过node exporter提供metrics1234docker run -d \\--name&#x3D;node-exporter \\-p 9100:9100 \\prom&#x2F;node-exporter 然后把这两些接口再次配置到prometheus.yml, 重新载入配置curl -X POST http://localhost:9090/-/reload 1234567891011121314151617181920global: scrape_interval: 15s # 默认抓取间隔, 15秒向目标抓取一次数据。 external_labels: monitor: &#39;codelab-monitor&#39;rule_files: #- &#39;prometheus.rules&#39;# 这里表示抓取对象的配置scrape_configs: #这个配置是表示在这个配置内的时间序例，每一条都会自动添加上这个&#123;job_name:&quot;prometheus&quot;&#125;的标签 - job_name: &#39;prometheus&#39; - job_name: &#39;prometheus&#39; scrape_interval: 5s # 重写了全局抓取间隔时间，由15秒重写成5秒 static_configs: - targets: [&#39;localhost:9090&#39;] - targets: [&#39;http:&#x2F;&#x2F;10.211.55.25:8080&#39;, &#39;http:&#x2F;&#x2F;10.211.55.25:8081&#39;,&#39;http:&#x2F;&#x2F;10.211.55.25:8082&#39;] labels: group: &#39;client-golang&#39; - targets: [&#39;http:&#x2F;&#x2F;10.211.55.25:9100&#39;] labels: group: &#39;client-node-exporter&#39; 可以看到接口都生效了 prometheus还提供了各种exporter工具，感兴趣小伙伴可以去研究一下 三.安装pushgatewaypushgateway是为了允许临时作业和批处理作业向普罗米修斯公开他们的指标。 由于这类作业的存在时间可能不够长, 无法抓取到, 因此它们可以将指标推送到推网关中。 Prometheus采集数据是用的pull也就是拉模型，这从我们刚才设置的5秒参数就能看出来。但是有些数据并不适合采用这样的方式，对这样的数据可以使用Push Gateway服务。 它就相当于一个缓存，当数据采集完成之后，就上传到这里，由Prometheus稍后再pull过来。 我们来试一下，首先启动Push Gateway 123mkdir -p &#x2F;home&#x2F;chenqionghe&#x2F;promethues&#x2F;pushgatewaycd !$docker run -d -p 9091:9091 --name pushgateway prom&#x2F;pushgateway 访问http://10.211.55.25:9091 已经pushgateway运行起来了 接下来我们就可以往pushgateway推送数据了，prometheus提供了多种语言的sdk，最简单的方式就是通过shell 推送一个指标 1echo &quot;cqh_metric 3.14&quot; | curl --data-binary @- http:&#x2F;&#x2F;ubuntu-linux:9091&#x2F;metrics&#x2F;job&#x2F;cqh 推送多个指标 12345678cat &lt;&lt;EOF | curl --data-binary @- http:&#x2F;&#x2F;10.211.55.25:9091&#x2F;metrics&#x2F;job&#x2F;cqh&#x2F;instance&#x2F;test# 锻炼场所价格muscle_metric&#123;label&#x3D;&quot;gym&quot;&#125; 8800# 三大项数据 kgbench_press 100dead_lift 160deep_squal 160EOF 然后我们再将pushgateway配置到prometheus.yml里边,重载配置 看到已经可以搜索出刚刚推送的指标了 四.安装Grafana展示Grafana是用于可视化大型测量数据的开源程序，它提供了强大和优雅的方式去创建、共享、浏览数据。 Dashboard中显示了你不同metric数据源中的数据。 Grafana最常用于因特网基础设施和应用分析，但在其他领域也有用到，比如：工业传感器、家庭自动化、过程控制等等。 Grafana支持热插拔控制面板和可扩展的数据源，目前已经支持Graphite、InfluxDB、OpenTSDB、Elasticsearch、Prometheus等。 我们使用docker安装 1docker run -d -p 3000:3000 --name grafana grafana&#x2F;grafana 默认登录账户和密码都是admin，进入后界面如下 我们添加一个数据源 把Prometheus的地址填上 导入prometheus的模板 打开左上角选择已经导入的模板会看到已经有各种图 我们来添加一个自己的图表 指定自己想看的图标和关键字，右上角保存 看到如下数据 到这里我们就已经实现了数据的自动收集和展示，下面来说下prometheus如何自动报警 五.安装AlterManagerPormetheus的警告由独立的两部分组成。 Prometheus服务中的警告规则发送警告到Alertmanager。 然后这个Alertmanager管理这些警告。包括silencing, inhibition, aggregation，以及通过一些方法发送通知，例如：email，PagerDuty和HipChat。 建立警告和通知的主要步骤： 创建和配置Alertmanager 启动Prometheus服务时，通过-alertmanager.url标志配置Alermanager地址，以便Prometheus服务能和Alertmanager建立连接。 创建和配置Alertmanager 12mkdir -p &#x2F;home&#x2F;chenqionghe&#x2F;promethues&#x2F;alertmanagercd !$ 创建配置文件alertmanager.yml 123456789101112131415161718global: resolve_timeout: 5mroute: group_by: [&#39;cqh&#39;] group_wait: 10s #组报警等待时间 group_interval: 10s #组报警间隔时间 repeat_interval: 1m #重复报警间隔时间 receiver: &#39;web.hook&#39;receivers: - name: &#39;web.hook&#39; webhook_configs: - url: &#39;http:&#x2F;&#x2F;10.211.55.2:8888&#x2F;open&#x2F;test&#39;inhibit_rules: - source_match: severity: &#39;critical&#39; target_match: severity: &#39;warning&#39; equal: [&#39;alertname&#39;, &#39;dev&#39;, &#39;instance&#39;] 这里配置成了web.hook的方式，当server通知alertmanager会自动调用webhook http://10.211.55.2:8888/open/test 下面运行altermanager 12345docker rm -f alertmanagerdocker run -d -p 9093:9093 \\--name alertmanager \\-v &#x2F;home&#x2F;chenqionghe&#x2F;promethues&#x2F;alertmanager&#x2F;alertmanager.yml:&#x2F;etc&#x2F;alertmanager&#x2F;alertmanager.yml \\prom&#x2F;alertmanager 访问http://10.211.55.25:9093 接下来修改Server端配置报警规则和altermanager地址 修改规则/home/chenqionghe/promethues/server/rules.yml 1234567891011groups: - name: cqh rules: - alert: cqh测试 expr: dead_lift &gt; 150 for: 1m labels: status: warning annotations: summary: &quot;&#123;&#123;$labels.instance&#125;&#125;:硬拉超标！lightweight baby!!!&quot; description: &quot;&#123;&#123;$labels.instance&#125;&#125;:硬拉超标！lightweight baby!!!&quot; 这条规则的意思是，硬拉超过150公斤，持续一分钟，就报警通知 然后再修改prometheus添加altermanager配置 1234567891011121314151617181920212223242526global: scrape_interval: 15s # 默认抓取间隔, 15秒向目标抓取一次数据。 external_labels: monitor: &#39;codelab-monitor&#39;rule_files: - &#x2F;etc&#x2F;prometheus&#x2F;rules.yml# 这里表示抓取对象的配置scrape_configs: #这个配置是表示在这个配置内的时间序例，每一条都会自动添加上这个&#123;job_name:&quot;prometheus&quot;&#125;的标签 - job_name: &#39;prometheus&#39; - job_name: &#39;prometheus&#39; scrape_interval: 5s # 重写了全局抓取间隔时间，由15秒重写成5秒 static_configs: - targets: [&#39;localhost:9090&#39;] - targets: [&#39;10.211.55.25:8080&#39;, &#39;10.211.55.25:8081&#39;,&#39;10.211.55.25:8082&#39;] labels: group: &#39;client-golang&#39; - targets: [&#39;10.211.55.25:9100&#39;] labels: group: &#39;client-node-exporter&#39; - targets: [&#39;10.211.55.25:9091&#39;] labels: group: &#39;pushgateway&#39;alerting: alertmanagers: - static_configs: - targets: [&quot;10.211.55.25:9093&quot;] 重载prometheus配置，规则就已经生效 接下来我们观察grafana中数据的变化 然后我们点击prometheus的Alert模块，会看到已经由绿-&gt;黄-红，触发了报警 然后我们再来看看提供的webhook接口，这里的接口我是用的golang写的，接到数据后将body内容报警到钉钉 钉钉收到报警内容如下 到这里，从零开始搭建Prometheus实现自动监控报警就说介绍完了，一条龙服务，自动抓取接口+自动报警+优雅的图表展示，你还在等什么，赶紧high起来！","categories":[{"name":"监控","slug":"监控","permalink":"https://blog.fenxiangz.com/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"监控","slug":"监控","permalink":"https://blog.fenxiangz.com/tags/%E7%9B%91%E6%8E%A7/"},{"name":"报警","slug":"报警","permalink":"https://blog.fenxiangz.com/tags/%E6%8A%A5%E8%AD%A6/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://blog.fenxiangz.com/tags/Prometheus/"}]},{"title":"两种高性能 I/O 设计模式 Reactor 和 Proactor","slug":"linux/2017-12-08_两种高性能IO设计模式Reactor和Proactor","date":"2017-12-08T00:00:00.000Z","updated":"2020-12-20T16:47:02.978Z","comments":true,"path":"post/linux/2017-12-08_两种高性能IO设计模式Reactor和Proactor.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-12-08_%E4%B8%A4%E7%A7%8D%E9%AB%98%E6%80%A7%E8%83%BDIO%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8FReactor%E5%92%8CProactor.html","excerpt":"","text":"Reactor 和 Proactor 是基于事件驱动，在网络编程中经常用到两种设计模式。 曾经在一个项目中用到了网络库 libevent，也学习了一段时间，其内部实现所用到的就是 Reactor，所知道的还有 ACE；Proactor 模式的库有 Boost.Asio，ACE，暂时没有用过。但我也翻阅了一些文档，理解了它的实现方法。下面是我在学习这两种设计模式过程的笔记。 ReactorReactor，即反应堆。Reactor 的一般工作过程是首先在 Reactor 中注册（Reactor）感兴趣事件，并在注册时候指定某个已定义的回调函数（callback）；当客户端发送请求时，在 Reactor 中会触发刚才注册的事件，并调用对应的处理函数。在这一个处理回调函数中，一般会有数据接收、处理、回复请求等操作。 libevent 采用的就是 Reactor 的设计思想。其 Reactor 的中心思想是众所周知的 I/O 多路复用：select,poll,epoll,kqueue 等.libevent 精彩的将定时事件，信号处理，I/O 事件结合在在一起，也就是说用户同时在 Reactor 中注册上述三类事件。遗憾的是，libevent 不支持多线程，也就是说它同步处理请求，导致不能处理大量的请求；这样并不是说 Reactor 实现的网络库都不支持多线程，而是 libevent 本身的原因，我们也可以通过修改让 ilbevent 支持多线程，并发处理多个请求。 下面是 libevent 的一段代码，大概能够说明 Reactor 工作模式： /*accept callback function.*/ void accept_callback(int fd, short ev,void *arg) &#123; ...... &#125; ...... struct event accept_event; event_set(&amp;accept_event, socketlisten, EV_READ|EV_PERSIST, accept_callback, NULL); event_add(&amp;accept_event, NULL); event_dispatch(); Proactor从上面 Reactor 模式中，发现服务端数据的接收和发送都占用了用户状态（还有一种内核态），这样服务器的处理操作就在数据的读写上阻塞花费了时间，节省这些时间的办法是借助操作系统的异步读写；异步读写在调用的时候可以传递回调函数或者回送信号，当异步操作完毕，内核会自动调用回调函数或者发送信号。Proactor 就是这么做的，所以很依赖操作系统。来一幅 UML： 和时序图： 注：这两幅美艳的图片来自 Proactor.doc，下面会提到. Proactor 的实现主要有三个部分：异步操作处理器，Proactor 和 事件处理函数。其中： - 异步操作处理器，很依赖操作系统的异步处理机制，如若操作系统没有实现，我们可以自行模拟，即开专门的数据读写线程，数据读写完毕触发相应的时间（如果有注册的话）； - Proactor，会接收异步操作的提醒，调用相应的事件处理函数，它有自己的 event loop； - 事件处理函数，事件触发，执行操作； 曾经看过 Proactor.doc，作者是 Douglas C. Schmidt，你可以在这里阅读此文档。里面的关于 Proactor 的讲解很精彩，部分摘抄和自己的理解如下：当连接 web 服务器时： web 服务器指定（1）接收器，此接收器相当于服务器的客户端，它可以启动异步的 accept 操作； 接收器调用操作系统上的异步接收操作（2），并传递自己和 Proactor 的引用；异步接收操作结束后，前者用作事件处理函数，后者会回过头来分发事件；注：传递 Proactor 是为了让操作系统通知正确的 Proactor，可能会存在多个 Proactor；传递接收器自己是为了在异步接收操作结束后 Proactor 能调用正确的事件处理函数，以下同理。 web 服务器调用 Proactor 的事件循环；（3） web 浏览器连接 web 服务器；（4） 异步接收操作结束后，操作系统产生事件（通过回调或者信号）并通知 Proactor（5），Proactor 收到后会调用相应的事件处理函数，即交由接收器处理；（6） 接收器生成 HTTP 处理器，执行操作；（7） HTTP 处理器解析事件，启动异步读操作（8），获取来自浏览器的 GET 请求。同样，HTTP 处理器传递自己和 Proactor 的引用； web 服务器的控制权交还回 Proactor 的事件循环。（9） 接收 GET 请求过后，会处理数据： 浏览器发送（1）一个 HTTP GET 请求； 异步读操作结束后，操作系统会通知 Proactor，Proactor 分发给事件处理函数；（2，3） 事件处理器解析请求。（4）2-4 步骤会重复，指导所有的数据都接收为止； 事件处理器产生答复数据；（5） HTTP 处理器启动异步写操作（6），传输应答数据，同样的这里还会传递处理器自己和 Proactor； 异步写操作结束，操作系统通知 Proactor（7），Proactor 分发给事件处理函数（8）。6-8 步骤会重复直到所有的数据写完为止。至此，一个请求回复完成。 总结相比网络编程中最简单的思路模式：bind,listen,accept,read,server operator,write，Reactor 和 Proactor 是两种高性能的设计模式，掌握此两种模式，有助于理解一些网络库的工作流程。此文提到了两种设计模式，但没有一些技术细节，譬如多线程同步。如果在 Reactor 中支持多线程，或多个线程共享一个 Proactor，线程的同步问题就来了。共享一篇印象笔记关于线程的综合讨论：这里. 《Comparing Two High-Performance I/O Design Patterns》提到一个将 Reactor 模拟 Proactor 而不借助操作系统异步机制的方法：同样在 Reactor 注册感兴趣的事件（比如读），当事件发生时，执行非阻塞的读，读毕即才调用数据处理——假异步。 最后，实践出真知。欢迎讨论。 原文：http://daoluan.net/linux/%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/2013/08/20/two-high-performance-io-design-patterns.html","categories":[{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/categories/IO/"}],"tags":[{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"},{"name":"网络","slug":"网络","permalink":"https://blog.fenxiangz.com/tags/%E7%BD%91%E7%BB%9C/"},{"name":"模型","slug":"模型","permalink":"https://blog.fenxiangz.com/tags/%E6%A8%A1%E5%9E%8B/"}]},{"title":"Unix 网络 IO 模型及 Linux 的 IO 多路复用模型","slug":"linux/2017-12-07_Unix网络IO模型及Linux的IO多路复用模型","date":"2017-12-07T00:00:00.000Z","updated":"2020-12-20T16:47:02.978Z","comments":true,"path":"post/linux/2017-12-07_Unix网络IO模型及Linux的IO多路复用模型.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-12-07_Unix%E7%BD%91%E7%BB%9CIO%E6%A8%A1%E5%9E%8B%E5%8F%8ALinux%E7%9A%84IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E6%A8%A1%E5%9E%8B.html","excerpt":"","text":"近段在看 Kafka 的网络模型时，遇到了很多 Java NIO 的内容，在学习 Java NIO 的过程中，发现需要把 UNIX 的这几种网络 IO 模型以及 Linux 的 IO 多路复用理解清楚，才能更好地理解 Java NIO，本文就是在学习 UNIX 的五种网络 IO 模型以及 Linux IO 多路复用模型后，做的一篇总结。 本文主要探讨的问题有以下两个： Unix 中的五种网络 IO 模型； Linux 中 IO 多路复用的实现。 基本概念 在介绍网络模型之前，先简单介绍一些基本概念。 文件描述符 fd文件描述符（file descriptor，简称 fd）在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。 在 Linux 中，内核将所有的外部设备都当做一个文件来进行操作，而对一个文件的读写操作会调用内核提供的系统命令，返回一个 fd，对一个 socket 的读写也会有相应的描述符，称为 socketfd（socket 描述符），实际上描述符就是一个数字，它指向内核中的一个结构体（文件路径、数据区等一些属性）。 用户空间与内核空间、内核态与用户态这个是经常提到的概念，具体含义可以参考这篇文章用户空间与内核空间，进程上下文与中断上下文【总结】，大概内容如下： 现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操心系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核，保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对 linux 操作系统而言（以32位操作系统为例） 将最高的 1G 字节（从虚拟地址 0xC0000000 到 0xFFFFFFFF），供内核使用，称为内核空间； 将较低的 3G 字节（从虚拟地址 0x00000000 到 0xBFFFFFFF），供各个进程使用，称为用户空间。 每个进程可以通过系统调用进入内核，因此，Linux 内核由系统内的所有进程共享。于是，从具体进程的角度来看，每个进程可以拥有 4G 字节的虚拟空间。 当一个任务（进程）执行系统调用而陷入内核代码中执行时，称进程处于内核运行态（内核态）。此时处理器处于特权级最高的（0级）内核代码中执行。当进程处于内核态时，执行的内核代码会使用当前进程的内核栈，每个进程都有自己的内核栈； 当进程在执行用户自己的代码时，则称其处于用户运行态（用户态）。此时处理器在特权级最低的（3级）用户代码中运行。当正在执行用户程序而突然被中断程序中断时，此时用户程序也可以象征性地称为处于进程的内核态。因为中断处理程序将使用当前进程的内核栈。 上下文切换当一个进程在执行时，CPU 的所有寄存器中的值、进程的状态以及堆栈中的内容被称为该进程的上下文。 当内核需要切换到另一个进程时，它需要保存当前进程的所有状态，即保存当前进程的上下文，以便在再次执行该进程时，能够必得到切换时的状态执行下去。在 Linux 中，当前进程上下文均保存在进程的任务数据结构中。在发生中断时，内核就在被中断进程的上下文中，在内核态下执行中断服务例程。但同时会保留所有需要用到的资源，以便中继服务结束时能恢复被中断进程的执行。 UNIX 的网络 IO 模型 根据 UNIX 网络编程对 IO 模型的分类，UNIX 提供了以下 5 种 IO 模型。 阻塞 IO 模型最常用的 IO 模型就是阻塞 IO 模型，在缺省条件下，所有文件操作都是阻塞的，以 socket 读为例来介绍一下此模型，如下图所示。 在用户空间调用 recvfrom，系统调用直到数据包达到且被复制到应用进程的缓冲区中或中间发生异常返回，在这个期间进程会一直等待。进程从调用 recvfrom 开始到它返回的整段时间内都是被阻塞的，因此，被称为阻塞 IO 模型。 非阻塞 IO 模型recvfrom 从应用到内核的时，如果该缓冲区没有数据，就会直接返回 EWOULDBLOCK 错误，一般都对非阻塞 IO 模型进行轮询检查这个状态，看看内核是不是有数据到来，流程如下图所示。 也就是说非阻塞的 recvform 系统调用调用之后，进程并没有被阻塞，内核马上返回给进程。 如果数据还没准备好，此时会返回一个 error。进程在返回之后，可以干点别的事情，然后再发起 recvform 系统调用。重复上面的过程，循环往复的进行 recvform 系统调用，这个过程通常被称之为轮询。 轮询检查内核数据，直到数据准备好，再拷贝数据到进程，进行数据处理。需要注意，拷贝数据整个过程，进程仍然是属于阻塞的状态。 在 Linux 下，可以通过设置 socket 使其变为 non-blocking。 IO 多路复用模型Linux 提供 select、poll、epoll，进程通过讲一个或者多个 fd 传递给 select、poll、epoll 系统调用，阻塞在 select 操作（这个是内核级别的调用）上，这样的话，可以同时监听多个 fd 是否处于就绪状态。其中， select/poll 是顺序扫描 fd 是否就绪，而且支持的 fd 数量有限； epoll 是基于事件驱动方式代替顺序扫描性能更高。 这个后面详细讲述，具体流程如下图所示。 多路复用的特点是通过一种机制一个进程能同时等待 IO 文件描述符，内核监视这些文件描述符（套接字描述符），其中的任意一个进入读就绪状态，select， poll，epoll 函数就可以返回，它最大的优势就是可以同时处理多个连接。 信号驱动 IO 模型首先需要开启 socket 信号驱动 IO 功能，并通过系统调用 sigaction 执行一个信号处理函数（非阻塞，立即返回）。当数据就绪时，会为该进程生成一个 SIGIO 信号，通过信号回调通知应用程序调用 recvfrom 来读取数据，并通知主循环喊出处理数据，流程如下图所示。 异步 IO 模型告知内核启动某个事件，并让内核在整个操作完成后（包括将数据从内核复制到用户自己的缓冲区）通过我们，流程如下图所示。 与信号驱动模式的主要区别是： 信号驱动 IO 由内核通知我们何时可以开始一个 IO 操作； 异步 IO 操作由内核通知我们 IO 何时完成。 内核是通过向应用程序发送 signal 或执行一个基于线程的回调函数来完成这次 IO 处理过程，告诉用户 read 操作已经完成，在 Linux 中，通知的方式是信号： 当进程正处于用户态时，应用需要立马进行处理，一般情况下，是先将事件登记一下，放进一个队列中； 当进程正处于内核态时，比如正在以同步阻塞模式读磁盘，那么只能先把这个通知挂起来，等内核态的事情完成之后，再触发信号通知； 如果这个进程现在被挂起来了，比如 sleep，那就把这个进程唤醒，等 CPU 空闲时，就会调度这个进程，触发信号通知。 几种 IO 模型比较 Linux 的 IO 多路复用模型IO 多路复用通过把多个 IO 阻塞复用到同一个 select 的阻塞上，从而使得系统在单线程的情况下，可以同时处理多个 client 请求，与传统的多线程/多进程模型相比，IO 多路复用的最大优势是系统开销小，系统不需要创建新的额外的进程或线程，也不需要维护这些进程和线程的运行，节省了系统资源，IO 多路复用的主要场景如下： Server 需要同时处理多个处于监听状态或者连接状态的 socket； Server 需要同时处理多种网络协议的 socket。 IO 多路复用实际上就是通过一种机制，一个进程可以监视多个描 fd，一旦某个 fd 就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作，目前支持 IO 多路复用的系统有 select、pselect、poll、epoll，但它们本质上都是同步 IO。 在 Linux 网络编程中，最初是选用 select 做轮询和网络事件通知，然而 select 的一些固有缺陷导致了它的应用受到了很大的限制，最终 Linux 选择 epoll。 selectselect 函数监视的 fd 分3类，分别是 writefds、readfds、和 exceptfds。调用后select 函数会阻塞，直到有 fd 就绪（有数据 可读、可写、或者有 except），或者超时（timeout 指定等待时间，如果立即返回设为 null 即可），函数返回。当select函数返回后，可以通过遍历 fdset，来找到就绪的 fd。 select 目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select 的一个最大的缺陷就是单个进程对打开的 fd 是有一定限制的，它由 FD_SETSIZE 限制，默认值是1024，如果修改的话，就需要重新编译内核，不过这会带来网络效率的下降。 select 和 poll 另一个缺陷就是随着 fd 数目的增加，可能只有很少一部分 socket 是活跃的，但是 select/poll 每次调用时都会线性扫描全部的集合，导致效率呈现线性的下降。 pollpoll 本质上和 select 没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个 fd 对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有 fd 后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历 fd。这个过程经历了多次无谓的遍历。 它没有最大连接数的限制，原因是它是基于链表来存储的，但是同样以下两个缺点： 大量的 fd 的数组被整体复制于用户态和内核地址空间之间； poll 还有一个特点是【水平触发】，如果报告了 fd 后，没有被处理，那么下次 poll 时会再次报告该 fd； fd 增加时，线性扫描导致性能下降。 epollepoll 支持水平触发和边缘触发，最大的特点在于边缘触发，它只告诉进程哪些 fd 变为就绪态，并且只会通知一次。还有一个特点是，epoll 使用【事件】的就绪通知方式，通过 epoll_ctl 注册 fd，一旦该 fd 就绪，内核就会采用类似 callback 的回调机制来激活该 fd，epoll_wait 便可以收到通知。 epoll的优点： 没有最大并发连接的限制，它支持的 fd 上限受操作系统最大文件句柄数； 效率提升，不是轮询的方式，不会随着 fd 数目的增加效率下降。epoll 只会对【活跃】的 socket 进行操作，这是因为在内核实现中 epoll 是根据每个 fd 上面的 callback 函数实现的，只有【活跃】的 socket 才会主动的去调用 callback 函数，其他 idle 状态的 socket 则不会。epoll 的性能不会受 fd 总数的限制。 select/poll 都需要内核把 fd 消息通知给用户空间，而 epoll 是通过内核和用户空间 mmap 同一块内存实现。 epoll 对 fd 的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT 模式是默认模式，LT 模式与 ET 模式的区别如下： LT 模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件，下次调用 epoll_wait 时，会再次响应应用程序并通知此事件； ET 模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件，如果不处理，下次调用 epoll_wait 时，不会再次响应应用程序并通知此事件。 三种模型的区别 类别 select poll epoll 支持的最大连接数 由 FD_SETSIZE 限制 基于链表存储，没有限制 受系统最大句柄数限制 fd 剧增的影响 线性扫描 fd 导致性能很低 同 select 基于 fd 上 callback 实现，没有性能下降的问题 消息传递机制 内核需要将消息传递到用户空间，需要内核拷贝 同 select epoll 通过内核与用户空间共享内存来实现 介绍完 IO 多路复用之后，后续我们看一下 Java 网络编程中的 NIO 模型及其背后的实现机制。 参考 《Netty 权威指南》 用户空间与内核空间，进程上下文与中断上下文【总结】 聊聊 Linux 中的五种 IO 模型 聊聊IO多路复用之select、poll、epoll详解 高性能Server—Reactor模型 原文：http://matt33.com/2017/08/06/unix-io/","categories":[{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/categories/IO/"}],"tags":[{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"},{"name":"网络","slug":"网络","permalink":"https://blog.fenxiangz.com/tags/%E7%BD%91%E7%BB%9C/"},{"name":"模型","slug":"模型","permalink":"https://blog.fenxiangz.com/tags/%E6%A8%A1%E5%9E%8B/"}]},{"title":"重命名磁盘挂载分区卷标(磁盘名)","slug":"linux/2017-12-06_重命名磁盘挂载分区卷标(磁盘名)","date":"2017-12-06T00:00:00.000Z","updated":"2020-12-20T16:47:02.978Z","comments":true,"path":"post/linux/2017-12-06_重命名磁盘挂载分区卷标(磁盘名).html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-12-06_%E9%87%8D%E5%91%BD%E5%90%8D%E7%A3%81%E7%9B%98%E6%8C%82%E8%BD%BD%E5%88%86%E5%8C%BA%E5%8D%B7%E6%A0%87(%E7%A3%81%E7%9B%98%E5%90%8D).html","excerpt":"","text":"编辑ext2/ext3/FAT32/NTFS磁盘分区卷标根据不同的磁盘分区类型,分别有3个程序可供选用. Mtools 适用于 FAT32 格式分区。ntfsprogs 适用于 NTFS 格式分区。e2label适用于 ext2 和 ext3 型格式分区。 以上程序的具体使用说明分别如下: 使用mtools编辑FAT32磁盘分区卷标我想更改由系统自动挂载的USB设备中的FAT32分区卷标.我有两个外接硬盘驱动器(一个日常家用,一个公司工作用),其中一个是iPod.这两个驱动器都被系统以”sda1”或”sda2”等名称挂载于”/media”目录下,在电脑里我很难通过这些名字辨认出哪个文件夹是对应哪个驱动器.后来,我发现更改这些驱动器上的FAT32分区卷标不是件容易事.所以我觉得有必要将我是如何修改这些FAT32分区卷标的过程写下来.方便那些遇到同样问题的人.讲解之前首先明白:系统会将外接的驱动器自动挂载到”/media/”目录下,以”sda1”类似的卷标名命名分区,为了容易区别各分区,我们需要修改默认的卷标 按如下步骤一步一步操作即可更改FAT分区卷标: 操作指导 安装mtools 软件包 sudo apt-get install mtools 系统自动装载插入的USB设备后,可以用如下命令查看新设备相关信息: mount 显示信息中”sda1”或与之相似的字段即是系统分配给设备的名字。 复制”/etc”目录下mtools.conf文件为新文件”~/.mtoolsrc” cp /etc/mtools.conf ~/.mtoolsrc 编辑刚复制的”~/.mtoolsrc”文件,在最后一行加入如下命令行： drive i: file=”/dev/sda2” 上面命令行中字段”sda2”应根据实际情况更改为你在第二步操作中所看到的新设备名称。 更改命令提示符路径到”i:”盘： mcd i: 查看”i:”当前的卷标 sudo mlabel -s i: 更改”i:”盘原始卷标为你喜欢的新卷标名： sudo mlabel i:my-ipod 你可以将上述命令行操作中的”my-ipod”字段替换为你喜欢的名字,用以代表插入的USB设备。 检查更改是否成功 sudo mlabel -s i: 经过以上操作,电脑显示如下信息: Volume label is MY-IPODYou’re 恭喜!卷标修改已经成功.下次插入USB设备后,你可以在目录” /media/MY-IPOD”下找到你USB设备上的文件。 使用ntfsprogs 修改NTFS分区卷标操作指导 安装ntfsprogs软件包 sudo apt-get install ntfsprogs NTFS分区驱动器插入后被自动装载,可以用如下命令查看此新设备相关信息： mount 显示信息中”sda1”或与之相似的字段即是系统分配给新设备的名字。 更改原始卷标为你喜欢的新卷标名： sudo ntfslabel /dev/sda1 newlabel replace newlabel with what you would like to name the usb drive / Harddisk用你喜欢的卷标名替换此命令中的单词”newlabel” 。 和FAT分区不同,更改NTFS分区卷标后你必须卸下此设备(卸载步骤:系统-&gt;管理-&gt;磁盘管理-&gt;硬盘分区下面的”禁用”按钮) 检查更改是否成功 重启电脑后查看相应NTFS分区卷标是否改变.如果你更改的是USB设备卷标,你需要重新插入设备。 使用e2label更改 ext2 或ext3 分区卷标操作指南 设备被自动加载后，可以用如下命令查看此新设备相关信息: mount 显示信息中”sda1”或与之相似的字段即是系统分配给新设备的名字。 更改原始卷标为你喜欢的新卷标名： sudo e2label /dev/sda1 newlabel 用你喜欢的卷标名替换此命令中的单词”newlabel” 更改分区卷标后你必须卸下此设备(卸载步骤:系统-&gt;管理-&gt;磁盘管理-&gt;硬盘分区下面的”禁用”按钮)。 检查更改是否成功： 重启电脑后查看相应分区卷标是否改变.如果你更改的是USB设备卷标,你需要重新插入设备。 原文：http://www.cnblogs.com/xusion/articles/3015145.html","categories":[{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"磁盘管理","slug":"磁盘管理","permalink":"https://blog.fenxiangz.com/tags/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/"},{"name":"挂载","slug":"挂载","permalink":"https://blog.fenxiangz.com/tags/%E6%8C%82%E8%BD%BD/"}]},{"title":"2017-12-05_为什么tcp/udp的端口号可以重复.md","slug":"network/2017-12-05_为什么TCP和UDP的端口号可以重复","date":"2017-12-05T00:00:00.000Z","updated":"2020-12-20T16:47:02.981Z","comments":true,"path":"post/network/2017-12-05_为什么TCP和UDP的端口号可以重复.html","link":"","permalink":"https://blog.fenxiangz.com/post/network/2017-12-05_%E4%B8%BA%E4%BB%80%E4%B9%88TCP%E5%92%8CUDP%E7%9A%84%E7%AB%AF%E5%8F%A3%E5%8F%B7%E5%8F%AF%E4%BB%A5%E9%87%8D%E5%A4%8D.html","excerpt":"","text":"使用 TCP/UDP 协议的上层应用程序可以分配重复的端口号，也就是说一个端口号 8080 可以同时出现在 TCP、UDP 中。看下 《TCP/IP 详解》中的这个图就很清楚了：","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"https://blog.fenxiangz.com/categories/TCP-IP/"}],"tags":[{"name":"协议","slug":"协议","permalink":"https://blog.fenxiangz.com/tags/%E5%8D%8F%E8%AE%AE/"},{"name":"TCP","slug":"TCP","permalink":"https://blog.fenxiangz.com/tags/TCP/"},{"name":"UDP","slug":"UDP","permalink":"https://blog.fenxiangz.com/tags/UDP/"}]},{"title":"XXNET 挺不错的，记录一下简单的配置教程","slug":"linux/2017-12-04_XXNET挺不错的，记录一下简单的配置教程","date":"2017-12-04T00:00:00.000Z","updated":"2020-12-20T16:47:02.978Z","comments":true,"path":"post/linux/2017-12-04_XXNET挺不错的，记录一下简单的配置教程.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-12-04_XXNET%E6%8C%BA%E4%B8%8D%E9%94%99%E7%9A%84%EF%BC%8C%E8%AE%B0%E5%BD%95%E4%B8%80%E4%B8%8B%E7%AE%80%E5%8D%95%E7%9A%84%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html","excerpt":"","text":"简单使用：https://github.com/XX-net/XX-Net/wiki/How-to-use 默认配置看不了视频。 自己创建 GAE appid因为官方教程有点出路，我自己整理了一下操作步骤。 自己创建 GAE appid 需要事先有一个梯子。 Google 部分操作 登录/注册Google帐户 访问：https://console.developers.google.com/cloud-resource-manager 点击图例： 按步骤输入项目名称创建就好了 创建完后，刷新：https://console.developers.google.com/cloud-resource-manager 页面，就能看见你创建的 app id 了 进入页面：https://console.cloud.google.com/home/dashboard 按图操作： 按图操作： 选 Python 进入下一个页面，选择一个服务器地区，选 us-central 就好，然后就等待初始化完成就好了。Google这边就设置完了。 XXNET 部分操作官方教程：https://github.com/XX-net/XX-Net/wiki/How-to-use","categories":[{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"XXNET","slug":"XXNET","permalink":"https://blog.fenxiangz.com/tags/XXNET/"}]},{"title":"开启bbr协议 一键安装脚本","slug":"linux/2017-12-03_开启bbr协议一键安装脚本","date":"2017-12-03T00:00:00.000Z","updated":"2020-12-20T16:47:02.977Z","comments":true,"path":"post/linux/2017-12-03_开启bbr协议一键安装脚本.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-12-03_%E5%BC%80%E5%90%AFbbr%E5%8D%8F%E8%AE%AE%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC.html","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216#!&#x2F;usr&#x2F;bin&#x2F;env bash## Auto install latest kernel for TCP BBR## System Required: CentOS 6+, Debian7+, Ubuntu12+## Copyright (C) 2016-2017 Teddysun &lt;i@teddysun.com&gt;## URL: https:&#x2F;&#x2F;teddysun.com&#x2F;489.html#red&#x3D;&#39;\\033[0;31m&#39;green&#x3D;&#39;\\033[0;32m&#39;yellow&#x3D;&#39;\\033[0;33m&#39;plain&#x3D;&#39;\\033[0m&#39;[[ $EUID -ne 0 ]] &amp;&amp; echo -e &quot;$&#123;red&#125;Error:$&#123;plain&#125; This script must be run as root!&quot; &amp;&amp; exit 1[[ -d &quot;&#x2F;proc&#x2F;vz&quot; ]] &amp;&amp; echo -e &quot;$&#123;red&#125;Error:$&#123;plain&#125; Your VPS is based on OpenVZ, not be supported.&quot; &amp;&amp; exit 1if [ -f &#x2F;etc&#x2F;redhat-release ]; then release&#x3D;&quot;centos&quot;elif cat &#x2F;etc&#x2F;issue | grep -Eqi &quot;debian&quot;; then release&#x3D;&quot;debian&quot;elif cat &#x2F;etc&#x2F;issue | grep -Eqi &quot;ubuntu&quot;; then release&#x3D;&quot;ubuntu&quot;elif cat &#x2F;etc&#x2F;issue | grep -Eqi &quot;centos|red hat|redhat&quot;; then release&#x3D;&quot;centos&quot;elif cat &#x2F;proc&#x2F;version | grep -Eqi &quot;debian&quot;; then release&#x3D;&quot;debian&quot;elif cat &#x2F;proc&#x2F;version | grep -Eqi &quot;ubuntu&quot;; then release&#x3D;&quot;ubuntu&quot;elif cat &#x2F;proc&#x2F;version | grep -Eqi &quot;centos|red hat|redhat&quot;; then release&#x3D;&quot;centos&quot;figet_latest_version() &#123; latest_version&#x3D;$(wget -qO- http:&#x2F;&#x2F;kernel.ubuntu.com&#x2F;~kernel-ppa&#x2F;mainline&#x2F; | awk -F&#39;\\&quot;v&#39; &#39;&#x2F;v[4-9].&#x2F;&#123;print $2&#125;&#39; | cut -d&#x2F; -f1 | grep -v - | sort -V | tail -1) [ -z $&#123;latest_version&#125; ] &amp;&amp; return 1 if [[ &#96;getconf WORD_BIT&#96; &#x3D;&#x3D; &quot;32&quot; &amp;&amp; &#96;getconf LONG_BIT&#96; &#x3D;&#x3D; &quot;64&quot; ]]; then deb_name&#x3D;$(wget -qO- http:&#x2F;&#x2F;kernel.ubuntu.com&#x2F;~kernel-ppa&#x2F;mainline&#x2F;v$&#123;latest_version&#125;&#x2F; | grep &quot;linux-image&quot; | grep &quot;generic&quot; | awk -F&#39;\\&quot;&gt;&#39; &#39;&#x2F;amd64.deb&#x2F;&#123;print $2&#125;&#39; | cut -d&#39;&lt;&#39; -f1 | head -1) deb_kernel_url&#x3D;&quot;http:&#x2F;&#x2F;kernel.ubuntu.com&#x2F;~kernel-ppa&#x2F;mainline&#x2F;v$&#123;latest_version&#125;&#x2F;$&#123;deb_name&#125;&quot; deb_kernel_name&#x3D;&quot;linux-image-$&#123;latest_version&#125;-amd64.deb&quot; else deb_name&#x3D;$(wget -qO- http:&#x2F;&#x2F;kernel.ubuntu.com&#x2F;~kernel-ppa&#x2F;mainline&#x2F;v$&#123;latest_version&#125;&#x2F; | grep &quot;linux-image&quot; | grep &quot;generic&quot; | awk -F&#39;\\&quot;&gt;&#39; &#39;&#x2F;i386.deb&#x2F;&#123;print $2&#125;&#39; | cut -d&#39;&lt;&#39; -f1 | head -1) deb_kernel_url&#x3D;&quot;http:&#x2F;&#x2F;kernel.ubuntu.com&#x2F;~kernel-ppa&#x2F;mainline&#x2F;v$&#123;latest_version&#125;&#x2F;$&#123;deb_name&#125;&quot; deb_kernel_name&#x3D;&quot;linux-image-$&#123;latest_version&#125;-i386.deb&quot; fi [ ! -z $&#123;deb_name&#125; ] &amp;&amp; return 0 || return 1&#125;get_opsy() &#123; [ -f &#x2F;etc&#x2F;redhat-release ] &amp;&amp; awk &#39;&#123;print ($1,$3~&#x2F;^[0-9]&#x2F;?$3:$4)&#125;&#39; &#x2F;etc&#x2F;redhat-release &amp;&amp; return [ -f &#x2F;etc&#x2F;os-release ] &amp;&amp; awk -F&#39;[&#x3D; &quot;]&#39; &#39;&#x2F;PRETTY_NAME&#x2F;&#123;print $3,$4,$5&#125;&#39; &#x2F;etc&#x2F;os-release &amp;&amp; return [ -f &#x2F;etc&#x2F;lsb-release ] &amp;&amp; awk -F&#39;[&#x3D;&quot;]+&#39; &#39;&#x2F;DESCRIPTION&#x2F;&#123;print $2&#125;&#39; &#x2F;etc&#x2F;lsb-release &amp;&amp; return&#125;opsy&#x3D;$( get_opsy )arch&#x3D;$( uname -m )lbit&#x3D;$( getconf LONG_BIT )kern&#x3D;$( uname -r )get_char() &#123; SAVEDSTTY&#x3D;&#96;stty -g&#96; stty -echo stty cbreak dd if&#x3D;&#x2F;dev&#x2F;tty bs&#x3D;1 count&#x3D;1 2&gt; &#x2F;dev&#x2F;null stty -raw stty echo stty $SAVEDSTTY&#125;getversion() &#123; if [[ -s &#x2F;etc&#x2F;redhat-release ]]; then grep -oE &quot;[0-9.]+&quot; &#x2F;etc&#x2F;redhat-release else grep -oE &quot;[0-9.]+&quot; &#x2F;etc&#x2F;issue fi&#125;centosversion() &#123; if [ &quot;$&#123;release&#125;&quot; &#x3D;&#x3D; &quot;centos&quot; ]; then local code&#x3D;$1 local version&#x3D;&quot;$(getversion)&quot; local main_ver&#x3D;$&#123;version%%.*&#125; if [ &quot;$main_ver&quot; &#x3D;&#x3D; &quot;$code&quot; ]; then return 0 else return 1 fi else return 1 fi&#125;check_bbr_status() &#123; local param&#x3D;$(sysctl net.ipv4.tcp_available_congestion_control | awk &#39;&#123;print $3&#125;&#39;) if uname -r | grep -Eqi &quot;4.10.&quot;; then if [[ &quot;$&#123;param&#125;&quot; &#x3D;&#x3D; &quot;bbr&quot; ]]; then return 0 else return 1 fi else return 1 fi&#125;install_elrepo() &#123; if centosversion 5; then echo -e &quot;$&#123;red&#125;Error:$&#123;plain&#125; not supported CentOS 5.&quot; exit 1 fi rpm --import https:&#x2F;&#x2F;www.elrepo.org&#x2F;RPM-GPG-KEY-elrepo.org if centosversion 6; then rpm -Uvh http:&#x2F;&#x2F;www.elrepo.org&#x2F;elrepo-release-6-6.el6.elrepo.noarch.rpm elif centosversion 7; then rpm -Uvh http:&#x2F;&#x2F;www.elrepo.org&#x2F;elrepo-release-7.0-2.el7.elrepo.noarch.rpm fi if [ ! -f &#x2F;etc&#x2F;yum.repos.d&#x2F;elrepo.repo ]; then echo -e &quot;$&#123;red&#125;Error:$&#123;plain&#125; Install elrepo failed, please check it.&quot; exit 1 fi&#125;install_config() &#123; if [[ &quot;$&#123;release&#125;&quot; &#x3D;&#x3D; &quot;centos&quot; ]]; then if centosversion 6; then if [ ! -f &quot;&#x2F;boot&#x2F;grub&#x2F;grub.conf&quot; ]; then echo -e &quot;$&#123;red&#125;Error:$&#123;plain&#125; &#x2F;boot&#x2F;grub&#x2F;grub.conf not found, please check it.&quot; exit 1 fi sed -i &#39;s&#x2F;^default&#x3D;.*&#x2F;default&#x3D;0&#x2F;g&#39; &#x2F;boot&#x2F;grub&#x2F;grub.conf elif centosversion 7; then if [ ! -f &quot;&#x2F;boot&#x2F;grub2&#x2F;grub.cfg&quot; ]; then echo -e &quot;$&#123;red&#125;Error:$&#123;plain&#125; &#x2F;boot&#x2F;grub2&#x2F;grub.cfg not found, please check it.&quot; exit 1 fi grub2-set-default 0 fi elif [[ &quot;$&#123;release&#125;&quot; &#x3D;&#x3D; &quot;debian&quot; || &quot;$&#123;release&#125;&quot; &#x3D;&#x3D; &quot;ubuntu&quot; ]]; then &#x2F;usr&#x2F;sbin&#x2F;update-grub fi sed -i &#39;&#x2F;net.core.default_qdisc&#x2F;d&#39; &#x2F;etc&#x2F;sysctl.conf sed -i &#39;&#x2F;net.ipv4.tcp_congestion_control&#x2F;d&#39; &#x2F;etc&#x2F;sysctl.conf echo &quot;net.core.default_qdisc &#x3D; fq&quot; &gt;&gt; &#x2F;etc&#x2F;sysctl.conf echo &quot;net.ipv4.tcp_congestion_control &#x3D; bbr&quot; &gt;&gt; &#x2F;etc&#x2F;sysctl.conf sysctl -p &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1&#125;install_bbr() &#123; check_bbr_status if [ $? -eq 0 ]; then echo echo -e &quot;$&#123;green&#125;Info:$&#123;plain&#125; TCP BBR has been successfully installed. nothing to do...&quot; exit fi if [[ &quot;$&#123;release&#125;&quot; &#x3D;&#x3D; &quot;centos&quot; ]]; then install_elrepo yum --enablerepo&#x3D;elrepo-kernel -y install kernel-ml kernel-ml-devel if [ $? -ne 0 ]; then echo -e &quot;$&#123;red&#125;Error:$&#123;plain&#125; Install latest kernel failed, please check it.&quot; exit 1 fi elif [[ &quot;$&#123;release&#125;&quot; &#x3D;&#x3D; &quot;debian&quot; || &quot;$&#123;release&#125;&quot; &#x3D;&#x3D; &quot;ubuntu&quot; ]]; then [[ ! -e &quot;&#x2F;usr&#x2F;bin&#x2F;wget&quot; ]] &amp;&amp; apt-get -y update &amp;&amp; apt-get -y install wget get_latest_version [ $? -ne 0 ] &amp;&amp; echo -e &quot;$&#123;red&#125;Error:$&#123;plain&#125; Get latest kernel version failed.&quot; &amp;&amp; exit 1 wget -c -t3 -T60 -O $&#123;deb_kernel_name&#125; $&#123;deb_kernel_url&#125; if [ $? -ne 0 ]; then echo -e &quot;$&#123;red&#125;Error:$&#123;plain&#125; Download $&#123;deb_kernel_name&#125; failed, please check it.&quot; exit 1 fi dpkg -i $&#123;deb_kernel_name&#125; rm -fv $&#123;deb_kernel_name&#125; else echo -e &quot;$&#123;red&#125;Error:$&#123;plain&#125; OS is not be supported, please change to CentOS&#x2F;Debian&#x2F;Ubuntu and try again.&quot; exit 1 fi install_config&#125;clearecho &quot;---------- System Information ----------&quot;echo &quot; OS : $opsy&quot;echo &quot; Arch : $arch ($lbit Bit)&quot;echo &quot; Kernel : $kern&quot;echo &quot;----------------------------------------&quot;echo &quot; Auto install latest kernel for TCP BBR&quot;echoecho &quot; URL: https:&#x2F;&#x2F;teddysun.com&#x2F;489.html&quot;echo &quot;----------------------------------------&quot;echoecho &quot;Press any key to start...or Press Ctrl+C to cancel&quot;char&#x3D;&#96;get_char&#96;install_bbrechoread -p &quot;Info: The system needs to be restart. Do you want to reboot? [y&#x2F;n]&quot; is_rebootif [[ $&#123;is_reboot&#125; &#x3D;&#x3D; &quot;y&quot; || $&#123;is_reboot&#125; &#x3D;&#x3D; &quot;Y&quot; ]]; then rebootelse exitfi","categories":[{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"bbr","slug":"bbr","permalink":"https://blog.fenxiangz.com/tags/bbr/"},{"name":"协议","slug":"协议","permalink":"https://blog.fenxiangz.com/tags/%E5%8D%8F%E8%AE%AE/"}]},{"title":"super + d 显示桌面快捷键","slug":"linux/2017-12-01_super+d显示桌面快捷键","date":"2017-12-01T00:00:00.000Z","updated":"2020-12-20T16:47:02.977Z","comments":true,"path":"post/linux/2017-12-01_super+d显示桌面快捷键.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-12-01_super+d%E6%98%BE%E7%A4%BA%E6%A1%8C%E9%9D%A2%E5%BF%AB%E6%8D%B7%E9%94%AE.html","excerpt":"","text":"如果某个 Linux 发行版没有super + d 显示桌面快捷键，可以自己配置一下 命令： wmctrl -k on 通常在 system - setting - keyboard 的 Custom 标签下进行设置 wmctrl 没有的话，需要自己安装一下 -EOF-","categories":[{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"快捷键","slug":"快捷键","permalink":"https://blog.fenxiangz.com/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/"}]},{"title":"解决 Linux 下 chrome 无法播放 flash 问题","slug":"linux/2017-11-30_解决Linux下Chrome无法播放flash问题","date":"2017-11-30T00:00:00.000Z","updated":"2020-12-20T16:47:02.977Z","comments":true,"path":"post/linux/2017-11-30_解决Linux下Chrome无法播放flash问题.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-11-30_%E8%A7%A3%E5%86%B3Linux%E4%B8%8BChrome%E6%97%A0%E6%B3%95%E6%92%AD%E6%94%BEflash%E9%97%AE%E9%A2%98.html","excerpt":"","text":"chrome 54 以上版本没有自带 flash，在播放 flash 视频时 chrome 会下载 swf 文件。如果在 chrome://plugins 启用了 flash 插件，则会显示 flash out of date。 需要让 chrome 安装 flash 插件。在地址栏输入 chrome://components，然后点击 Adobe Flash Player 下面的 Check for update，chrome 会下载安装 flash 插件。下载插件需要代理，但 chrome 不会使用代理插件（例如 SwithyOmega）提供的代理，应该用外部程序代理。我直接用路由器的 shadowsocks 了。 此时如果启用了 flash 插件，则会卡在 Checking for status。需要先到 chrome://plugins 禁用 flash 插件。 安装完毕显示 Component updated 后，在地址栏输入 chrome://plugins，点击页面右侧的 Details，点击 Adobe Flash Player 项中 PPAPI 下面的 Enable，然后勾选 “Always allowed to run”，启用 flash。","categories":[{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"deb","slug":"deb","permalink":"https://blog.fenxiangz.com/tags/deb/"},{"name":"chrome","slug":"chrome","permalink":"https://blog.fenxiangz.com/tags/chrome/"}]},{"title":"安装 .deb 包被中断问题","slug":"linux/2017-11-29_安装deb包被中断问题","date":"2017-11-29T00:00:00.000Z","updated":"2020-12-20T16:47:02.977Z","comments":true,"path":"post/linux/2017-11-29_安装deb包被中断问题.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-11-29_%E5%AE%89%E8%A3%85deb%E5%8C%85%E8%A2%AB%E4%B8%AD%E6%96%AD%E9%97%AE%E9%A2%98.html","excerpt":"","text":"安装 .deb 包被中断，导致重启也不能再次进行安装，报错： only one software management tool is allowed to run restarting is not work 解决方法： Make sure software centre is not open and that updates are not running. 使用： 1sudo rm -f &#x2F;var&#x2F;lib&#x2F;dpkg&#x2F;lock 可以解锁 software installer programs 然后执行： 12sudo fuser -vki &#x2F;var&#x2F;lib&#x2F;dpkg&#x2F;locksudo dpkg --configure -a 即可解决。","categories":[{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"deb","slug":"deb","permalink":"https://blog.fenxiangz.com/tags/deb/"}]},{"title":"Unix目录结构的来历","slug":"linux/2017-11-28_Unix目录结构的来历","date":"2017-11-28T00:00:00.000Z","updated":"2020-12-20T16:47:02.977Z","comments":true,"path":"post/linux/2017-11-28_Unix目录结构的来历.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-11-28_Unix%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E7%9A%84%E6%9D%A5%E5%8E%86.html","excerpt":"","text":"原文出处： 阮一峰（ @ruanyf ） Unix（包含Linux）的初学者，常常会很困惑，不明白目录结构的含义何在。 举例来说，根目录下面有一个子目录/bin，用于存放二进制程序。但是，/usr子目录下面还有/usr/bin，以及/usr/local/bin，也用于存放二进制程序；某些系统甚至还有/opt/bin。它们有何区别？ 长久以来，我也感到很费解，不明白为什么这样设计。像大多数人一样，我只是根据《Unix文件系统结构标准》（Filesystem Hierarchy Standard），死记硬背不同目录的区别。 昨天，我读到了Rob Landley的简短解释，这才恍然大悟，原来Unix目录结构是历史造成的。 话说1969年，Ken Thompson和Dennis Ritchie在小型机PDP-7上发明了Unix。1971年，他们将主机升级到了PDP-11。 当时，他们使用一种叫做RK05的储存盘，一盘的容量大约是1.5MB。 没过多久，操作系统（根目录）变得越来越大，一块盘已经装不下了。于是，他们加上了第二盘RK05，并且规定第一块盘专门放系统程序，第二块盘专门放用户自己的程序，因此挂载的目录点取名为/usr。也就是说，根目录”/”挂载在第一块盘，”/usr”目录挂载在第二块盘。除此之外，两块盘的目录结构完全相同，第一块盘的目录（/bin, /sbin, /lib, /tmp…）都在/usr目录下重新出现一次。 后来，第二块盘也满了，他们只好又加了第三盘RK05，挂载的目录点取名为/home，并且规定/usr用于存放用户的程序，/home用于存放用户的数据。 从此，这种目录结构就延续了下来。随着硬盘容量越来越大，各个目录的含义进一步得到明确。 /：存放系统程序，也就是At&amp;t开发的Unix程序。 /usr：存放Unix系统商（比如IBM和HP）开发的程序。 /usr/local：存放用户自己安装的程序。 /opt：在某些系统，用于存放第三方厂商开发的程序，所以取名为option，意为”选装”","categories":[{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"目录","slug":"目录","permalink":"https://blog.fenxiangz.com/tags/%E7%9B%AE%E5%BD%95/"}]},{"title":"Docker 学习笔记","slug":"docker/2017-11-26_Docker学习笔记","date":"2017-11-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.951Z","comments":true,"path":"post/docker/2017-11-26_Docker学习笔记.html","link":"","permalink":"https://blog.fenxiangz.com/post/docker/2017-11-26_Docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html","excerpt":"","text":"Google Doc：https://docs.google.com/presentation/d/1mw7KaoBe7mtghJ_-ww3ZhakbQUSSI4FiXRxdOUzu47o/edit?usp=sharing","categories":[{"name":"Docker","slug":"Docker","permalink":"https://blog.fenxiangz.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.fenxiangz.com/tags/Docker/"},{"name":"入门","slug":"入门","permalink":"https://blog.fenxiangz.com/tags/%E5%85%A5%E9%97%A8/"},{"name":"学习笔记","slug":"学习笔记","permalink":"https://blog.fenxiangz.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"IO - 同步，异步，阻塞，非阻塞","slug":"linux/2017-11-26_IO-同步，异步，阻塞，非阻塞","date":"2017-11-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.977Z","comments":true,"path":"post/linux/2017-11-26_IO-同步，异步，阻塞，非阻塞.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-11-26_IO-%E5%90%8C%E6%AD%A5%EF%BC%8C%E5%BC%82%E6%AD%A5%EF%BC%8C%E9%98%BB%E5%A1%9E%EF%BC%8C%E9%9D%9E%E9%98%BB%E5%A1%9E.html","excerpt":"","text":"同步（synchronous） IO和异步（asynchronous） IO，阻塞（blocking） IO和非阻塞（non-blocking）IO分别是什么，到底有什么区别？这个问题其实不同的人给出的答案都可能不同，比如wiki，就认为asynchronous IO和non-blocking IO是一个东西。这其实是因为不同的人的知识背景不同，并且在讨论这个问题的时候上下文(context)也不相同。所以，为了更好的回答这个问题，我先限定一下本文的上下文。 本文讨论的背景是Linux环境下的network IO。 本文最重要的参考文献是Richard Stevens的“UNIX® Network Programming Volume 1, Third Edition: The Sockets Networking ”，6.2节“I/O Models ”，Stevens在这节中详细说明了各种IO的特点和区别，如果英文够好的话，推荐直接阅读。Stevens的文风是有名的深入浅出，所以不用担心看不懂。本文中的流程图也是截取自参考文献。 Stevens在文章中一共比较了五种IO Model： blocking IO nonblocking IO IO multiplexing signal driven IO asynchronous IO 由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model。 再说一下IO发生时涉及的对象和步骤。 对于一个network IO (这里我们以read举例)，它会涉及到两个系统对象，一个是调用这个IO的process (or thread)，另一个就是系统内核(kernel)。当一个read操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready) 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)记住这两点很重要，因为这些IO Model的区别就是在两个阶段上各有不同的情况。 blocking IO在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样： 当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据。 对于network io来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的UDP包），这个时候kernel就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 non-blocking IOlinux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子： 从图中可以看出，当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。 所以，用户进程其实是需要不断的主动询问kernel数据好了没有。 IO multiplexingIO multiplexing这个词可能有点陌生，但是如果我说select，epoll，大概就都能明白了。有些地方也称这种IO方式为event driven IO。我们都知道，select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select/epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。它的流程如图： 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。 这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。（多说一句。所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 Asynchronous I/Olinux下的asynchronous IO其实用得很少。先看一下它的流程： 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 到目前为止，已经将四个IO Model都介绍完了。现在回过头来回答最初的那几个问题：blocking和non-blocking的区别在哪，synchronous IO和asynchronous IO的区别在哪。 先回答最简单的这个：blocking vs non-blocking。前面的介绍中其实已经很明确的说明了这两者的区别。调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。 在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。Stevens给出的定义（其实是POSIX的定义）是这样子的： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked; 两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。有人可能会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。 各个IO Model的比较如图所示： 经过上面的介绍，会发现non-blocking IO和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。 最后，再举几个不是很恰当的例子来说明这四个IO Model: 有A，B，C，D四个人在钓鱼： A用的是最老式的鱼竿，所以呢，得一直守着，等到鱼上钩了再拉杆； B的鱼竿有个功能，能够显示是否有鱼上钩，所以呢，B就和旁边的MM聊天，隔会再看看有没有鱼上钩，有的话就迅速拉杆； C用的鱼竿和B差不多，但他想了一个好办法，就是同时放好几根鱼竿，然后守在旁边，一旦有显示说鱼上钩了，它就将对应的鱼竿拉起来； D是个有钱人，干脆雇了一个人帮他钓鱼，一旦那个人把鱼钓上来了，就给D发个短信。 原文：http://blog.csdn.net/historyasamirror/article/details/5778378","categories":[{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/categories/IO/"}],"tags":[{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"},{"name":"同步","slug":"同步","permalink":"https://blog.fenxiangz.com/tags/%E5%90%8C%E6%AD%A5/"},{"name":"异步","slug":"异步","permalink":"https://blog.fenxiangz.com/tags/%E5%BC%82%E6%AD%A5/"},{"name":"阻塞","slug":"阻塞","permalink":"https://blog.fenxiangz.com/tags/%E9%98%BB%E5%A1%9E/"},{"name":"非阻塞","slug":"非阻塞","permalink":"https://blog.fenxiangz.com/tags/%E9%9D%9E%E9%98%BB%E5%A1%9E/"}]},{"title":"面向报文（UDP）和面向字节流（TCP）的区别","slug":"network/2017-11-24_面向报文（UDP）和面向字节流（TCP）的区别","date":"2017-11-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.980Z","comments":true,"path":"post/network/2017-11-24_面向报文（UDP）和面向字节流（TCP）的区别.html","link":"","permalink":"https://blog.fenxiangz.com/post/network/2017-11-24_%E9%9D%A2%E5%90%91%E6%8A%A5%E6%96%87%EF%BC%88UDP%EF%BC%89%E5%92%8C%E9%9D%A2%E5%90%91%E5%AD%97%E8%8A%82%E6%B5%81%EF%BC%88TCP%EF%BC%89%E7%9A%84%E5%8C%BA%E5%88%AB.html","excerpt":"","text":"面向报文的传输方式是应用层交给 UDP 多长的报文，UDP 就照样发送，即一次发送一个报文。因此，应用程序必须选择合适大小的报文。若报文太长，则 IP 层需要分片，降低效率。若太短，会是 IP 太小。UDP 对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。这也就是说，应用层交给 UDP 多长的报文，UDP 就照样发送，即一次发送一个报文。 面向字节流的话，虽然应用程序和 TCP 的交互是一次一个数据块（大小不等），但 TCP 把应用程序看成是一连串的无结构的字节流。TCP 有一个缓冲，当应用程序传送的数据块太长，TCP 就可以把它划分短一些再传送。如果应用程序一次只发送一个字节，TCP 也可以等待积累有足够多的字节后再构成报文段发送出去。 下图是 TCP 和 UDP 协议的一些应用。 下图是 TCP 和 UDP 协议的比较。 这里再详细说一下面向连接和面向无连接的区别： 面向连接举例：两个人之间通过电话进行通信;面向无连接举例：邮政服务，用户把信函放在邮件中期待邮政处理流程来传递邮政包裹。显然，不可达代表不可靠。从程序实现的角度来看，可以用下图来进行描述。 从上图也能清晰的看出，TCP 通信需要服务器端侦听 listen、接收客户端连接请求 accept，等待客户端 connect 建立连接后才能进行数据包的收发（recv/send）工作。而 UDP 则服务器和客户端的概念不明显，服务器端即接收端需要绑定端口，等待客户端的数据的到来。后续便可以进行数据的收发（recvfrom/sendto）工作。 在前面讲解 UDP 时，提到了 UDP 保留了报文的边界，下面我们来谈谈 TCP 和 UDP 中报文的边界问题。在默认的阻塞模式下，TCP 无边界，UDP 有边界。 对于 TCP 协议，客户端连续发送数据，只要服务端的这个函数的缓冲区足够大，会一次性接收过来，即客户端是分好几次发过来，是有边界的，而服务端却一次性接收过来，所以证明是无边界的；而对于 UDP 协议，客户端连续发送数据，即使服务端的这个函数的缓冲区足够大，也只会一次一次的接收，发送多少次接收多少次，即客户端分几次发送过来，服务端就必须按几次接收，从而证明，这种 UDP 的通讯模式是有边界的。 TCP 无边界，造成对采用 TCP 协议发送的数据进行接收比较麻烦，在接收的时候易出现粘包，即发送方发送的若干包数据到接收方接收时粘成一包。由于 TCP 是流协议，对于一个 socket 的包，如发送 10AAAAABBBBB 两次，由于网络原因第一次又分成两次发送， 10AAAAAB 和 BBBB，如果接包的时候先读取 10(包长度) 再读入后续数据，当接收得快，发送的慢时，就会出现先接收了 10AAAAAB, 会解释错误 , 再接到 BBBB10AAAAABBBBB，也解释错误的情况。这就是 TCP 的粘包。 在网络传输应用中，通常需要在网络协议之上再自定义一个协议封装一下，简单做法就是在要发送的数据前面再加一个自定义的包头，包头中可以包含数据长度和其它一些信息，接收的时候先收包头，再根据包头中描述的数据长度来接收后面的数据。详细做法是：先接收包头，在包头里指定包体长度来接收。设置包头包尾的检查位（ 比如以 0xAA 开头，0xCC 结束来检查一个包是否完整）。对于 TCP 来说： 1）不存在丢包，错包，所以不会出现数据出错 ；2）如果包头检测错误，即为非法或者请求，直接重置即可。 为了避免粘包现象，可采取以下几种措施。 一、对于发送方引起的粘包现象，用户可通过编程设置来避免，TCP 提供了强制数据立即传送的操作指令 push，TCP 软件收到该操作指令后，就立即将本段数据发送出去，而不必等待发送缓冲区满； 二、对于接收方引起的粘包，则可通过优化程序设计、精简接收进程工作量、提高接收进程优先级等措施，使其及时接收数据，从而尽量避免出现粘包现象；三、由接收方控制，将一包数据按结构字段，人为控制分多次接收，然后合并，通过这种手段来避免粘包。 原文：http://blog.csdn.net/ce123/article/details/8976006","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"https://blog.fenxiangz.com/categories/TCP-IP/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"https://blog.fenxiangz.com/tags/TCP/"},{"name":"UDP","slug":"UDP","permalink":"https://blog.fenxiangz.com/tags/UDP/"},{"name":"网络协议","slug":"网络协议","permalink":"https://blog.fenxiangz.com/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}]},{"title":"一张图说明 CDN 网络的原理","slug":"network/2017-11-25_一张图说明CDN网络的原理","date":"2017-11-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.981Z","comments":true,"path":"post/network/2017-11-25_一张图说明CDN网络的原理.html","link":"","permalink":"https://blog.fenxiangz.com/post/network/2017-11-25_%E4%B8%80%E5%BC%A0%E5%9B%BE%E8%AF%B4%E6%98%8ECDN%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86.html","excerpt":"","text":"CDN用户访问流程图 用户向浏览器输入 www.web.com 这个域名，浏览器第一次发现本地没有 dns 缓存，则向网站的 DNS 服务器请求； 网站的 DNS 域名解析器设置了 CNAME，指向了 www.web.51cdn.com, 请求指向了 CDN 网络中的智能 DNS 负载均衡系统； 智能 DNS 负载均衡系统解析域名，把对用户响应速度最快的 IP 节点返回给用户； 用户向该 IP 节点（CDN 服务器）发出请求； 由于是第一次访问，CDN 服务器会向原 web 站点请求，并缓存内容； 请求结果发给用户。 CDN 网络是在用户和服务器之间增加 Cache 层，如何将用户的请求引导到 Cache 上获得源服务器的数据，主要是通过接管 DNS 实现，这就是 CDN 的最基本的原理，当然很多细节没有涉及到，比如第 1 步，首先向本地的 DNS 服务器请求。第 5 步，内容淘汰机制（根据 TTL）等。但原理大体如此。 当用户访问加入 CDN 服务的网站时，域名解析请求将最终交给全局负载均衡 DNS 进行处理。全局负载均衡 DNS 通过一组预先定义好的策略，将当时最接近用 户的节点地址提供给用户，使用户能够得到快速的服务。同时，它还与分布在世界各地的所有 CDNC 节点保持通信，搜集各节点的通信状态，确保不将用户的请求 分配到不可用的 CDN 节点上，实际上是通过 DNS 做全局负载均衡。 对于普通的 Internet 用户来讲，每个 CDN 节点就相当于一个放置在它周围的 WEB。通过全局负载均衡 DNS 的控制，用户的请求被透明地指向离他最近的节点，节点中 CDN 服务器会像网站的原始服务器一样，响应用户的请求。由于它离用户更近，因而响应时间必然更快。 每个 CDN 节点由两部分组成: 负载均衡设备和高速缓存服务器 负载均衡设备负责每个节点中各个 Cache 的负载均衡，保证节点的工作效率; 同时，负载均衡设备还负责收集节点与周围环境的信息，保持与全局负载 DNS 的通信，实现整个系统的负载均衡。CDN 的管理系统是整个系统能够正常运转的保证。它不仅能对系统中的各个子系统和设备进行实时监控，对各种故障产生相应的告警，还可以实时监测到系统中 总的流量和各节点的流量，并保存在系统的数据库中，使网管人员能够方便地进行进一步分析。通过完善的网管系统，用户可以对系统配置进行修改。 理论上，最简单的 CDN 网络有一个负责全局负载均衡的 DNS 和各节点一台 Cache，即可运行。DNS 支持根据用户源 IP 地址解析不同的 IP，实现 就近访问。为了保证高可用性等，需要监视各节点的流量、健康状况等。一个节点的单台 Cache 承载数量不够时，才需要多台 Cache，多台 Cache 同时 工作，才需要负载均衡器，使 Cache 群协同工作。","categories":[{"name":"网络/其他","slug":"网络-其他","permalink":"https://blog.fenxiangz.com/categories/%E7%BD%91%E7%BB%9C-%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"CDN","slug":"CDN","permalink":"https://blog.fenxiangz.com/tags/CDN/"}]},{"title":"深入剖析 Socket 实现","slug":"network/2017-11-26_深入剖析Socket实现","date":"2017-11-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.981Z","comments":true,"path":"post/network/2017-11-26_深入剖析Socket实现.html","link":"","permalink":"https://blog.fenxiangz.com/post/network/2017-11-26_%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90Socket%E5%AE%9E%E7%8E%B0.html","excerpt":"","text":"在我们平时的开发中用到的最多的是 HTTP 协议，而 HTTP 协议本身是一种应用层协议，属于文本协议；并且这种协议也基本上满足了应用的大部分需求。HTTP 协议当初的设计并没有想到它应用的是如此的广泛，所以设计的时候考虑的比较简单实用，也许也就是这种简单实用才这么广泛；但如今，HTTP 协议似乎并不能满足所有的需求，特别是当今的 web2.0 时代，浏览器应用横行的年代，也越来越多需要长连接的应用，所以在 HTML5 以及 Flash 等客户端应用中都加入了长连接的定义，并且我也相信在未来的互联网开发中会出现很多的长连接应用。在我们公司也曾经自己开发过长连接的应用，前端是基于 flash 的，后端是基于 Java 的实现，自己基于 TCP/IP 协议制定了一套稳定，安全，可靠的应用层协议，至今一直在线上运行，情况也比较稳定；在此，我想基于我的知识和对于 socket 的理解在这里做一次分享，也许不是很深入和透彻，但绝对很基础。 其实如果不理解套接字的具体实现所关联的数据结构和底层协议的工作细节，就很难抓住网络编程的精妙之处，对于 TCP 套接字（即 Socket 的实例）来说更是如此。这里我就对创建和使用 Socket 和 ServerSocket 实例的底层细节进行介绍。请注意，这些内容仅仅涵盖了一些普通的事件实例，略去了很多细节。尽管如此，我相信即使是这样的基础的理解也是有用的。如果希望了解更详尽的内容，可以参考 TCP 规范，或关于该方面的其他著作（例如 TCP/IP 详解）。 图 1 是一个 Socket 实例所关联的一些信息的简化视图。JVM 或其运行的平台（即，主机操作系统中的 “套接字层”）为这些类的支持提供了底层实现。Java 对象上的操作则转换成了这种底层抽象上的操作。在这里，“Socket” 指的是图 1 中的类之一，而 “套接字（socket）” 指的是底层抽象，这种抽象是有操作系统提供或由 JVM 自己实现（例如在嵌入式系统中）。有一点需要注意，即运行在统一主机上的其他程序可能也会通过底层套接字抽象来使用网络，因此会与 Java Socket 实例竞争系统资源，如端口等。 在此，“套接字结构” 是指底层实现（包括 JVM 和 TCP/IP，但通常是后者）的数据结构集，这些数据结构包括了特定 Socket 实例所关联的信息。例如，套接字结构除其他信息外还包括： 该套接字所关联的本地和远程互联网地址和端口号。本地互联网地址（图中标记为 “Local IP”）是赋值给本地主机的；本地端口号在 Socket 实例创建时设置的。远程地址和端口号标记了与本地套接字连接的远程套接字（如果没有连接的话）。不久，我们将对这些值确定的时间和方式做进一步介绍。 一个 FIFO（先进先出，First In First Out）队列用于存放接收到的等待分配的数据，以及一个用于存放等待传输的数据的队列。 列表对于 TCP 套接字，还包括了与打开和关闭 TCP 握手相关的额外协议状态信息。图 1 中，状态是 “关闭”；所有套接字的起始状态都是关闭的。 一些多用途操作系统为用户提供了获取底层数据结构 “快照” 的工具，netstat 是其中之一，它在 UNIX（Linux）和 Windows 平台上都可用。只要给定适当的选项，netstat 就能显示和图 1 的那些信息：SendQ 和 RecvQ 中的字节数，本地和远程 IP 地址和端口号，以及连接状态等。netstat 的命令行选项有多种，但它输出看起来是这样的： 12345678910Active Internet connections(server and established)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 0.0.0.0:36045 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:111 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:53363 0.0.0.0:* LISTENtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTENtcp 0 0 128.133.190.219:34077 4.71.104.187:80 TIME_WAITtcp 0 0 128.133.190.219:43346 79.62.132.8:22 ESTABLISHEDtcp 0 0 128.133.190.219:875 128.133.190.43:2409 ESTABLISHEDtcp6 0 0 :::22 :::* LISTEN 前 4 行和最后一行描述了正在侦听连接的服务器套接字。 第 5 行代表了到一个 Web 服务器（80 端口）的连接，该服务器已经单方面关闭。 倒数第 2 行是现有的 TCP 连接。 如果系统支持的话，你可能想要尝试一下 netstat，来检测下上文描述的场景的连接状态。然而要知道，这些图中描述的状态转换过程转瞬即逝，可能很难通过 netstat 提供的 “快照” 功能将其捕获。 了解这些数据结构，以及底层协议如何对其进行影响是非常有用的，因为它们控制了各种 Socket 对象行为的各个方面。例如，由于 TCP 提供了一种可信赖的字节流服务，任何写入 Socket 的 OutputStream 的数据副本都必须保留，直到其在连接的另一端被成功接收。向输出流写数据并不意味着数据实际上已经被发送，他们只是被复制到了本地缓冲区。就算在 Socket 的 OutputStream 上进行 flush 操作，也不能保证数据能够立即发送到信道。此外，字节流服务的自身属性决定了其无法保留输入流中消息的边界信息，这里的边界信息的意思就是上一个数据包和下一个数据包之间的区别信息。这使一些协议的接收和解析过程变得复杂。另一方面，对于 DatagramSocket，数据包并没有为重传而进行缓存，任何时候调用 send() 方法返回后，数据就已经发送给了执行传输任务的网络子系统。如果网络子系统由于某种原因无法处理这些消息，该数据包将毫无提示地被丢弃（不过这种情况很少发生）。 缓冲区和 TCP 作为程序员，在使用 TCP 套接字时需要记住的最重要一点是： 不能假设在连接的一端将数据写入输出流和在另一端从输入流读取数据之间有任何一致性。 尤其是在发送端由单个输出流的 write() 方法传输的数据，可能会通过另一端的多个输入流的 read() 方法来获取；而一个 read() 方法可能会返回多个 write() 方法传输的数据。 为了展示这种情况，考虑如下程序： 1234567891011121314byte[] buf0 &#x3D; new byte[1000];byte[] buf1 &#x3D; new byte[2000];byte[] buf2 &#x3D; new byte[5000];…Socket s &#x3D; new Socket(destAddr, destPort);OutputStream out &#x3D; s.getOutputStream();…out.write(buf0);…out.write(buf1);…out.write(buf2);…s.close(); 其中，圆点代表了设置缓冲区数据的代码，但不包括对 out.write() 方法的调用。在本节的讨论中，“in” 代表接收端 Socket 的 InputStream，“out” 代表发送端 Socket 的 OutputStream。 这个 TCP 连接想接收端传输 8000 字节。在连接的接收端，这 8000 字节的分组方式取决于连接两端 out.write() 方法和 in.read() 方法的调用时间差，以及提供给 in.read() 方法的缓冲区大小。 我们可以认为 TCP 连接上发送的所有字节序列在某一瞬间被分成了 3 个 FIFO 队列； 列表 SendQ：在发送端底层实现中缓存的字节，这些字节已经写入了输出流，但还没在接收端主机上成功接收。 列表 RecvQ：在接收端底层实现中缓存的字节，等待分配到接收程序，即从输入流中读取。 列表 Delivered：接收者从输入流已经读取到的字节。 调用 out.write() 方法将向 SendQ 追加字节。TCP 协议负责将字节按顺序从 SendQ 移动到 RecvQ。有重要的一点需要明确，这个转移过程无法由用户程序控制或直接观察到，并且在块中（chunk）发生，这些块的大小在一定程度上独立于传递给 write() 方法的缓冲区大小。 接收程序从 Socket 的 InputStream 读取数据时，字节就从 RecvQ 移动到 Delivered 中，而转移的块的大小依赖于 RecvQ 中的数据量和传递给 read() 方法缓冲区大小。 -&gt;图 2 3 次调用 write() 方法后 3 个队列的状态&lt;- 图 2 展示了上例中 3 次调用 out.write() 方法后，另一端调用 in.read() 方法前，以上 3 个队列的可能状态。不同的阴影效果分别代表了上文中 3 次调用 write() 方法传输的不同数据。 图 2 描述的发送端主机的 netstat 输出的瞬间状态中，会包含类似于下一行的内容： 在接收端主机，netstat 会显示： 现在假设接收者调用 read() 方法时使用的缓冲区数组大小为 2000 字节，read() 调用则将把等待分配队列 (RecvQ) 中的 1500 字节全部移动到数组中，返回值为 1500。注意，这些数据包括了第一次和第二次调用 write() 方法时传输的字节。在过一段时间，但 TCP 连接传完更多数据后，这三部分的状态可能如图 3 所示。 -&gt;图 3 第一次调用 read() 方法后&lt;- 如果接收者现在调用 read() 方法时使用 4000 字节的缓冲区数组，将有很多字节从等待分配队列（RecvQ）转移到已分配队列（Delivered）中。这包括第二次调用 write() 方法时剩下的 1500 字节加上第三次调用 write() 的前 2500 字节。此时队列的状态如图 4 所示。 -&gt;图 4 另一次调用 read() 后&lt;- 下次调用 read() 方法返回的字节数，取决于缓冲区数组的大小，以及发送方套接字 /TCP 实现通过网络向接收方实现传输数据的时机。数据从 SendQ 到 RecvQ 缓冲区的移动过程对应用程序协议的设计有重要的指导性。我们已经遇到过需要对使用带内（in-band）分隔符，并通过 Socket 来接收的消息进行解析的情况。 source: http://blog.csdn.net/zapldy/article/details/5813984","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"https://blog.fenxiangz.com/categories/TCP-IP/"}],"tags":[{"name":"Socket","slug":"Socket","permalink":"https://blog.fenxiangz.com/tags/Socket/"}]},{"title":"搬瓦工 VPN 安装脚本","slug":"network/2017-11-27_搬瓦工VPN安装脚本","date":"2017-11-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.981Z","comments":true,"path":"post/network/2017-11-27_搬瓦工VPN安装脚本.html","link":"","permalink":"https://blog.fenxiangz.com/post/network/2017-11-27_%E6%90%AC%E7%93%A6%E5%B7%A5VPN%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC.html","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#!&#x2F;bin&#x2F;bashfunction installVPN()&#123; echo &quot;begin to install VPN services&quot;; #check wether vps suppot ppp and tun yum remove -y pptpd ppp iptables --flush POSTROUTING --table nat iptables --flush FORWARD rm -rf &#x2F;etc&#x2F;pptpd.conf rm -rf &#x2F;etc&#x2F;ppp arch&#x3D;&#96;uname -m&#96; wget http:&#x2F;&#x2F;www.hi-vps.com&#x2F;downloads&#x2F;dkms-2.0.17.5-1.noarch.rpm wget http:&#x2F;&#x2F;wty.name&#x2F;linux&#x2F;sources&#x2F;kernel_ppp_mppe-1.0.2-3dkms.noarch.rpm wget http:&#x2F;&#x2F;www.hi-vps.com&#x2F;downloads&#x2F;kernel_ppp_mppe-1.0.2-3dkms.noarch.rpm wget http:&#x2F;&#x2F;www.hi-vps.com&#x2F;downloads&#x2F;pptpd-1.3.4-2.el6.$arch.rpm wget http:&#x2F;&#x2F;www.hi-vps.com&#x2F;downloads&#x2F;ppp-2.4.5-17.0.rhel6.$arch.rpm yum -y install make libpcap iptables gcc-c++ logrotate tar cpio perl pam tcp_wrappers rpm -ivh dkms-2.0.17.5-1.noarch.rpm rpm -ivh kernel_ppp_mppe-1.0.2-3dkms.noarch.rpm rpm -qa kernel_ppp_mppe rpm -Uvh ppp-2.4.5-17.0.rhel6.$arch.rpm rpm -ivh pptpd-1.3.4-2.el6.$arch.rpm mknod &#x2F;dev&#x2F;ppp c 108 0 echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_forward echo &quot;mknod &#x2F;dev&#x2F;ppp c 108 0&quot; &gt;&gt; &#x2F;etc&#x2F;rc.local echo &quot;echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_forward&quot; &gt;&gt; &#x2F;etc&#x2F;rc.local echo &quot;localip 172.16.36.1&quot; &gt;&gt; &#x2F;etc&#x2F;pptpd.conf echo &quot;remoteip 172.16.36.2-254&quot; &gt;&gt; &#x2F;etc&#x2F;pptpd.conf echo &quot;ms-dns 8.8.8.8&quot; &gt;&gt; &#x2F;etc&#x2F;ppp&#x2F;options.pptpd echo &quot;ms-dns 8.8.4.4&quot; &gt;&gt; &#x2F;etc&#x2F;ppp&#x2F;options.pptpd pass&#x3D;&#96;openssl rand 6 -base64&#96; if [ &quot;$1&quot; !&#x3D; &quot;&quot; ] then pass&#x3D;$1 fi echo &quot;vpn pptpd $&#123;pass&#125; *&quot; &gt;&gt; &#x2F;etc&#x2F;ppp&#x2F;chap-secrets iptables -t nat -A POSTROUTING -s 172.16.36.0&#x2F;24 -j SNAT --to-source &#96;ifconfig | grep &#39;inet addr:&#39;| grep -v &#39;127.0.0.1&#39; | cut -d: -f2 | awk &#39;NR&#x3D;&#x3D;1 &#123; print $1&#125;&#39;&#96; iptables -A FORWARD -p tcp --syn -s 172.16.36.0&#x2F;24 -j TCPMSS --set-mss 1356 service iptables save chkconfig iptables on chkconfig pptpd on service iptables start service pptpd start echo &quot;VPN service is installed, your VPN username is vpn, VPN password is $&#123;pass&#125;&quot;&#125;function repaireVPN()&#123; echo &quot;begin to repaire VPN&quot;; mknod &#x2F;dev&#x2F;ppp c 108 0 service iptables restart service pptpd start&#125;function addVPNuser()&#123; echo &quot;input user name:&quot; read username echo &quot;input password:&quot; read userpassword echo &quot;$&#123;username&#125; pptpd $&#123;userpassword&#125; *&quot; &gt;&gt; &#x2F;etc&#x2F;ppp&#x2F;chap-secrets service iptables restart service pptpd start&#125;echo &quot;which do you want to?input the number.&quot;echo &quot;1. install VPN service&quot;echo &quot;2. repaire VPN service&quot;echo &quot;3. add VPN user&quot;read numcase &quot;$num&quot; in[1] ) (installVPN);;[2] ) (repaireVPN);;[3] ) (addVPNuser);;*) echo &quot;nothing,exit&quot;;;esac","categories":[{"name":"网络/其他","slug":"网络-其他","permalink":"https://blog.fenxiangz.com/categories/%E7%BD%91%E7%BB%9C-%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"vpn","slug":"vpn","permalink":"https://blog.fenxiangz.com/tags/vpn/"}]},{"title":"Heidi SQL 通过 SSH 隧道链接 MySQL","slug":"network/2017-11-28_HeidiSQL通过SSH隧道链接mysql","date":"2017-11-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.981Z","comments":true,"path":"post/network/2017-11-28_HeidiSQL通过SSH隧道链接mysql.html","link":"","permalink":"https://blog.fenxiangz.com/post/network/2017-11-28_HeidiSQL%E9%80%9A%E8%BF%87SSH%E9%9A%A7%E9%81%93%E9%93%BE%E6%8E%A5mysql.html","excerpt":"","text":"Heidi SQL 通过 SSH 隧道链接mysql，适用于 mysql 数据库本地机器不可达，只能通过 SSH 隧道的情况。 步骤： 1. 选： 2. 下载 plink，选择安装路径： 3. 正常配置数据库链接信息即可","categories":[{"name":"网络/其他","slug":"网络-其他","permalink":"https://blog.fenxiangz.com/categories/%E7%BD%91%E7%BB%9C-%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"ssh","slug":"ssh","permalink":"https://blog.fenxiangz.com/tags/ssh/"},{"name":"Heidi","slug":"Heidi","permalink":"https://blog.fenxiangz.com/tags/Heidi/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/tags/MySQL/"}]},{"title":"Docker Get Started","slug":"docker/2017-11-25_Docker_Get_Started","date":"2017-11-25T00:00:00.000Z","updated":"2020-12-20T16:47:02.951Z","comments":true,"path":"post/docker/2017-11-25_Docker_Get_Started.html","link":"","permalink":"https://blog.fenxiangz.com/post/docker/2017-11-25_Docker_Get_Started.html","excerpt":"","text":"环境 Vmware11 安装 Centos 7 系统，版本如下。 12$ uname -s -r Linux 3.10.0-229.el7.x86_64 安装 1.使用 root 账户或具有 root 权限的用户登录。 2.更新安装包。 1sudo yum update 3.给 yum 添加 docker repository。 12345678$ sudo tee &#x2F;etc&#x2F;yum.repos.d&#x2F;docker.repo &lt;&lt;-&#39;EOF&#39;[dockerrepo]name&#x3D;Docker Repositorybaseurl&#x3D;https:&#x2F;&#x2F;yum.dockerproject.org&#x2F;repo&#x2F;main&#x2F;centos&#x2F;$releasever&#x2F;enabled&#x3D;1gpgcheck&#x3D;1gpgkey&#x3D;https:&#x2F;&#x2F;yum.dockerproject.org&#x2F;gpgEOF 4.安装 docker 引擎 1$ sudo yum install docker-engine 启动 docker 守护进程1sudo service docker start 验证 docker 安装和启动是否正确12345678910111213141516171819202122$ sudo docker run hello-worldUnable to find image &#39;hello-world:latest&#39; locally latest: Pulling from hello-world a8219747be10: Pull complete 91c95931e552: Already exists hello-world:latest: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security. Digest: sha256:aa03e5d0d5553b4c3473e89c8619cf79df368babd1.7.1cf5daeb82aab55838d Status: Downloaded newer image for hello-world:latest Hello from Docker. This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (Assuming it was not already locally available.) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash For more examples and ideas, visit: http:&#x2F;&#x2F;docs.docker.com&#x2F;userguide&#x2F; ok，看见以上信息，就说明 docker 已经正确安装并启动。 在系统里创建一个 docker 用户组 从 0.5.2 开始 docker 的守护进程总是以 root 用户来运行。docker 守护进程绑定的是 Unix 的 socket 而不是一个 TCP 端口。Unix 的 socket 默认属于 root 用户，所以，使用 docker 时必须加上 sudo。 从 0.5.3 开始，创建一个名为 docker 组，然后将用户加入这个组内。当 docker 守护进程启动时，它会把 Unix 的读写权限赋予 docker 组。这样，当你作为 docker 组内用户使用 docker 客户端时，你就无须使用 sudo 了。 需要注意的是，docker 用户组是等价于 root 用户的，进一步了 docker 用户组对系统安全的影响，可以看看这个：走你。 执行: 1sudo usermod -aG docker your_username 执行完后，退出 shell，再重新登录，以确保用户获得正确的运行权限。 重新登录系统后，执行： 1docker run hello-world 执行结果应该同 “安装” 部分第六步的执行结果一样。 设置 docker 守护进程开机启动 1$ sudo chkconfig docker on 运行一下官方教程提供的镜像 执行： 1234567891011121314151617181920212223242526272829$ docker run docker&#x2F;whalesay cowsay booUnable to find image &#39;docker&#x2F;whalesay:latest&#39; locallylatest: Pulling from docker&#x2F;whalesaye9e06b06e14c: Pull completea82efea989f9: Pull complete37bea4ee0c81: Pull complete07f8e8c5e660: Pull complete676c4a1897e6: Pull complete5b74edbcaa5b: Pull complete1722f41ddcb5: Pull complete99da72cfe067: Pull complete5d5bd9951e26: Pull completefb434121fc77: Already existsDigest: sha256:d6ee73f978a366cf97974115abe9c4099ed59c6f75c23d03c64446bb9cd49163Status: Downloaded newer image for docker&#x2F;whalesay:latest _____&lt; boo &gt; ----- \\ \\ \\ ## . ## ## ## &#x3D;&#x3D; ## ## ## ## &#x3D;&#x3D;&#x3D; &#x2F;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;___&#x2F; &#x3D;&#x3D;&#x3D; ~~~ &#123;~~ ~~~~ ~~~ ~~~~ ~~ ~ &#x2F; &#x3D;&#x3D;&#x3D;- ~~~ \\______ o __&#x2F; \\ \\ __&#x2F; \\____\\______&#x2F; 第一次执行的时候，会从 Docker Hub 上下载该镜像。镜像还挺大的，有 247MB，如果网速不快的，需要耐心等待。 Docker 的镜像（images）和容器（containers） 首先，看一下下面这个图例: 执行以上命令，docker 引擎会做这几件事： 检查本地是否有 hello-world 镜像 如果没有则从 Docker Hub 下载该镜像，如果有，则使用本地镜像 把 hello-world 镜像加载到容器里，并执行。 Docker 镜像可以是一个简单的命令行，执行完后就退出了。也可以是一个复杂的程序集合，比如一个数据库，启动后，可以进行复杂的数据库操作。 Docker 的镜像是只读的，可以理解为是一个软件。Docker 镜像通过镜像 ID 进行识别。镜像 ID 是一个 64 字符的十六进制的字符串。通常我们不会使用镜像 ID 来引用镜像，而是使用镜像名来引用。 查看本地镜像： 12345$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker-whale latest 03643993f0ca 7 minutes ago 255.6 MBhello-world latest 690ed74de00f 4 months ago 960 Bdocker&#x2F;whalesay latest 6b362a9f73eb 8 months ago 247 MB 这里的 Image ID 只有前 12 个字符，想要显示全部，需要添加选项：–no-trunc。 12345$ docker images --no-truncREPOSITORY TAG IMAGE ID CREATED SIZEdocker-whale latest sha256:03643993f0cacd07968d99265815543a0f2d296e4fc434253b2b4ac2e2c84aae About an hour ago 255.6 MBhello-world latest sha256:690ed74de00f99a7d00a98a5ad855ac4febd66412be132438f9b8dbd300a937d 4 months ago 960 Bdocker&#x2F;whalesay latest sha256:6b362a9f73eb8c33b48c95f4fcce1b6637fc25646728cf7fb0679b2da273c3f4 8 months ago 247 MB 容器就是运行镜像的地方。当执行 docker run 的时候，就会启动一个容器，而 docker run hello-world 便会把 hello-world 这个程序加载到这个容器里面运行。每个容器相互独立，我们可以使用同一个镜像启动多个容器（多个虚拟环境），我们对其中一个容器所做的变更只会局限于那个容器本身，对容器的变更是写入到容器的文件系统的，而不是写入到 Docker 镜像中的。 容器之间可以通过暴露端口进行通信，这个在以后学习中再说。 Docker 使用 64 字符的十六进制的字符串来定义容器 ID，它是容器的唯一标识符。容器之间的交互是依靠容器 ID 识别的，由于容器 ID 的字符太长，我们通常只需键入容器 ID 的前 4 个字符即可。当然，我们还可以使用容器名，但显然用 4 字符的容器 ID 更为简便。 查看当前运行的容器可以使用命令： 1234$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESa45a2d3b95e7 sample-spring-boot-app &quot;&#x2F;bin&#x2F;sh -c &#39;java -DC&quot; 3 seconds ago Up 2 seconds 0.0.0.0:8080-&gt;8080&#x2F;tcp silly_morse0b2494881e6c sample-spring-boot-app &quot;&#x2F;bin&#x2F;sh -c &#39;java -DC&quot; 3 minutes ago Up 3 minutes 0.0.0.0:32771-&gt;8080&#x2F;tcp compassionate_austin 简单的理解，可以把 “镜像 - 容器” 理解为 “类 - 实例”。容器是一个实例，这个实例是根据镜像创建的。","categories":[{"name":"Docker","slug":"Docker","permalink":"https://blog.fenxiangz.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.fenxiangz.com/tags/Docker/"},{"name":"入门","slug":"入门","permalink":"https://blog.fenxiangz.com/tags/%E5%85%A5%E9%97%A8/"}]},{"title":"update-alternatives 命令","slug":"linux/2017-11-27_update-alternatives_命令","date":"2017-11-25T00:00:00.000Z","updated":"2020-12-20T16:47:02.977Z","comments":true,"path":"post/linux/2017-11-27_update-alternatives_命令.html","link":"","permalink":"https://blog.fenxiangz.com/post/linux/2017-11-27_update-alternatives_%E5%91%BD%E4%BB%A4.html","excerpt":"","text":"update-alternatives 是符号链接管理工具。用于分组管理命令的链接和优先级。 update-alternatives 以链接组进行管理，每一个链接组（link group）都有两种不同的模式：自动模式和手动模式，任一给定时刻一个组都是而且只能是其中的一种模式。 如果一个组处于自动模式，当包被安装或删除时，备选方案系统会自己决定是否和如何来更新相应链接（links）。 如果处于手动模式，备选方案系统会保留原先管理员所做的选择并且避免改变链接（除非发生 broken）。 当第一次被安装到系统时链接组被分配为自动模式；如果之后系统管理员对模式的设置做出更改，这个组会被自动转换为手动模式。 –display name 显示链接组的信息。信息包括链接组的模式（自动或手动）；链接的指针（链到了那一个文件）；优先级是多少；当前最优版本等。 –install link name path priority [–slave slink sname spath] … 其中 link 为系统中功能相同软件的公共链接目录，比如 / usr/bin/java(需绝对目录)； name 为命令链接符名称，如 java； path 为你所要使用新命令、新软件的所在目录； priority 为优先级，当命令链接已存在时，需高于当前值，因为当 alternative 为自动模式时, 系统默认启用 priority 高的链接; –slave 为从 alternative。 例如： 1sudo update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;java java &#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;jdk8&#x2F;bin&#x2F;java 300 install 默认都为 auto 模式，因为大多数情况下 update-alternatives 命令都被 postinst (configure) or prerm (install) 调用的，如果将其更改成手动的话安装脚本将不会更新它了。 –config name 当使用 –config 选项时，update-alternatives 会列出所有链接组的主链接名，当前被选择的组会以 * 号标出。可以在提示下对链接指向做出改变，不过这会将模式变为手动。如果想恢复自动模式，你可以使用 –auto 选项，或者 –config 重新选择标为自动的组。 例如： 1234567891011$ sudo update-alternatives --config editorThere are 4 choices for the alternative editor (providing &#x2F;usr&#x2F;bin&#x2F;editor). Selection Path Priority Status------------------------------------------------------------* 0 &#x2F;bin&#x2F;nano 40 auto mode 1 &#x2F;bin&#x2F;ed -100 manual mode 2 &#x2F;bin&#x2F;nano 40 manual mode 3 &#x2F;usr&#x2F;bin&#x2F;vim.basic 30 manual mode 4 &#x2F;usr&#x2F;bin&#x2F;vim.tiny 10 manual modePress enter to keep the current choice[*], or type selection number:--auto name 重新使 name 链接组为自动模式。 –remove name path 删除 name 链接组里的 path 对应的符号链接","categories":[{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"符号链接","slug":"符号链接","permalink":"https://blog.fenxiangz.com/tags/%E7%AC%A6%E5%8F%B7%E9%93%BE%E6%8E%A5/"}]},{"title":"Java8 lambda表达式10个示例","slug":"java/basic/2017-11-10_java-lambda","date":"2017-11-10T15:51:24.000Z","updated":"2020-12-20T16:47:02.964Z","comments":true,"path":"post/java/basic/2017-11-10_java-lambda.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/basic/2017-11-10_java-lambda.html","excerpt":"","text":"Java 8 刚于几周前发布，日期是2014年3月18日，这次开创性的发布在Java社区引发了不少讨论，并让大家感到激动。特性之一便是随同发布的lambda表达式，它将允许我们将行为传到函数里。在Java 8之前，如果想将行为传入函数，仅有的选择就是匿名类，需要6行代码。而定义行为最重要的那行代码，却混在中间不够突出。Lambda表达式取代了匿名类，取消了模板，允许用函数式风格编写代码。这样有时可读性更好，表达更清晰。在Java生态系统中，函数式表达与对面向对象的全面支持是个激动人心的进步。将进一步促进并行第三方库的发展，充分利用多核CPU。尽管业界需要时间来消化Java 8，但我认为任何严谨的Java开发者都不应忽视此次Java发布的核心特性，即lambda表达式、函数式接口、流API、默认方法和新的Date以及Time API。作为开发人员，我发现学习和掌握lambda表达式的最佳方法就是勇于尝试，尽可能多练习lambda表达式例子。鉴于受Java 8发布的影响最大的是Java集合框架（Java Collections framework），所以最好练习流API和lambda表达式，用于对列表（Lists）和集合（Collections）数据进行提取、过滤和排序。我一直在进行关于Java 8的写作，过去也曾分享过一些资源来帮助大家掌握Java 8。本文分享在代码中最有用的10个lambda表达式的使用方法，这些例子都短小精悍，将帮助你快速学会lambda表达式。 Java 8 lambda表达式示例我个人对Java 8发布非常激动，尤其是lambda表达式和流API。越来越多的了解它们，我能写出更干净的代码。虽然一开始并不是这样。第一次看到用lambda表达式写出来的Java代码时，我对这种神秘的语法感到非常失望，认为它们把Java搞得不可读，但我错了。花了一天时间做了一些lambda表达式和流API示例的练习后，我开心的看到了更清晰的Java代码。这有点像学习泛型，第一次见的时候我很讨厌它。我甚至继续使用老版Java 1.4来处理集合，直到有一天，朋友跟我介绍了使用泛型的好处（才意识到它的好处）。所以基本立场就是，不要畏惧lambda表达式以及方法引用的神秘语法，做几次练习，从集合类中提取、过滤数据之后，你就会喜欢上它。下面让我们开启学习Java 8 lambda表达式的学习之旅吧，首先从简单例子开始。 例1、用lambda表达式实现Runnable我开始使用Java 8时，首先做的就是使用lambda表达式替换匿名类，而实现Runnable接口是匿名类的最好示例。看一下Java 8之前的runnable实现方法，需要4行代码，而使用lambda表达式只需要一行代码。我们在这里做了什么呢？那就是用() -&gt; {}代码块替代了整个匿名类。 // Java 8之前： new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;Before Java8, too much code for too little to do&quot;); &#125; &#125;).start(); //Java 8方式： new Thread( () -&gt; System.out.println(&quot;In Java8, Lambda expression rocks !!&quot;) ).start(); 输出： too much code, for too little to do Lambda expression rocks !! 这个例子向我们展示了Java 8 lambda表达式的语法。你可以使用lambda写出如下代码： (params) -&gt; expression (params) -&gt; statement (params) -&gt; &#123; statements &#125; 例如，如果你的方法不对参数进行修改、重写，只是在控制台打印点东西的话，那么可以这样写： () -&gt; System.out.println(&quot;Hello Lambda Expressions&quot;); 如果你的方法接收两个参数，那么可以写成如下这样： (int even, int odd) -&gt; even + odd 顺便提一句，通常都会把lambda表达式内部变量的名字起得短一些。这样能使代码更简短，放在同一行。所以，在上述代码中，变量名选用a、b或者x、y会比even、odd要好。 例2、使用Java 8 lambda表达式进行事件处理如果你用过Swing API编程，你就会记得怎样写事件监听代码。这又是一个旧版本简单匿名类的经典用例，但现在可以不这样了。你可以用lambda表达式写出更好的事件监听代码，如下所示： // Java 8之前： JButton show = new JButton(&quot;Show&quot;); show.addActionListener(new ActionListener() &#123; @Override public void actionPerformed(ActionEvent e) &#123; System.out.println(&quot;Event handling without lambda expression is boring&quot;); &#125; &#125;); // Java 8方式： show.addActionListener((e) -&gt; &#123; System.out.println(&quot;Light, Camera, Action !! Lambda expressions Rocks&quot;); &#125;); Java开发者经常使用匿名类的另一个地方是为 Collections.sort() 定制 Comparator。在Java 8中，你可以用更可读的lambda表达式换掉丑陋的匿名类。我把这个留做练习，应该不难，可以按照我在使用lambda表达式实现 Runnable 和 ActionListener 的过程中的套路来做。 例3、使用lambda表达式对列表进行迭代如果你使过几年Java，你就知道针对集合类，最常见的操作就是进行迭代，并将业务逻辑应用于各个元素，例如处理订单、交易和事件的列表。由于Java是命令式语言，Java 8之前的所有循环代码都是顺序的，即可以对其元素进行并行化处理。如果你想做并行过滤，就需要自己写代码，这并不是那么容易。通过引入lambda表达式和默认方法，将做什么和怎么做的问题分开了，这意味着Java集合现在知道怎样做迭代，并可以在API层面对集合元素进行并行处理。下面的例子里，我将介绍如何在使用lambda或不使用lambda表达式的情况下迭代列表。你可以看到列表现在有了一个 forEach() 方法，它可以迭代所有对象，并将你的lambda代码应用在其中。 // Java 8之前： List features = Arrays.asList(&quot;Lambdas&quot;, &quot;Default Method&quot;, &quot;Stream API&quot;, &quot;Date and Time API&quot;); for (String feature : features) &#123; System.out.println(feature); &#125; // Java 8之后： List features = Arrays.asList(&quot;Lambdas&quot;, &quot;Default Method&quot;, &quot;Stream API&quot;, &quot;Date and Time API&quot;); features.forEach(n -&gt; System.out.println(n)); // 使用Java 8的方法引用更方便，方法引用由::双冒号操作符标示， // 看起来像C++的作用域解析运算符 features.forEach(System.out::println); 输出： Lambdas Default Method Stream API Date and Time API 列表循环的最后一个例子展示了如何在Java 8中使用方法引用（method reference）。你可以看到C++里面的双冒号、范围解析操作符现在在Java 8中用来表示方法引用。 例4、使用lambda表达式和函数式接口Predicate除了在语言层面支持函数式编程风格，Java 8也添加了一个包，叫做 java.util.function。它包含了很多类，用来支持Java的函数式编程。其中一个便是Predicate，使用 java.util.function.Predicate 函数式接口以及lambda表达式，可以向API方法添加逻辑，用更少的代码支持更多的动态行为。下面是Java 8 Predicate 的例子，展示了过滤集合数据的多种常用方法。Predicate接口非常适用于做过滤。 public static void main(args[])&#123; List languages = Arrays.asList(&quot;Java&quot;, &quot;Scala&quot;, &quot;C++&quot;, &quot;Haskell&quot;, &quot;Lisp&quot;); System.out.println(&quot;Languages which starts with J :&quot;); filter(languages, (str)-&gt;str.startsWith(&quot;J&quot;)); System.out.println(&quot;Languages which ends with a &quot;); filter(languages, (str)-&gt;str.endsWith(&quot;a&quot;)); System.out.println(&quot;Print all languages :&quot;); filter(languages, (str)-&gt;true); System.out.println(&quot;Print no language : &quot;); filter(languages, (str)-&gt;false); System.out.println(&quot;Print language whose length greater than 4:&quot;); filter(languages, (str)-&gt;str.length() &gt; 4); &#125; public static void filter(List names, Predicate condition) &#123; for(String name: names) &#123; if(condition.test(name)) &#123; System.out.println(name + &quot; &quot;); &#125; &#125; &#125; 输出： Languages which starts with J : Java Languages which ends with a Java Scala Print all languages : Java Scala C++ Haskell Lisp Print no language : Print language whose length greater than 4: Scala Haskell 更好的办法 public static void filter(List names, Predicate condition) &#123; names.stream().filter((name) -&gt; (condition.test(name))).forEach((name) -&gt; &#123; System.out.println(name + &quot; &quot;); &#125;); &#125; 可以看到，Stream API的过滤方法也接受一个Predicate，这意味着可以将我们定制的 filter() 方法替换成写在里面的内联代码，这就是lambda表达式的魔力。另外，Predicate接口也允许进行多重条件的测试，下个例子将要讲到。 例5、如何在lambda表达式中加入Predicate上个例子说到，java.util.function.Predicate 允许将两个或更多的 Predicate 合成一个。它提供类似于逻辑操作符AND和OR的方法，名字叫做and()、or()和xor()，用于将传入 filter() 方法的条件合并起来。例如，要得到所有以J开始，长度为四个字母的语言，可以定义两个独立的 Predicate 示例分别表示每一个条件，然后用 Predicate.and() 方法将它们合并起来，如下所示： // 甚至可以用and()、or()和xor()逻辑函数来合并Predicate， // 例如要找到所有以J开始，长度为四个字母的名字，你可以合并两个Predicate并传入 Predicate&lt;String&gt; startsWithJ = (n) -&gt; n.startsWith(&quot;J&quot;); Predicate&lt;String&gt; fourLetterLong = (n) -&gt; n.length() == 4; names.stream() .filter(startsWithJ.and(fourLetterLong)) .forEach((n) -&gt; System.out.print(&quot;nName, which starts with &#39;J&#39; and four letter long is : &quot; + n)); 类似地，也可以使用 or() 和 xor() 方法。本例着重介绍了如下要点：可按需要将 Predicate 作为单独条件然后将其合并起来使用。简而言之，你可以以传统Java命令方式使用 Predicate 接口，也可以充分利用lambda表达式达到事半功倍的效果。 例6、Java 8中使用lambda表达式的Map和Reduce示例本例介绍最广为人知的函数式编程概念map。它允许你将对象进行转换。例如在本例中，我们将 costBeforeTax 列表的每个元素转换成为税后的值。我们将 x -&gt; x*x lambda表达式传到 map() 方法，后者将其应用到流中的每一个元素。然后用 forEach() 将列表元素打印出来。使用流API的收集器类，可以得到所有含税的开销。有 toList() 这样的方法将 map 或任何其他操作的结果合并起来。由于收集器在流上做终端操作，因此之后便不能重用流了。你甚至可以用流API的 reduce() 方法将所有数字合成一个，下一个例子将会讲到。 // 不使用lambda表达式为每个订单加上12%的税 List costBeforeTax = Arrays.asList(100, 200, 300, 400, 500); for (Integer cost : costBeforeTax) &#123; double price = cost + .12*cost; System.out.println(price); &#125; // 使用lambda表达式 List costBeforeTax = Arrays.asList(100, 200, 300, 400, 500); costBeforeTax.stream().map((cost) -&gt; cost + .12*cost).forEach(System.out::println); 输出： 112.0 224.0 336.0 448.0 560.0 112.0 224.0 336.0 448.0 560.0 例6.2、Java 8中使用lambda表达式的Map和Reduce示例在上个例子中，可以看到map将集合类（例如列表）元素进行转换的。还有一个 reduce() 函数可以将所有值合并成一个。Map和Reduce操作是函数式编程的核心操作，因为其功能，reduce 又被称为折叠操作。另外，reduce 并不是一个新的操作，你有可能已经在使用它。SQL中类似 sum()、avg() 或者 count() 的聚集函数，实际上就是 reduce 操作，因为它们接收多个值并返回一个值。流API定义的 reduceh() 函数可以接受lambda表达式，并对所有值进行合并。IntStream这样的类有类似 average()、count()、sum() 的内建方法来做 reduce 操作，也有mapToLong()、mapToDouble() 方法来做转换。这并不会限制你，你可以用内建方法，也可以自己定义。在这个Java 8的Map Reduce示例里，我们首先对所有价格应用 12% 的VAT，然后用 reduce() 方法计算总和。 // 为每个订单加上12%的税 // 老方法： List costBeforeTax = Arrays.asList(100, 200, 300, 400, 500); double total = 0; for (Integer cost : costBeforeTax) &#123; double price = cost + .12*cost; total = total + price; &#125; System.out.println(&quot;Total : &quot; + total); // 新方法： List costBeforeTax = Arrays.asList(100, 200, 300, 400, 500); double bill = costBeforeTax.stream().map((cost) -&gt; cost + .12*cost).reduce((sum, cost) -&gt; sum + cost).get(); System.out.println(&quot;Total : &quot; + bill); 输出： Total : 1680.0 Total : 1680.0 例7、通过过滤创建一个String列表过滤是Java开发者在大规模集合上的一个常用操作，而现在使用lambda表达式和流API过滤大规模数据集合是惊人的简单。流提供了一个 filter() 方法，接受一个 Predicate 对象，即可以传入一个lambda表达式作为过滤逻辑。下面的例子是用lambda表达式过滤Java集合，将帮助理解。 // 创建一个字符串列表，每个字符串长度大于2 List&lt;String&gt; filtered = strList.stream().filter(x -&gt; x.length()&gt; 2).collect(Collectors.toList()); System.out.printf(&quot;Original List : %s, filtered list : %s %n&quot;, strList, filtered); 输出： Original List : [abc, , bcd, , defg, jk], filtered list : [abc, bcd, defg] 另外，关于 filter() 方法有个常见误解。在现实生活中，做过滤的时候，通常会丢弃部分，但使用filter()方法则是获得一个新的列表，且其每个元素符合过滤原则。 例8、对列表的每个元素应用函数我们通常需要对列表的每个元素使用某个函数，例如逐一乘以某个数、除以某个数或者做其它操作。这些操作都很适合用 map() 方法，可以将转换逻辑以lambda表达式的形式放在 map() 方法里，就可以对集合的各个元素进行转换了，如下所示。 // 将字符串换成大写并用逗号链接起来 List&lt;String&gt; G7 = Arrays.asList(&quot;USA&quot;, &quot;Japan&quot;, &quot;France&quot;, &quot;Germany&quot;, &quot;Italy&quot;, &quot;U.K.&quot;,&quot;Canada&quot;); String G7Countries = G7.stream().map(x -&gt; x.toUpperCase()).collect(Collectors.joining(&quot;, &quot;)); System.out.println(G7Countries); 输出： USA, JAPAN, FRANCE, GERMANY, ITALY, U.K., CANADA 例9、复制不同的值，创建一个子列表本例展示了如何利用流的 distinct() 方法来对集合进行去重。 // 用所有不同的数字创建一个正方形列表 List&lt;Integer&gt; numbers = Arrays.asList(9, 10, 3, 4, 7, 3, 4); List&lt;Integer&gt; distinct = numbers.stream().map( i -&gt; i*i).distinct().collect(Collectors.toList()); System.out.printf(&quot;Original List : %s, Square Without duplicates : %s %n&quot;, numbers, distinct); 输出： Original List : [9, 10, 3, 4, 7, 3, 4], Square Without duplicates : [81, 100, 9, 16, 49] 例10、计算集合元素的最大值、最小值、总和以及平均值IntStream、LongStream 和 DoubleStream 等流的类中，有个非常有用的方法叫做 summaryStatistics() 。可以返回 IntSummaryStatistics、LongSummaryStatistics 或者 DoubleSummaryStatistic s，描述流中元素的各种摘要数据。在本例中，我们用这个方法来计算列表的最大值和最小值。它也有 getSum() 和 getAverage() 方法来获得列表的所有元素的总和及平均值。 //获取数字的个数、最小值、最大值、总和以及平均值 List&lt;Integer&gt; primes = Arrays.asList(2, 3, 5, 7, 11, 13, 17, 19, 23, 29); IntSummaryStatistics stats = primes.stream().mapToInt((x) -&gt; x).summaryStatistics(); System.out.println(&quot;Highest prime number in List : &quot; + stats.getMax()); System.out.println(&quot;Lowest prime number in List : &quot; + stats.getMin()); System.out.println(&quot;Sum of all prime numbers : &quot; + stats.getSum()); System.out.println(&quot;Average of all prime numbers : &quot; + stats.getAverage()); 输出： Highest prime number in List : 29 Lowest prime number in List : 2 Sum of all prime numbers : 129 Average of all prime numbers : 12.9 Lambda表达式 vs 匿名类既然lambda表达式即将正式取代Java代码中的匿名内部类，那么有必要对二者做一个比较分析。一个关键的不同点就是关键字 this。匿名类的 this 关键字指向匿名类，而lambda表达式的 this 关键字指向包围lambda表达式的类。另一个不同点是二者的编译方式。Java编译器将lambda表达式编译成类的私有方法。使用了Java 7的 invokedynamic 字节码指令来动态绑定这个方法。 Java 8 Lambda表达式要点10个Java lambda表达式、流API示例到目前为止我们看到了Java 8的10个lambda表达式，这对于新手来说是个合适的任务量，你可能需要亲自运行示例程序以便掌握。试着修改要求创建自己的例子，达到快速学习的目的。我还想建议大家使用Netbeans IDE来练习lambda表达式，它对Java 8支持良好。当把代码转换成函数式的时候，Netbeans会及时给你提示。只需跟着Netbeans的提示，就能很容易地把匿名类转换成lambda表达式。此外，如果你喜欢阅读，那么记得看一下Java 8的lambdas，实用函数式编程这本书（Java 8 Lambdas, pragmatic functional programming），作者是Richard Warburton，或者也可以看看Manning的Java 8实战（Java 8 in Action），这本书虽然还没出版，但我猜线上有第一章的免费pdf。不过，在你开始忙其它事情之前，先回顾一下Java 8的lambda表达式、默认方法和函数式接口的重点知识。 1）lambda表达式仅能放入如下代码：预定义使用了 @Functional 注释的函数式接口，自带一个抽象函数的方法，或者SAM（Single Abstract Method 单个抽象方法）类型。这些称为lambda表达式的目标类型，可以用作返回类型，或lambda目标代码的参数。例如，若一个方法接收Runnable、Comparable或者 Callable 接口，都有单个抽象方法，可以传入lambda表达式。类似的，如果一个方法接受声明于 java.util.function 包内的接口，例如 Predicate、Function、Consumer 或 Supplier，那么可以向其传lambda表达式。 2）lambda表达式内可以使用方法引用，仅当该方法不修改lambda表达式提供的参数。本例中的lambda表达式可以换为方法引用，因为这仅是一个参数相同的简单方法调用。 list.forEach(n -&gt; System.out.println(n)); list.forEach(System.out::println); // 使用方法引用 然而，若对参数有任何修改，则不能使用方法引用，而需键入完整地lambda表达式，如下所示： list.forEach((String s) -&gt; System.out.println(&quot;*&quot; + s + &quot;*&quot;)); 事实上，可以省略这里的lambda参数的类型声明，编译器可以从列表的类属性推测出来。 3）lambda内部可以使用静态、非静态和局部变量，这称为lambda内的变量捕获。 4）Lambda表达式在Java中又称为闭包或匿名函数，所以如果有同事把它叫闭包的时候，不用惊讶。 5）Lambda方法在编译器内部被翻译成私有方法，并派发 invokedynamic 字节码指令来进行调用。可以使用JDK中的 javap 工具来反编译class文件。使用 javap -p 或 javap -c -v 命令来看一看lambda表达式生成的字节码。大致应该长这样： private static java.lang.Object lambda$0(java.lang.String); 6）lambda表达式有个限制，那就是只能引用 final 或 final 局部变量，这就是说不能在lambda内部修改定义在域外的变量。 List&lt;Integer&gt; primes = Arrays.asList(new Integer[]&#123;2, 3,5,7&#125;); int factor = 2; primes.forEach(element -&gt; &#123; factor++; &#125;); Compile time error : &quot;local variables referenced from a lambda expression must be final or effectively final&quot; 另外，只是访问它而不作修改是可以的，如下所示： List&lt;Integer&gt; primes = Arrays.asList(new Integer[]&#123;2, 3,5,7&#125;); int factor = 2; primes.forEach(element -&gt; &#123; System.out.println(factor*element); &#125;); 输出： 4 6 10 14 因此，它看起来更像不可变闭包，类似于Python。 以上就是Java 8的lambda表达式的全部10个例子。此次修改将成为Java史上最大的一次，将深远影响未来Java开发者使用集合框架的方式。我想规模最相似的一次修改就是Java 5的发布了，它带来了很多优点，提升了代码质量，例如：泛型、枚举、自动装箱（Autoboxing）、静态导入、并发API和变量参数。上述特性使得Java代码更加清晰，我想lambda表达式也将进一步改进它。我在期待着开发并行第三方库，这可以使高性能应用变得更容易写。 更多阅读：http://javarevisited.blogspot.com/2014/02/10-example-of-lambda-expressions-in-java8.html#ixzz3gCMp6Vhc 原文链接： javarevisited 翻译： ImportNew.com - lemeilleur译文链接： http://www.importnew.com/16436.html [ 转载请保留原文出处、译者和译文链接。]","categories":[{"name":"Java 基础","slug":"Java-基础","permalink":"https://blog.fenxiangz.com/categories/Java-%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"lambda表达式","slug":"lambda表达式","permalink":"https://blog.fenxiangz.com/tags/lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/"}]},{"title":"Learn Gradle - 3 Java 快速入门","slug":"util/gradle/LearnGradle_3_Java快速入门","date":"2017-10-28T00:00:00.000Z","updated":"2020-12-20T16:47:02.986Z","comments":true,"path":"post/util/gradle/LearnGradle_3_Java快速入门.html","link":"","permalink":"https://blog.fenxiangz.com/post/util/gradle/LearnGradle_3_Java%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html","excerpt":"上一节主要对Gradle的脚本进行了简要的介绍，本节将继续学习Gradle的另外一个特性——插件（plugins）。 1、插件介绍插件是对Gradle功能的扩展，Gradle有着丰富的插件，你可以在这里搜索相关插件（传送门）。本章将简要介绍Gradle的Java插件（Java plugin），这个插件会给你的构建项目添加一些任务，比如编译java类、执行单元测试和将编译的class文件打包成jar文件等。 Java插件是基于约定的（约定优于配置），它在项目的很多方面定义了默认值，例如，Java源文件应该位于什么位置。我们只要遵循插件的约定，就不需要在Gradle配置脚本进行额外的相关配置。当然，在某些情况下，你的项目不想或不能遵循这个约定也是可以的，这样你就需要额外的配置你的构建脚本。 Gradle Java插件对于项目文件存放的默认位置与maven类似。","text":"上一节主要对Gradle的脚本进行了简要的介绍，本节将继续学习Gradle的另外一个特性——插件（plugins）。 1、插件介绍插件是对Gradle功能的扩展，Gradle有着丰富的插件，你可以在这里搜索相关插件（传送门）。本章将简要介绍Gradle的Java插件（Java plugin），这个插件会给你的构建项目添加一些任务，比如编译java类、执行单元测试和将编译的class文件打包成jar文件等。 Java插件是基于约定的（约定优于配置），它在项目的很多方面定义了默认值，例如，Java源文件应该位于什么位置。我们只要遵循插件的约定，就不需要在Gradle配置脚本进行额外的相关配置。当然，在某些情况下，你的项目不想或不能遵循这个约定也是可以的，这样你就需要额外的配置你的构建脚本。 Gradle Java插件对于项目文件存放的默认位置与maven类似。 Java源码存放在目录：src/main/java Java测试代码存放目录：src/test/java 资源文件存放目录：src/main/resources 测试相关资源文件存放目录：src/test/resources 所有输出文件位于目录：build 输出的jar文件位于目录：build/libs 2、一个简单的Java项目新建一个文件build.gradle，添加代码： 1apply plugin: &#x27;java&#x27; 以上代码即配置java插件到构建脚本中。当执行构建脚本时，它将给项目添加一系列任务。我们执行：gradle build，来看看输出的结果： 12345678910111213:compileJava UP-TO-DATE:processResources UP-TO-DATE:classes UP-TO-DATE:jar UP-TO-DATE:assemble UP-TO-DATE:compileTestJava UP-TO-DATE:processTestResources UP-TO-DATE:testClasses UP-TO-DATE:test UP-TO-DATE:check UP-TO-DATE:build UP-TO-DATE BUILD SUCCESSFUL 根据输出结果可以看出，我们执行的build这个任务依赖其他任务，比如compileJava等，这就是java插件预先定义好的一系列任务。 你还可以执行一些其他的任务，比如执行：gradle clean，gradle assemble，gradle check等。 gradle clean：删除构建目录以及已经构建完成的文件； gradle assemble（装配）：编译和打包java代码，但是不会执行单元测试（从上面的任务依赖结果也可以看出来）。如果你应用了其他插件，那么还会完成一下其他动作。例如，如果你应用了War这个插件，那么这个任务将会为你的项目生成war文件。 gradle check：编译且执行测试。与assemble类似，如果你应用了其他包含check任务的插件，例如，Checkstyle插件，那么这个任务将会检查你的项目代码的质量，并且生成检测报告。 如果想知道Gradle当前配置下哪些任务可执行，可以执行：gradle tasks，例如应用了java插件的配置，执行该命令，输出： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253:tasks ------------------------------------------------------------All tasks runnable from root project------------------------------------------------------------ Build tasks-----------assemble - Assembles the outputs of this project.build - Assembles and tests this project.buildDependents - Assembles and tests this project and all projects that depend on it.buildNeeded - Assembles and tests this project and all projects it depends on.classes - Assembles classes &#x27;main&#x27;.clean - Deletes the build directory.jar - Assembles a jar archive containing the main classes.testClasses - Assembles classes &#x27;test&#x27;. Build Setup tasks-----------------init - Initializes a new Gradle build. [incubating]wrapper - Generates Gradle wrapper files. [incubating] Documentation tasks-------------------javadoc - Generates Javadoc API documentation for the main source code. Help tasks----------components - Displays the components produced by root project &#x27;learn-gradle&#x27;. [incubating]dependencies - Displays all dependencies declared in root project &#x27;learn-gradle&#x27;.dependencyInsight - Displays the insight into a specific dependency in root project &#x27;learn-gradle&#x27;.help - Displays a help message.model - Displays the configuration model of root project &#x27;learn-gradle&#x27;. [incubating]projects - Displays the sub-projects of root project &#x27;learn-gradle&#x27;.properties - Displays the properties of root project &#x27;learn-gradle&#x27;.tasks - Displays the tasks runnable from root project &#x27;learn-gradle&#x27;. Verification tasks------------------check - Runs all checks.test - Runs the unit tests. Rules-----Pattern: clean&lt;TaskName&gt;: Cleans the output files of a task.Pattern: build&lt;ConfigurationName&gt;: Assembles the artifacts of a configuration.Pattern: upload&lt;ConfigurationName&gt;: Assembles and uploads the artifacts belonging to a configuration. To see all tasks and more detail, run gradle tasks --all To see more detail about a task, run gradle help --task &lt;task&gt; BUILD SUCCESSFUL 小伙伴们看到这里会不会有疑问，如果在构建脚本中定义了名为tasks的任务，执行会是如何？好奇的小伙伴可以自己试一试噢。事实上，是会覆盖原有的任务的。 3、外部依赖通常一个Java项目会依赖多个其他项目或jar文件，我们可以通过配置gradle仓库（repository）告诉gradle从哪里获取需要的依赖，并且gradle还可以配置使用maven仓库。例如，我们配置gradle使用maven中央仓库，在build.gradle中添加代码： 123repositories &#123; mavenCentral()&#125; 接下来，我们来添加一些依赖。代码示例： 1234dependencies &#123; compile group: &#x27;commons-collections&#x27;, name: &#x27;commons-collections&#x27;, version: &#x27;3.2&#x27; testCompile group: &#x27;junit&#x27;, name: &#x27;junit&#x27;, version: &#x27;4.+&#x27;&#125; 关于依赖，暂时就点这么多。详细可以参考gradle依赖管理基础，也可以关注后续文章。 4、定义项目属性Java插件会为项目添加一系列的属性，通常情况下，初始的Java项目使用这些默认配置就足够了，我们不需要进行额外的配置。但是如果默认属性不满足于你的项目，你也可以进行自定义项目的一些信息。例如我们为项目指定版本号和一些jar manifest信息。 1234567sourceCompatibility = 1.5version = &#x27;1.0&#x27;jar &#123; manifest &#123; attributes &#x27;Implementation-Title&#x27;: &#x27;Gradle Quickstart&#x27;, &#x27;Implementation-Version&#x27;: version &#125;&#125; 事实上，Java插件添加的一系列任务与我们之前在脚本中自定义的任务没什么区别，都是很常规的任务。我们可以随意定制和修改这些任务。例如，设置任务的属性、为任务添加行为、改变任务的依赖，甚至替换已有的任务。例如我们可以配置Test类型的test任务，当test任务执行的时候，添加一个系统属性。配置脚本如下： 123test &#123; systemProperties &#x27;property&#x27;: &#x27;value&#x27;&#125; 另外，与之前提到的“gradle tasks”命令类型，我们可以通过“gradle properties”来查看当前配置所支持的可配置属性有哪些。 5、将Jar文件发布到仓库1234567uploadArchives &#123; repositories &#123; flatDir &#123; dirs &#x27;repos&#x27; &#125; &#125;&#125; 执行gradle uploadArchives，将会把相关jar文件发布到reops仓库中。更多参考：Publishing artifacts 6、构建多个Java项目假设我们的项目结构如下所示： 12345multiproject/--api/--services/webservice/--shared/--services/shared/ 项目api生成jar文件，Java客户端通过jar提供的接口访问web服务；项目services/webservice是一个webapp，提供web服务；项目shared 包含api和webservice公共代码；项目services/shared依赖shared项目，包含webservice公共代码。 接下来，我们开始定义多项目构建。 1）首先，我们需要添加一个配置文件：settings.gradle文件。settings.gradle位于项目的根目录，也就是multiproject目录。编辑settings.gradle，输入配置信息： 1include &quot;shared&quot;, &quot;api&quot;, &quot;services:webservice&quot;, &quot;services:shared&quot; include是Gradle DSL定义的核心类型Settings的方法，用于构建指定项目。配置中指定的参数“shared”、“api”等值默认是当前配置目录的目录名称，而“services:webservice”将根据默认约定映射系统物理路径”services/webservice”（相对于根目录）。关于include更详细的信息可以参考：构建树。 2）定义所有子项目公用配置。在根目录创建文件：build.gradle，输入配置信息： 123456789101112131415161718subprojects &#123; apply plugin: &#x27;java&#x27; apply plugin: &#x27;eclipse-wtp&#x27; repositories &#123; mavenCentral() &#125; dependencies &#123; testCompile &#x27;junit:junit:4.12&#x27; &#125; version = &#x27;1.0&#x27; jar &#123; manifest.attributes provider: &#x27;gradle&#x27; &#125;&#125; subprojects 是Gradle DSL定义的构建脚本模块之一，用于定义所有子项目的配置信息。在以上配置中，我们给所有子项目定义了使用“java”和“eclipse-wtp”插件，还定义了仓库、依赖、版本号以及jar（jar是Gradle的任务类型之一，任务是装配jar包，jar任务包含属性manifest，用于描述jar的信息，具体参考：Jar）。 我们在根目录执行gradle build命令时，这些配置会应用到所有子项目中。 3）给项目添加依赖 新建文件：api/build.gradle，添加配置： 123dependencies &#123; compile project(&#x27;:shared&#x27;)&#125; 以上，我们定义了api项目依赖shared项目，当我们在根目录执行gradle build命令时，gradle会确保在编译api之前，先完成shared项目编译，然后才会编译api项目。 同样，添加services/webservice/build.gradle，添加配置： 123dependencies &#123; compile project(&#x27;:services:shared&#x27;)&#125; 在根目录执行：gradle compileJava，输出： 12345678910111213:shared:compileJava UP-TO-DATE:shared:processResources UP-TO-DATE:shared:classes UP-TO-DATE:shared:jar UP-TO-DATE:api:compileJava UP-TO-DATE:services:compileJava UP-TO-DATE:services:shared:compileJava UP-TO-DATE:services:shared:processResources UP-TO-DATE:services:shared:classes UP-TO-DATE:services:shared:jar UP-TO-DATE:services:webservice:compileJava UP-TO-DATE BUILD SUCCESSFUL 通过输出信息我们就可以清楚看出依赖配置是否正确啦。","categories":[{"name":"Gradle","slug":"Gradle","permalink":"https://blog.fenxiangz.com/categories/Gradle/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"包管理","slug":"包管理","permalink":"https://blog.fenxiangz.com/tags/%E5%8C%85%E7%AE%A1%E7%90%86/"},{"name":"Gradle","slug":"Gradle","permalink":"https://blog.fenxiangz.com/tags/Gradle/"}]},{"title":"Learn Gradle - 2 基本的构建脚本介绍","slug":"util/gradle/LearnGradle_2_基本的构建脚本介绍","date":"2017-10-27T00:00:00.000Z","updated":"2020-12-20T16:47:02.985Z","comments":true,"path":"post/util/gradle/LearnGradle_2_基本的构建脚本介绍.html","link":"","permalink":"https://blog.fenxiangz.com/post/util/gradle/LearnGradle_2_%E5%9F%BA%E6%9C%AC%E7%9A%84%E6%9E%84%E5%BB%BA%E8%84%9A%E6%9C%AC%E4%BB%8B%E7%BB%8D.html","excerpt":"1、项目和任务Gradle 构建脚本包括两个最基本的概念，就是项目（projects）和任务（tasks）。 项目是指我们的构建产物（比如jar包）或实施产物（比如web application等）。Gradle构建脚本包含一个或多个项目。 任务是指不可分的最小工作单元，执行构建工作（比如编译一些类文件、创建jar文件、生成javadoc以及发布架构文档到仓库等）。一个项目包含一个或多个任务。 2、Hello World！！下面我们学习一个简单的hello world例子来简单认识一下Gradle构建脚本。 新建文件：build.gradle 添加内容： 12345task hello &#123; doLast &#123; println &#x27;Hello world!&#x27; &#125;&#125; 使用命令行进入build.gradle所在目录，执行：gradle hello ，输出：","text":"1、项目和任务Gradle 构建脚本包括两个最基本的概念，就是项目（projects）和任务（tasks）。 项目是指我们的构建产物（比如jar包）或实施产物（比如web application等）。Gradle构建脚本包含一个或多个项目。 任务是指不可分的最小工作单元，执行构建工作（比如编译一些类文件、创建jar文件、生成javadoc以及发布架构文档到仓库等）。一个项目包含一个或多个任务。 2、Hello World！！下面我们学习一个简单的hello world例子来简单认识一下Gradle构建脚本。 新建文件：build.gradle 添加内容： 12345task hello &#123; doLast &#123; println &#x27;Hello world!&#x27; &#125;&#125; 使用命令行进入build.gradle所在目录，执行：gradle hello ，输出： 1234:helloHello world! BUILD SUCCESSFUL 在这个例子中build.gradle 文件就是一个构建脚本（严格的说，这是一个构建配置脚本），这个脚本定义了一个项目以及项目包含的任务。 Gradle是领域驱动设计的构建工具，在它的实现当中，Project接口对应上面的project概念，Task接口对应上面的task概念，实际上除此之外还有一个重要的领域对象，即Action，对应的是task里面具体的某一个操作。一个project由多个task组成，一个task也是由多个action组成。 当执行gradle hello的时候，Gradle就会去调用这个hello task来执行给定操作(Action)。这个操作其实就是一个用Groovy代码写的闭包，代码中的task是Project类里的一个方法，通过调用这里的task方法创建了一个Task对象，并在对象的doLast方法中传入 println ‘Hello world!’ 这个闭包。这个闭包就是一个Action。 Task是Gradle里定义的一个接口，表示上述概念中的task。它定义了一系列的诸如doLast, doFirst等抽象方法，具体可以看gradle api里org.gradle.api.Task的文档。 3、使用快捷键定义任务doLast还有另外一种简单的写法： 123task hello &lt;&lt; &#123; println &#x27;Hello world!&#x27;&#125; 执行gradle hello 命令，输出结果与之前的相同。也就是我们把像doLast这样的代码，直接简化为&lt;&lt;这个符号了。这其实是Gradle利用了Groovy的操作符重载的特性，把左位移操作符实现为将action加到task的最后，相当于调用doLast方法。看Gradle的api文档里对doLast()和leftShift()这两个方法的介绍，可知它们的作用是一样的，所以在这里，&lt;&lt;左移操作符即doLast的简写方式。 4、脚本即代码在Gradle的构建脚本中，可以使用Groovy代码以实现更强大的功能。 例1： 12345task upper &lt;&lt; &#123; String someString = &#x27;mY_nAmE&#x27; println &quot;Original: &quot; + someString println &quot;Upper case: &quot; + someString.toUpperCase()&#125; 执行gradle upper，输出： 12345:upperOriginal: mY_nAmEUpper case: MY_NAMEBUILD SUCCESSFUL 例2： 123task count &lt;&lt; &#123; 4.times &#123; print &quot;$it &quot; &#125;&#125; 执行gradle count，输出： 123:count0 1 2 3BUILD SUCCESSFUL 5、任务依赖1、在脚本中定义任务依赖： 123456task hello &lt;&lt; &#123; println &#x27;Hello world!&#x27;&#125;task intro(dependsOn: hello) &lt;&lt; &#123; println &quot;I&#x27;m Gradle&quot;&#125; 执行gradle intro，输出： 123456:helloHello world!:introI&#x27;m Gradle BUILD SUCCESSFUL 2、任务可以依赖尚未出现的任务： 123456task taskX(dependsOn: &#x27;taskY&#x27;) &lt;&lt; &#123; println &#x27;taskX&#x27;&#125;task taskY &lt;&lt; &#123; println &#x27;taskY&#x27;&#125; 本例中任务taskX依赖taskY，但是taskY在taskX之后才定义。执行gradle taskX，输出： 123456:taskYtaskY:taskXtaskX BUILD SUCCESSFUL 6、动态任务我们可以使用Groovy的语法动态创建任务，例如： 123454.times &#123; counter -&gt; task &quot;task$counter&quot; &lt;&lt; &#123; println &quot;I&#x27;m task number $counter&quot; &#125;&#125; 执行gradle -q task1，输出： 1234:task1I&#x27;m task number 1 BUILD SUCCESSFUL 7、操纵任务已经创建的任务可以通过api（例1用到的api是dependsOn ）进行访问。 例1：给一个任务添加依赖 1234564.times &#123; counter -&gt; task &quot;task$counter&quot; &lt;&lt; &#123; println &quot;I&#x27;m task number $counter&quot; &#125;&#125;task0.dependsOn task2, task3 执行gradle task0 ，输出 12345678:task2I&#x27;m task number 2:task3I&#x27;m task number 3:task0I&#x27;m task number 0 BUILD SUCCESSFUL 例2：给一个任务添加行为 123456789101112task hello &lt;&lt; &#123; println &#x27;Hello Earth&#x27;&#125;hello.doFirst &#123; println &#x27;Hello Venus&#x27;&#125;hello.doLast &#123; println &#x27;Hello Mars&#x27;&#125;hello &lt;&lt; &#123; println &#x27;Hello Jupiter&#x27;&#125; 执行gradle hello，输出： 1234567:helloHello VenusHello EarthHello MarsHello Jupiter BUILD SUCCESSFUL 任务先执行了doFirst，再按顺序执行了doLast（”&lt;&lt;”可以理解为doLast的别名，所以相同的api方法将按照配置文件顺序执行） 8、自定义属性123456task myTask &#123; ext.myProperty = &quot;myValue&quot;&#125;task printTaskProperties &lt;&lt; &#123; println myTask.myProperty&#125; 执行gradle printTaskProperties，输出： 1234:printTaskPropertiesmyValue BUILD SUCCESSFUL 9、默认任务gradle允许在构建过程中配置一个或多个任务作为默认任务来执行，例如： 12345678910111213defaultTasks &#x27;clean&#x27;, &#x27;run&#x27; task clean &lt;&lt; &#123; println &#x27;Default Cleaning!&#x27;&#125; task run &lt;&lt; &#123; println &#x27;Default Running!&#x27;&#125; task other &lt;&lt; &#123; println &quot;I&#x27;m not a default task!&quot;&#125; 执行 gradle ，输出： 123456:cleanDefault Cleaning!:runDefault Running! BUILD SUCCESSFUL 10、DAG（Directed acyclic graph，有向非循环图）配置Gradle构建的生命周期包含初始化阶段、配置阶段和执行阶段。Gradle使用DAG来记录任务执行的顺序。配置阶段完成后，Gradle就明确了所有需要被执行的任务，这些任务将被存储到taskGraph。Gradle提供了一个钩子来使用这个（taskGraph）信息。下面这个例子我们将判断taskGraph是否包含release任务来确定项目发布的版本号。 12345678910111213task distribution &lt;&lt; &#123; println &quot;We build the zip with version=$version&quot;&#125; task release(dependsOn: &#x27;distribution&#x27;) &lt;&lt; &#123; println &#x27;We release now&#x27;&#125; gradle.taskGraph.whenReady &#123;taskGraph -&gt; if (taskGraph.hasTask(release)) &#123; version = &#x27;1.0&#x27; &#125; else &#123; version = &#x27;1.0-SNAPSHOT&#x27; &#125;&#125; 执行 gradle distribution ，输出： 1234:distributionWe build the zip with version=1.0-SNAPSHOT BUILD SUCCESSFUL 执行gradle release ，输出： 123456:distributionWe build the zip with version=1.0:releaseWe release now BUILD SUCCESSFUL","categories":[{"name":"Gradle","slug":"Gradle","permalink":"https://blog.fenxiangz.com/categories/Gradle/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"包管理","slug":"包管理","permalink":"https://blog.fenxiangz.com/tags/%E5%8C%85%E7%AE%A1%E7%90%86/"},{"name":"Gradle","slug":"Gradle","permalink":"https://blog.fenxiangz.com/tags/Gradle/"}]},{"title":"Learn Gradle - 1 安装","slug":"util/gradle/LearnGradle_1_安装","date":"2017-10-26T00:00:00.000Z","updated":"2020-12-20T16:47:02.985Z","comments":true,"path":"post/util/gradle/LearnGradle_1_安装.html","link":"","permalink":"https://blog.fenxiangz.com/post/util/gradle/LearnGradle_1_%E5%AE%89%E8%A3%85.html","excerpt":"1、下载Gradle方式一：http://gradle.org/ 使用首页Download链接直接下载最新版。 方式二：http://gradle.org/gradle-download/ 在“PREVIOUS RELEASES”（右侧）下方选择一个版本，然后选择完整版“Complete distribution”或者选择不含源码和文档仅包含程序的版本“Binary only distribution”下载。 （这里下载最新版本完整压缩包：gradle-2.5-all.zip） 2、安装解压缩下载的zip文件：gradle-2.5-all.zip 得到目录 gradle-2.5 ，将文件夹移动到合适的位置，如 F:\\gradle-2.5，这个文件包含了所有gradle的内容，包括： 12345执行程序（bin、lib）文档（docs）源码（src）例子（samples）配置环境变量： 新增变量名：GRADLE_HOME，变量值：F:\\gradle-2.5在已有Path变量的末尾追加字符串 ”;%GRADLE_HOME%\\bin;“（引号内的字符串）","text":"1、下载Gradle方式一：http://gradle.org/ 使用首页Download链接直接下载最新版。 方式二：http://gradle.org/gradle-download/ 在“PREVIOUS RELEASES”（右侧）下方选择一个版本，然后选择完整版“Complete distribution”或者选择不含源码和文档仅包含程序的版本“Binary only distribution”下载。 （这里下载最新版本完整压缩包：gradle-2.5-all.zip） 2、安装解压缩下载的zip文件：gradle-2.5-all.zip 得到目录 gradle-2.5 ，将文件夹移动到合适的位置，如 F:\\gradle-2.5，这个文件包含了所有gradle的内容，包括： 12345执行程序（bin、lib）文档（docs）源码（src）例子（samples）配置环境变量： 新增变量名：GRADLE_HOME，变量值：F:\\gradle-2.5在已有Path变量的末尾追加字符串 ”;%GRADLE_HOME%\\bin;“（引号内的字符串） 3、测试打开cmd，执行命令：gradle -v，输出 123456789101112------------------------------------------------------------Gradle 2.5------------------------------------------------------------Build time: 2015-07-08 07:38:37 UTCBuild number: noneRevision: 093765bccd3ee722ed5310583e5ed140688a8c2bGroovy: 2.3.10Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013JVM: 1.7.0_71 (Oracle Corporation 24.71-b01)OS: Windows 8 6.2 x86 至此，Gradle安装完毕。","categories":[{"name":"Gradle","slug":"Gradle","permalink":"https://blog.fenxiangz.com/categories/Gradle/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"包管理","slug":"包管理","permalink":"https://blog.fenxiangz.com/tags/%E5%8C%85%E7%AE%A1%E7%90%86/"},{"name":"Gradle","slug":"Gradle","permalink":"https://blog.fenxiangz.com/tags/Gradle/"}]},{"title":"IntelliJ 各种快捷键","slug":"util/ide/2017-10-25_idea_shotcut","date":"2017-10-25T11:29:24.000Z","updated":"2021-01-18T05:30:05.574Z","comments":true,"path":"post/util/ide/2017-10-25_idea_shotcut.html","link":"","permalink":"https://blog.fenxiangz.com/post/util/ide/2017-10-25_idea_shotcut.html","excerpt":"","text":"工欲善其事必先利其器，花一点时间，记录一下 Intellij 各种快捷键。 窗口交互 Alt + 1 显示 Project 窗口 ☆☆☆ Alt + 2 显示 Favorite 窗口 Alt + 3 显示 Find 窗口 Alt + 5 显示 Debug 窗口 …… 具体看显示窗口的标签标注 1 类似的就是快捷键值了 Ctrl + Shift + F12 最大化编辑窗口与上次视图进行切换 Alt + Home 定位到导航目录，然后可以使用上下左右建进行文件目录导航 Esc 焦点进入编辑器 代码补全 Ctrl + Space 基本补全 Ctrl + Shift + Space 智能补全 Alt + / 按照最近的关键字补全 Ctrl + Shift + Enter 补全代码结构，包括缺失的小括号、中括号、大括号以及必要的结构 编辑器常用 Ctrl + Shift + Up 和 Ctrl + Shift + Down 移动选中的代码行 ☆☆☆ Ctrl + D 复制选中的代码行 ☆☆☆☆ Ctrl + Y 删除选中的代码行 ☆☆☆☆ Ctrl + / 按行注释代码或取消注释代码 ☆☆☆☆☆ Ctrl + Shift + / 按块注释代码或取消注释代码 ☆☆☆☆☆ Ctrl + F 或 Alt + F3 在当前打开文件查找 ☆☆☆☆☆ Ctrl + R 在当前打开文件查找并替换 Alt + Right 和 Alt + Left 导航打开的编辑器窗口 ☆☆☆☆ Ctrl + Alt + Left 和 Ctrl + Alt + Right 导航焦点 ☆☆☆☆☆ Ctrl + - （中横线，减号） 收起代码块 Ctrl + - （加号，等于号） 展开代码块 Alt + Insert 打开代码生成菜单 Ctrl + Alt + T 打开代码包围菜单，可以快速将代码使用 if ，try catch 等语法进行围绕 Ctrl + W 选中代码块，试几次就知道啥意思了 Alt + J 和 Alt + Shift + J 向下或向上查找选中的关键字 ☆☆☆☆ 导航 Ctrl + E 打开最近文件菜单 ☆☆☆ Ctrl + N 类搜索，这个功能应该非常常用了，使用频率很高，模糊搜索非常好用 ☆☆☆☆☆ Ctrl + Shift + N 文件搜索，搜索资源文件常用到 ☆☆☆☆☆ Ctrl + Shift + Alt + N 符号搜索，搜索方法和变量名的时候会用到 查看结构 Ctrl + F12 查看文件结构，类，JSP，XML，Properties 文件等都可以查看 ☆☆☆☆☆ Alt + F1 选择文件在什么窗口打开，选项很多，可以自己看看。 Alt + F1 然后 C ，使用本地 Explorer 打开还挺好用的 ☆☆☆☆ Ctrl + Q 查看 Java doc Ctrl + Shift + I 查看定义 Ctrl + Alt + F7 查看调用方，这个我比较不习惯，喜欢用 Alt + F7，在 Find 窗口上查看 ☆☆☆☆☆ Ctrl + Alt + B 查看实现，非常常用 ☆☆☆☆☆ Ctrl + U 查看接口 重构 Shift + F6 重命名 ☆☆☆☆☆ Ctrl + Alt + V 重构成变量，一般就是要给方法的返回值，赋值给一个变量 Ctrl + Alt + F 重构成成员变量 和 V 类似 Ctrl + Alt + C 重构成常量 Ctrl + Alt + M 重构成一个方法，这个还挺常用的 ☆☆☆☆ Ctrl + Alt + N 选择多行代码，重构成一行代码，这个应该不常用，影响代码可读性 F5 复制 F6 移动 Ctrl + Shift + Alt + T 打开重构菜单 代码格式 Ctrl + Alt + I 默认使用空格缩进 Ctrl + Alt + L 格式化，呵呵，这个和 QQ 冲突了 Ctrl + Alt + L 代码导入（import） 版本控制 Alt + 9 or Shift + Alt + 9 上面已经提到过，打开版本控制窗口 Alt + Back Quote 这个说是打开 CVS 操作菜单，but，我的 IDE 没有反应 Ctrl + K 提交代码 Ctrl + T 更新代码 Ctrl + Shift + K push 代码 GIT 用的吧？我这 SVN 没有发现有什么功能 编译 Ctrl + F9 Debug Ctrl + F8 打断点 F7 进入下一步 Shift + F7 智能进入下一步 F8 结束这一步 Shift + F8 结束当前方法，返回 … 在面板上可以查看其他快捷键的使用 完。","categories":[{"name":"IDE","slug":"IDE","permalink":"https://blog.fenxiangz.com/categories/IDE/"}],"tags":[{"name":"Idea","slug":"Idea","permalink":"https://blog.fenxiangz.com/tags/Idea/"},{"name":"IDE","slug":"IDE","permalink":"https://blog.fenxiangz.com/tags/IDE/"}]},{"title":"Java 引用","slug":"java/basic/2017-07-14_java_reference","date":"2017-07-14T11:51:24.000Z","updated":"2020-12-20T16:47:02.964Z","comments":true,"path":"post/java/basic/2017-07-14_java_reference.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/basic/2017-07-14_java_reference.html","excerpt":"","text":"在JDK 1.2以前的版本中，若一个对象不被任何变量引用，那么程序就无法再使用这个对象。也就是说，只有对象处于可触及（reachable）状态，程序才能使用它。从JDK 1.2版本开始，把对象的引用分为4种级别，从而使程序能更加灵活地控制对象的生命周期。这4种级别由高到低依次为：强引用、软引用、弱引用和虚引用。 概要强引用 只要引用存在，垃圾回收器永远不会回收 Object obj = new Object(); //可直接通过obj取得对应的对象 如obj.equels(new Object()); 而这样 obj对象对后面new Object的一个强引用，只有当obj这个引用被释放之后，对象才会被释放掉，这也是我们经常所用到的编码形式。 软引用 非必须引用，内存溢出之前进行回收，可以通过以下代码实现 Object obj = new Object(); SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj); obj = null; sf.get();//有时候会返回null 这时候sf是对obj的一个软引用，通过sf.get()方法可以取到这个对象，当然，当这个对象被标记为需要回收的对象时，则返回null；软引用主要用户实现类似缓存的功能，在内存足够的情况下直接通过软引用取值，无需从繁忙的真实来源查询数据，提升速度；当内存不足时，自动删除这部分缓存数据，从真正的来源查询这些数据。 弱引用 第二次垃圾回收时回收，可以通过如下代码实现 Object obj = new Object(); WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj); obj = null; wf.get();//有时候会返回null wf.isEnQueued();//返回是否被垃圾回收器标记为即将回收的垃圾 弱引用是在第二次垃圾回收时回收，短时间内通过弱引用取对应的数据，可以取到，当执行过第二次垃圾回收时，将返回null。弱引用主要用于监控对象是否已经被垃圾回收器标记为即将回收的垃圾，可以通过弱引用的isEnQueued方法返回对象是否被垃圾回收器标记。 虚引用 垃圾回收时回收，无法通过引用取到对象值，可以通过如下代码实现 Object obj = new Object(); PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj); obj=null; pf.get();//永远返回null pf.isEnQueued();//返回是否从内存中已经删除 虚引用是每次垃圾回收的时候都会被回收，通过虚引用的get方法永远获取到的数据为null，因此也被成为幽灵引用。虚引用主要用于检测对象是否已经从内存中删除。 详细解释⑴强引用（StrongReference） 强引用是使用最普遍的引用。如果一个对象具有强引用，那垃圾回收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足的问题。 ps：强引用其实也就是我们平时A a = new A()这个意思。 ⑵软引用（SoftReference） 如果一个对象只具有软引用，则内存空间足够，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存（下文给出示例）。软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收器回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。 ⑶弱引用（WeakReference） 弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 ⑷虚引用（PhantomReference） “虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。虚引用主要用来跟踪对象被垃圾回收器回收的活动。虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。 ReferenceQueue queue = new ReferenceQueue (); PhantomReference pr = new PhantomReference (object, queue); 程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 使用软引用构建敏感数据的缓存1 为什么需要使用软引用 首先，我们看一个雇员信息查询系统的实例。我们将使用一个Java语言实现的雇员信息查询系统查询存储在磁盘文件或者数据库中的雇员人事档案信息。作为一个用户，我们完全有可能需要回头去查看几分钟甚至几秒钟前查看过的雇员档案信息(同样，我们在浏览WEB页面的时候也经常会使用“后退”按钮)。这时我们通常会有两种程序实现方式:一种是把过去查看过的雇员信息保存在内存中，每一个存储了雇员档案信息的Java对象的生命周期贯穿整个应用程序始终;另一种是当用户开始查看其他雇员的档案信息的时候，把存储了当前所查看的雇员档案信息的Java对象结束引用，使得垃圾收集线程可以回收其所占用的内存空间，当用户再次需要浏览该雇员的档案信息的时候，重新构建该雇员的信息。很显然，第一种实现方法将造成大量的内存浪费，而第二种实现的缺陷在于即使垃圾收集线程还没有进行垃圾收集，包含雇员档案信息的对象仍然完好地保存在内存中，应用程序也要重新构建一个对象。我们知道，访问磁盘文件、访问网络资源、查询数据库等操作都是影响应用程序执行性能的重要因素，如果能重新获取那些尚未被回收的Java对象的引用，必将减少不必要的访问，大大提高程序的运行速度。 2 如果使用软引用 SoftReference的特点是它的一个实例保存对一个Java对象的软引用，该软引用的存在不妨碍垃圾收集线程对该Java对象的回收。也就是说，一旦SoftReference保存了对一个Java对象的软引用后，在垃圾线程对这个Java对象回收前，SoftReference类所提供的get()方法返回Java对象的强引用。另外，一旦垃圾线程回收该Java对象之后，get()方法将返回null。看下面代码: MyObject aReference = new MyObject(); SoftReference aSoftRef = new SoftReference(aRef); 此时，对于这个MyObject对象，有两个引用路径，一个是来自aSoftRef对象的软引用，一个来自变量aReference的强引用，所以这个MyObject对象是强可及对象。随即，我们可以结束aReference对这个MyObject实例的强引用: aReference = null; 此后，这个MyObject对象成为了软可及对象。如果垃圾收集线程进行内存垃圾收集，并不会因为有一个SoftReference对该对象的引用而始终保留该对象。Java虚拟机的垃圾收集线程对软可及对象和其他一般Java对象进行了区别对待：软可及对象的清理是由垃圾收集线程根据其特定算法按照内存需求决定的。也就是说，垃圾收集线程会在虚拟机抛出OutOfMemoryError之前回收软可及对象，而且虚拟机会尽可能优先回收长时间闲置不用的软可及对象，对那些刚刚构建的或刚刚使用过的“新”软可反对象会被虚拟机尽可能保留。在回收这些对象之前，我们可以通过: MyObject anotherRef = (MyObject)aSoftRef.get(); 重新获得对该实例的强引用。而回收之后，调用get()方法就只能得到null了。 3 使用ReferenceQueue清除失去了软引用对象的SoftReference 作为一个Java对象，SoftReference对象除了具有保存软引用的特殊性之外，也具有Java对象的一般性。所以，当软可及对象被回收之后，虽然这个SoftReference对象的get()方法返回null,但这个SoftReference对象已经不再具有存在的价值，需要一个适当的清除机制，避免大量SoftReference对象带来的内存泄漏。在java.lang.ref包里还提供了ReferenceQueue。如果在创建SoftReference对象的时候，使用了一个ReferenceQueue对象作为参数提供给SoftReference的构造方法，如: ReferenceQueue queue = new ReferenceQueue(); SoftReference ref = new SoftReference(aMyObject, queue); 那么当这个SoftReference所软引用的aMyOhject被垃圾收集器回收的同时，ref所强引用的SoftReference对象被列入ReferenceQueue。也就是说，ReferenceQueue中保存的对象是Reference对象，而且是已经失去了它所软引用的对象的Reference对象。另外从ReferenceQueue这个名字也可以看出，它是一个队列，当我们调用它的poll()方法的时候，如果这个队列中不是空队列，那么将返回队列前面的那个Reference对象。在任何时候，我们都可以调用ReferenceQueue的poll()方法来检查是否有它所关心的非强可及对象被回收。如果队列为空，将返回一个null，否则该方法返回队列中前面的一个Reference对象。利用这个方法，我们可以检查哪个SoftReference所软引用的对象已经被回收。于是我们可以把这些失去所软引用的对象的SoftReference对象清除掉。常用的方式为: SoftReference ref = null; while ((ref = (EmployeeRef) q.poll()) != null) &#123; // 清除ref &#125;","categories":[{"name":"Java 基础","slug":"Java-基础","permalink":"https://blog.fenxiangz.com/categories/Java-%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://blog.fenxiangz.com/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"name":"Java引用","slug":"Java引用","permalink":"https://blog.fenxiangz.com/tags/Java%E5%BC%95%E7%94%A8/"}]},{"title":"Java NIO(3) 选择器 Selector","slug":"java/nio/2017-04-23_java_nio_3","date":"2017-04-23T00:00:00.000Z","updated":"2020-12-20T16:47:02.969Z","comments":true,"path":"post/java/nio/2017-04-23_java_nio_3.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/nio/2017-04-23_java_nio_3.html","excerpt":"","text":"Selector 是Java NIO的重要组件之一，它可以检查一个或多个通道（Channel），并确定哪个通道进入准备好的状态，比如：准备读或者准备写数据。通过Selector 这种方式，单个线程可以管理多个通道，从而实现处理多个网络连接。 为什么用选择器(Selector)？仅使用单个线程来处理多个通道的优点是，可以需要较少的线程来处理通道。实际上，你可以只使用一个线程来处理所有的通道。对于操作系统来说，在线程之间切换成本很高，而且每个线程都占用了操作系统中的一些资源(内存)。因此，使用的线程越少越好。 现代操作系统和CPU在多任务处理方面变得越来越好，因此随着时间的推移，多线程的开销变得越来越小。如果一个CPU有多个内核，那么如果不同时处理多个任务，反而会浪费CPU资源。但是请了解，设计问题不属于本文讨论范畴。在这里，你只需要了解，使用一个选择器，你可以通过一个线程来处理多个通道。 下图是一个线程处理3个通道的示意图： 创建选择器通过调用selector. open()方法创建一个选择器，如下所示: Selector selector = Selector.open(); 使用选择器的注册通道为了让选择器使用通道，你必须在选择器上注册通道。你可以使用SelectableChannel.register() 方法来注册，如下： channel.configureBlocking(false); SelectionKey key = channel.register(selector, SelectionKey.OP_READ); 注意， 只有非阻塞模式的通道才可以使用选择器。这意味着，因为 FileChannel 不能切换成非阻塞模式，所以不能使用选择器。Socket的通道 则可以正常工作。 注意 register() 方法的第二个参数。这是一个“兴趣集”，意思是你想通过选择器在通道中监听的事件。你可以监听下面四个不同的事件： ConnectAcceptReadWrite 一个通道“触发一个事件”称之为“就绪”。所以，一个通道成功连接到另一台服务器，这里称之为“连接（Connect）就绪”；服务器套接字通道（ socket channel ）接收了来自客户端的连接，称之为“接收（Accept）就绪”；一个可以读取数据的通道是“读（Read）”就绪；一个可以写数据的通道是“写（Write）”就绪。 这四个事件由四个 SelectionKey 常量表示: SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 如果你对不止一个事件感兴趣，可以通过 “OR” 把常量放在一起，像这样： int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; SelectionKey正如你在上一节中看到的，当您使用选择器注册一个通道时，register()方法将返回一个SelectionKey对象。这个SelectionKey对象包含一些有趣的属性： 兴趣集 interest set 就绪集 ready set 通道Channel 选择器Selector一个连接对象(可选) Interest Set兴趣集是你感兴趣的“选择”的事件集合，如“使用选择器的注册通道”中所描述的，你可以通过下面这样的写法来判断趣集： int interestSet = selectionKey.interestOps(); boolean isInterestedInAccept = interestSet &amp; SelectionKey.OP_ACCEPT; boolean isInterestedInConnect = interestSet &amp; SelectionKey.OP_CONNECT; boolean isInterestedInRead = interestSet &amp; SelectionKey.OP_READ; boolean isInterestedInWrite = interestSet &amp; SelectionKey.OP_WRITE; Ready Set已就绪的集合是通道已准备就绪的操作集合。在“选择（selection）”之后，你将主要访问已就绪的集合。“选择（selection）” 将在后面的部分中解释。你可以像这样访问就绪集： int readySet = selectionKey.readyOps(); 你可以用类似判断兴趣集的方式来判断哪个渠道的事件或操作已经就绪，也可以通过下面的四个方法来判断： selectionKey.isAcceptable(); selectionKey.isConnectable(); selectionKey.isReadable(); selectionKey.isWritable(); 获取 Channel 和 SelectorChannel channel = selectionKey.channel(); Selector selector = selectionKey.selector(); 附加对象你可以将一个对象附加到SelectionKey，这是用于识别特定的通道的简便方法，或者将更多的信息附加到通道上。例如，你可以附加使用通道的缓冲区，或者一个包含更多聚合数据的对象。下面是如何附加对象： selectionKey.attach(theObject); Object attachedObj = selectionKey.attachment(); 你还可以在使用选择器注册通道的时候，同时附加一个对象。像下面这样： SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); 通过选择器（Selector）选择通道一旦你使用选择器（Selector）注册了一个或多个通道，你就可以调用Selector中的 select() 方法。这个方法返回对你所感兴趣的事件(connect, accept, read or write) 已经就绪的通道。比如，如果你对已经准备好read的通道感兴趣，那么你将可以接收准备好read的通道。 select()方法有三个： int select() //阻塞，直到至少一个通道的事件且是你注册的事件就绪 int select(long timeout) // 与select() 一样阻塞，直到达到了超时的时间（毫秒数）为止 int selectNow()//不阻塞，它会立即返回任何准备好的通道 select()方法返回的 int 表示多少通道准备好了。也就是说，自上次调用select()以来，有多少通道已经准备好了。如果你调用select()，它返回1，表示一个通道已经准备好了。如果你再次调用select()一次，此时另外一个通道已经就绪，它将再次返回1。如果你没有处理第一次调用select()时准备好的通道，那么此时实际上有两个就绪通道，但是每次select()调用，只有一个通道已经准备好了。 selectedKeys()一旦你调用 select() 方法返回大于0，表明一个或多个通道已经就绪，你就可以通过调用选择器的selectedKeys()方法，得到“SelectionKey”集合，通过集合访问就绪的通道。代码： Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); 当你使用选择器注册一个通道时，Channel.register()方法将返回一个SelectionKey对象。这个SelectionKey对象就代表了该选择器注册的一个通道。你可以通过selectedKeySet()方法得到这些SelectionKey。例子： Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. SocketChannel client = (SocketChannel) key.channel(); &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove(); &#125; 注意在每次迭代结束时调用 keyIterator.remove() 。选择器不会从SelectionKey集合删除SelectionKey实例。当你处理通道时，你需要手动这样做。当通道再次变为其他“就绪”状态时，选择器会再次将它（Channel）添加到所选的键集中。 调用SelectionKey.channel()方法返回通道。你需要自己转换成实际的类型使用，比如：ServerSocketChannel 或 SocketChannel 。 wakeUp()如果一个线程调用了select() 方法，这个线程阻塞了。那么你可以通过另外一个线程调用Selector.wakeup() 方法，来使这个阻塞的线程立刻返回，即使还没有就绪的Channel事件。 如果一个线程调用了Selector.wakeup() 方法，且当前没有其他因为调用select() 方法而阻塞的线程。那么当下一次某个线程调用select()方法的时候，它将立即返回。 close()当选择器调用 close()，选择器所以的SelectionKey 将失效，对应的渠道则不会关闭。 完整的示例代码： Selector selector = Selector.open(); channel.configureBlocking(false); SelectionKey key = channel.register(selector, SelectionKey.OP_READ); while(true) &#123; int readyChannels = selector.select(); if(readyChannels == 0) continue; Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove(); &#125; &#125;","categories":[{"name":"Java NIO","slug":"Java-NIO","permalink":"https://blog.fenxiangz.com/categories/Java-NIO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"NIO","slug":"NIO","permalink":"https://blog.fenxiangz.com/tags/NIO/"},{"name":"Selector","slug":"Selector","permalink":"https://blog.fenxiangz.com/tags/Selector/"}]},{"title":"Java NIO(2) Buffer","slug":"java/nio/2017-04-22_java_nio_2","date":"2017-04-22T00:00:00.000Z","updated":"2020-12-20T16:47:02.968Z","comments":true,"path":"post/java/nio/2017-04-22_java_nio_2.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/nio/2017-04-22_java_nio_2.html","excerpt":"","text":"Buffer 本质上是内存区域上的一块你可以读写的内存块。这个内存块被包在 NIO Buffer对象中，我们通过 Buffer 提供的一系列方法来便捷的操作这个内存块。 Buffer 的基本使用Buffer 的使用有下面四个典型的过程： 写数据到 Buffer 调用 buffer.flip() 从 Buffer 读取数据 调用 buffer.clear() 或 buffer.compact() 当往 Buffer 里面写数据的时候，Buffer 对象会跟踪写了多少数据。当你需要读取数据的时候，你需要调用 flip() 方法，将写模式转换成读模式。当读取完数据，需要调用clear() 或compact() 方法来清楚buffer里的数据，才可以重新进行写数据。 clear和compact区别：clear() 方法清空buffer里面的全部数据，写数据的时候从0开始写；compact()方法会清空已读的数据，将未读的数据移动到buffer开始处，写数据的时候，从未读数据之后开始写； 下面是一个简单的例子: RandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;); FileChannel inChannel = aFile.getChannel(); //create buffer with capacity of 48 bytes ByteBuffer buf = ByteBuffer.allocate(48); int bytesRead = inChannel.read(buf); //read into buffer. while (bytesRead != -1) &#123; buf.flip(); //make buffer ready for read while(buf.hasRemaining())&#123; System.out.print((char) buf.get()); // read 1 byte at a time &#125; buf.clear(); //make buffer ready for writing bytesRead = inChannel.read(buf); &#125; aFile.close(); Buffer 容量（capacity）、位置（position）和限制（limit）容量：论buffer在读模式还是写模式，都是一样的，即buffer可容纳的数据量；位置和限制：由buffer写模式或读模式决定。 如下图所示： 当调用 flip()，buffer从写模式切换为读模式，position为0，limit为可读取的最多的数据量；当调用clear()，buffer从读模式切换为写模式，position为0，limit为容量大小；当调用compact()，buffer从读模式切换为写模式，position为上次未读数据大小，limit位容量大小； 感受一下： public static void main(String[] args) throws IOException &#123; IntBuffer buf = IntBuffer.allocate(20); System.out.println(&quot;----init----&quot;); printParameters(buf); System.out.println(&quot;----put 1----&quot;); buf.put(1); printParameters(buf); System.out.println(&quot;----put 2 3 4 5----&quot;); buf.put(2); buf.put(3); buf.put(4); buf.put(5); printParameters(buf); System.out.println(&quot;----flip()----&quot;); buf.flip(); printParameters(buf); System.out.println(&quot;----call get()----&quot;); System.out.print(&quot;read: &quot;); for (int i=0;i &lt;= buf.limit() - 1; i++)&#123; System.out.print(buf.get()+&quot; &quot;); &#125; System.out.println(); printParameters(buf); System.out.println(&quot;----call flip() and get() 3 times----&quot;); buf.flip(); System.out.print(&quot;read: &quot;); for (int i=0;i &lt; 3; i++)&#123; System.out.print(buf.get()+&quot; &quot;); &#125; System.out.println(); printParameters(buf); System.out.println(&quot;----call compact()----&quot;); buf.compact(); printParameters(buf); &#125; private static void printParameters(IntBuffer buf) &#123; System.out.println(&quot;limit:&quot; + buf.limit()); System.out.println(&quot;position:&quot; + buf.position()); System.out.println(&quot;capacity:&quot; + buf.capacity()); &#125; 输出： ----init---- limit:20 position:0 capacity:20 ----put 1---- limit:20 position:1 capacity:20 ----put 2 3 4 5---- limit:20 position:5 capacity:20 ----flip()---- limit:5 position:0 capacity:20 ----call get()---- read: 1 2 3 4 5 limit:5 position:5 capacity:20 ----call flip() and get() 3 times---- read: 1 2 3 limit:5 position:3 capacity:20 ----call compact() and read 1 2 3---- limit:20 position:2 capacity:20 NIO Buffer类型包括： ByteBufferMappedByteBufferCharBufferDoubleBufferFloatBufferIntBufferLongBufferShortBuffer 顾名思义，允许你使用不同类型的Buffer，操作不同类型的数据。 分配Buffer大小： ByteBuffer buf = ByteBuffer.allocate(48); CharBuffer buf2 = CharBuffer.allocate(1024); 写数据到Buffer： 1、通过Channel： int bytesRead = inChannel.read(buf); //read into buffer. 2、通过put()方法： buf.put(127); 从Buffer读数据： 1、通过Channel: int bytesWritten = inChannel.write(buf); 2、 通过get()方法： byte aByte = buf.get(); rewind()只重置position为0，不改变limit。 rewind，flip， clear 源码对比，可以简单感受一下： public final Buffer rewind() &#123; position = 0; mark = -1; return this; &#125; public final Buffer flip() &#123; limit = position; position = 0; mark = -1; return this; &#125; public final Buffer clear() &#123; position = 0; limit = capacity; mark = -1; return this; &#125; mark() and reset()mark() 标记一个位置，再次调用reset()的时候，position回到mark()标记的位置。 源码： public final Buffer mark() &#123; mark = position; return this; &#125; public final Buffer reset() &#123; int m = mark; if (m &lt; 0) throw new InvalidMarkException(); position = m; return this; &#125;","categories":[{"name":"Java NIO","slug":"Java-NIO","permalink":"https://blog.fenxiangz.com/categories/Java-NIO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"NIO","slug":"NIO","permalink":"https://blog.fenxiangz.com/tags/NIO/"},{"name":"Buffer","slug":"Buffer","permalink":"https://blog.fenxiangz.com/tags/Buffer/"}]},{"title":"Java NIO (1) 基本概念","slug":"java/nio/2017-04-21_java_nio_1","date":"2017-04-21T00:00:00.000Z","updated":"2020-12-20T16:47:02.968Z","comments":true,"path":"post/java/nio/2017-04-21_java_nio_1.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/nio/2017-04-21_java_nio_1.html","excerpt":"","text":"Java 1.4之后引入了NIO框架。 Java NIO: Channels and Buffers（通道和缓冲区）标准的IO是在字节流和字符流进行操作。NIO 是在通道（Channel）和缓冲区（Buffer）进行操作。数据总是从通道读取到缓冲区里，或者从缓冲区写入到通道里。 Java NIO: Asynchronous IO（异步IO）NIO可以做异步IO，例如：当线程从通道读取数据到缓冲区时，线程还是可以进行其他事情。当数据被写入到缓冲区时，线程可以继续处理它。从缓冲区写入通道也类似。 Java NIO: Selectors（选择器）Java NIO包含了选择器的概念。选择器用于监听多个通道的事件（比如：连接打开，数据达到）。因此，单个的线程可以监听多个数据通道。 所以 Java NIO 核心组件包括： ChannelsBuffersSelectors 除了以上列出来的，还有一些其他的组件，比如Pipe 、FileLock等组件类 ，这些类在使用的过程中，都会结合上面列出的三个核心组件类来使用。 ChannelsNIO里的 Channel 类似一个IO流， 数据通过Buffer可以写到Channel，通过Channel读取数据到Buffer。 Channel 实现类有： FileChannel —— 读写文件数据DatagramChannel —— 通过UDP读写网络上的数据SocketChannel —— 通过TCP读写网络上的数据ServerSocketChannel —— 监听TCP连接，为每一个新的请求连接创建一个SocketChannel 这些实现覆盖了 UDP + TCP 网络 IO 和文件 IO。 使用FileChannel读文件数据例子： 12345678910111213141516171819RandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;);FileChannel inChannel = aFile.getChannel();ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf);while (bytesRead != -1) &#123; System.out.println(&quot;Read &quot; + bytesRead); buf.flip();//从写模式切换到读模式，后续会有详解 while(buf.hasRemaining())&#123; System.out.print((char) buf.get()); &#125; buf.clear();//清空buf里面的缓冲数据 bytesRead = inChannel.read(buf);&#125;aFile.close(); BuffersBuffer 实现类有： ByteBufferCharBufferDoubleBufferFloatBufferIntBufferLongBufferShortBuffer 这些实现累覆盖了Java基本数据类型：byte, short, int, long, float, double 和 char。我们可以使用不同的Buffer来传递不同类型的数据。 另外还有，MappedByteBuffer 这个实现类，它是ByteBuffer的子类。具体用法后续给出单独的文章。 Channels和IO流不同点：支持读和写、支持异步读写、总是从Buffers读取数据或写入数据到Buffers。 Selectors一个线程可以通过Selector来管理多个Channel对象。如果你的应用程序打开了多个连接（Channels），处理起来是非常方便的。例如下面的图示，一个线程，处理3个Channel。 使用Selector，需要先把Channel注册到Selector上，然后调用select() 方法，这个方法调用后将阻塞，直到注册到Selector上的Channel有一个事件准备就绪，例如：一个连接接入或收到了数据等，这时就可以对这个事件进行后续的处理了。","categories":[{"name":"Java NIO","slug":"Java-NIO","permalink":"https://blog.fenxiangz.com/categories/Java-NIO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"NIO","slug":"NIO","permalink":"https://blog.fenxiangz.com/tags/NIO/"}]},{"title":"Java IO(6) InputStream和OutputStream","slug":"java/io/2017-04-20_java_io_6_stream","date":"2017-04-20T00:00:00.000Z","updated":"2020-12-20T16:47:02.966Z","comments":true,"path":"post/java/io/2017-04-20_java_io_6_stream.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/io/2017-04-20_java_io_6_stream.html","excerpt":"","text":"InputStream例子： InputStream inputstream = new FileInputStream(&quot;c:\\\\data\\\\input-text.txt&quot;); int data = inputstream.read(); while(data != -1) &#123; //do something with data... doSomethingWithData(data); data = inputstream.read(); &#125; inputstream.close(); Java7 try-with-resources 写法： try( InputStream inputstream = new FileInputStream(&quot;file.txt&quot;) ) &#123; int data = inputstream.read(); while(data != -1)&#123; System.out.print((char) data); data = inputstream.read(); &#125; &#125; read()返回 int类型，一个字节值。返回 -1 表示没有读取到字节值，流结束。如下： int data = inputstream.read(); 字节可以转为字符，像这样： char aChar = (char) data; 有些子类实现了可以替代read的方法，比如：DataInputStream ，你可以通过调用：readBoolean() 、 readDouble()…… 来直接读取Java的基础类型数据，而无需读取原生的字节码。 read(byte[])两个方法： int read(byte[])int read(byte[], int offset, int length) 通过读到数组的方式来替代 read()，效率会更高。这两个方法返回读取到的字节个数，同样，返回-1表示结束，没有读取到字节。 read(byte[]) 这个方法会尝试尽可能多的读取byte[]长度的字节个数，放入其中。每次读取数据，从数组的0处开始填充，直到没有数据或数组被填满。 如果未读满数组，其剩余空间将存储上一次读取的数据。所以在循环读取的时，要注意最后一次读取的内容长度，做响应的截取处理。 read(byte[], int offset, int length)这个方法功能类型，只不过每次都指定了读取的数据从数组的offset开始填充，最多填充length个字节数。注意数组越界异常。 mark() 和 reset()mark()和 reset() 需要配合使用，mark()做标记，reset() 被调用后，再次read，流将从上次mark()的位置来算重新读取流数据。功能常于解析流数据。 这个接口需要子类实现，如果子类实现了这个功能，需要重写 markSupported() 方法，返回true；否则返回false。 OutputStreamwrite(byte)例子： OutputStream output = new FileOutputStream(&quot;c:\\\\data\\\\output-text.txt&quot;); while(hasMoreData()) &#123; int data = getMoreData(); output.write(data); &#125; output.close(); write(byte[]) write(byte[] bytes)write(byte[] bytes, int offset, int length) 与read参数一致。 flush()OutputStream调用write写的数据，可能未写入磁盘，调用该方法，将已经写入到OutputStream的数据刷新到磁盘中。 close()例子： OutputStream output = null; try&#123; output = new FileOutputStream(&quot;c:\\\\data\\\\output-text.txt&quot;); while(hasMoreData()) &#123; int data = getMoreData(); output.write(data); &#125; &#125; finally &#123; if(output != null) &#123; output.close(); &#125; &#125; 异常处理：http://tech.fenxiangz.com/topic/55/java-io-5-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86","categories":[{"name":"Java IO","slug":"Java-IO","permalink":"https://blog.fenxiangz.com/categories/Java-IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"},{"name":"Stream","slug":"Stream","permalink":"https://blog.fenxiangz.com/tags/Stream/"}]},{"title":"Java IO(5) 异常处理","slug":"java/io/2017-04-19_java_io_5_exception","date":"2017-04-19T00:00:00.000Z","updated":"2020-12-20T16:47:02.966Z","comments":true,"path":"post/java/io/2017-04-19_java_io_5_exception.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/io/2017-04-19_java_io_5_exception.html","excerpt":"","text":"传统的写法： InputStream input = null; try &#123; input = new FileInputStream(&quot;c:\\\\data\\\\input-text.txt&quot;); int data = input.read(); while (data != -1) &#123; //do something with data... doSomethingWithData(data); data = input.read(); &#125; &#125; catch (IOException e) &#123; //do something with e... log, perhaps rethrow etc. &#125; finally &#123; try &#123; if (input != null) input.close(); &#125; catch (IOException e) &#123; //do something, or ignore. &#125; &#125; 使用模板方法： public abstract class InputStreamProcessingTemplate &#123; public void process(String fileName) &#123; IOException processException = null; InputStream input = null; try &#123; input = new FileInputStream(fileName); doProcess(input); &#125; catch (IOException e) &#123; processException = e; &#125; finally &#123; if (input != null) &#123; try &#123; input.close(); &#125; catch (IOException e) &#123; if (processException != null) &#123; throw new MyException(processException, e, &quot;Error message...&quot; + fileName); &#125; else &#123; throw new MyException(e, &quot;Error closing InputStream for file &quot; + fileName; &#125; &#125; &#125; if (processException != null) &#123; throw new MyException(processException, &quot;Error processing InputStream for file &quot; + fileName; &#125; &#125; //override this method in a subclass, to process the stream. public abstract void doProcess(InputStream input) throws IOException; &#125; //调用 new InputStreamProcessingTemplate()&#123; public void doProcess(InputStream input) throws IOException&#123; int inChar = input.read(); while(inChar !- -1)&#123; //do something with the chars... &#125; &#125; &#125;.process(&quot;someFile.txt&quot;); 使用接口实现： public interface InputStreamProcessor &#123; public void process(InputStream input) throws IOException; &#125; public class InputStreamProcessingTemplate &#123; public void process(String fileName, InputStreamProcessor processor)&#123; IOException processException = null; InputStream input = null; try&#123; input = new FileInputStream(fileName); processor.process(input); &#125; catch (IOException e) &#123; processException = e; &#125; finally &#123; if(input != null)&#123; try &#123; input.close(); &#125; catch(IOException e)&#123; if(processException != null)&#123; throw new MyException(processException, e, &quot;Error message...&quot; + fileName; &#125; else &#123; throw new MyException(e, &quot;Error closing InputStream for file &quot; + fileName); &#125; &#125; &#125; if(processException != null)&#123; throw new MyException(processException, &quot;Error processing InputStream for file &quot; + fileName; &#125; &#125; &#125; new InputStreamProcessingTemplate() .process(&quot;someFile.txt&quot;, new InputStreamProcessor()&#123; public void process(InputStream input) throws IOException&#123; int inChar = input.read(); while(inChar !- -1)&#123; //do something with the chars... &#125; &#125; &#125;); 改成静态方法： public class InputStreamProcessingTemplate &#123; public static void process(String fileName, InputStreamProcessor processor)&#123; IOException processException = null; InputStream input = null; try&#123; input = new FileInputStream(fileName); processor.process(input); &#125; catch (IOException e) &#123; processException = e; &#125; finally &#123; if(input != null)&#123; try &#123; input.close(); &#125; catch(IOException e)&#123; if(processException != null)&#123; throw new MyException(processException, e, &quot;Error message...&quot; + fileName); &#125; else &#123; throw new MyException(e, &quot;Error closing InputStream for file &quot; + fileName; &#125; &#125; &#125; if(processException != null)&#123; throw new MyException(processException, &quot;Error processing InputStream for file &quot; + fileName; &#125; &#125; &#125; InputStreamProcessingTemplate.process(&quot;someFile.txt&quot;, new InputStreamProcessor()&#123; public void process(InputStream input) throws IOException&#123; int inChar = input.read(); while(inChar !- -1)&#123; //do something with the chars... &#125; &#125; &#125;);","categories":[{"name":"Java IO","slug":"Java-IO","permalink":"https://blog.fenxiangz.com/categories/Java-IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"},{"name":"Exception","slug":"Exception","permalink":"https://blog.fenxiangz.com/tags/Exception/"}]},{"title":"Java Thread 线程知识整理（思维导图）","slug":"java/basic/2017-04-18_java_thread_mind","date":"2017-04-18T10:51:24.000Z","updated":"2020-12-20T16:47:02.964Z","comments":true,"path":"post/java/basic/2017-04-18_java_thread_mind.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/basic/2017-04-18_java_thread_mind.html","excerpt":"","text":"整理一份Java Thread的思维导图，持续更新中… https://blog.fenxiangz.com/mind/?url=examples%2FJavaThread.mymind","categories":[{"name":"Java 基础","slug":"Java-基础","permalink":"https://blog.fenxiangz.com/categories/Java-%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"Java线程","slug":"Java线程","permalink":"https://blog.fenxiangz.com/tags/Java%E7%BA%BF%E7%A8%8B/"},{"name":"思维导图","slug":"思维导图","permalink":"https://blog.fenxiangz.com/tags/%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/"}]},{"title":"为什么要与上0xff（&0xff），有什么意义？","slug":"java/advance/2017-04-18_java_0xff","date":"2017-04-18T00:00:00.000Z","updated":"2020-12-20T16:47:02.958Z","comments":true,"path":"post/java/advance/2017-04-18_java_0xff.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2017-04-18_java_0xff.html","excerpt":"","text":"java.io.PipedInputStream#receive(int) 源码： protected synchronized void receive(int b) throws IOException &#123; checkStateForReceive(); writeSide = Thread.currentThread(); if (in == out) awaitSpace(); if (in &lt; 0) &#123; in = 0; out = 0; &#125; buffer[in++] = (byte)(b &amp; 0xFF); if (in &gt;= buffer.length) &#123; in = 0; &#125; &#125; 先复习一下，原码反码补码这三个概念：对于正数（00000001）原码来说，首位表示符号位，反码 补码都是本身；对于负数（100000001）原码来说，反码是对原码除了符号位之外作取反运算即（111111110），补码是对反码作+1运算即（111111111）。 看一个 demo： public class Test &#123; public static void main(String[] args) &#123; byte[] a = new byte[10]; a[0]= -127; System.out.println(a[0]); int c = a[0]&amp;0xff; System.out.println(c); &#125; &#125; 输出： -127129 当将 -127 赋值给 a[0] 时候，a[0] 作为一个 byte 类型，其计算机存储的补码是10000001（8位）。 将 a[0] 作为 int 类型向控制台输出的时候，JVM 作了一个补位的处理，因为 int 类型是 32 位所以补位后的补码就是 1111111111111111111111111 10000001（32位），这个 32 位二进制补码表示的也是 -127。 发现没有，虽然byte-&gt;int计算机背后存储的二进制补码由 10000001（8位）转化成了1111111111111111111111111 10000001（32位）很显然这两个补码表示的十进制数字依然是相同的。 但是我做 byte -&gt; int 的转化，所有时候都只是为了保持 十进制的一致性吗？ 不一定吧？好比我们拿到的文件流转成byte数组，难道我们关心的是byte数组的十进制的值是多少吗？我们关心的是其背后二进制存储的补码吧。所以大家应该能猜到为什么byte类型的数字要 &amp;0xff 再赋值给 int 类型，其本质原因就是想保持二进制补码的一致性。 当byte要转化为int的时候，高的24位必然会补1，这样，其二进制补码其实已经不一致了，&amp;0xff 可以将高的24位置为0，低8位保持原样。这样做的目的就是为了保证二进制数据的一致性。 当然拉，保证了二进制数据性的同时，如果二进制被当作 byte 和 int 来解读，其10进制的值必然是不同的，因为符号位位置已经发生了变化。 象例2中，int c = a[0]&0xff; a[0]&amp;0xff=1111111111111111111111111 10000001&amp;11111111=000000000000000000000000 10000001 ，这个值算一下就是129， 所以 c 的输出的值就是129。有人问为什么上面的式子中a[0]不是8位而是32位，因为当系统检测到 byte 可能会转化成 int 或者说 byte 与 int 类型进行运算的时候，就会将byte的内存空间高位补1（也就是按符号位补位）扩充到32位，再参与运算。上面的 0xff 其实是 int 类型的字面量值，所以可以说 byte 与 int 进行运算。 解释来源：http://www.cnblogs.com/think-in-java/p/5527389.html","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"二进制","slug":"二进制","permalink":"https://blog.fenxiangz.com/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6/"},{"name":"反码","slug":"反码","permalink":"https://blog.fenxiangz.com/tags/%E5%8F%8D%E7%A0%81/"},{"name":"补码","slug":"补码","permalink":"https://blog.fenxiangz.com/tags/%E8%A1%A5%E7%A0%81/"}]},{"title":"Java IO(4) 替换流，数组，文件或者大的字符串的一种方式","slug":"java/io/2017-04-18_java_io_4_reader","date":"2017-04-18T00:00:00.000Z","updated":"2020-12-20T16:47:02.966Z","comments":true,"path":"post/java/io/2017-04-18_java_io_4_reader.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/io/2017-04-18_java_io_4_reader.html","excerpt":"","text":"传统的字符替换，我们通常会想到使用 String.replace()，但是这个有些问题。String.replace()每次替换，都会产生新的字符串，替换5次就产生5个新的字符串。这样空间复杂度就是 O(N*M)。N是字符串大小，M是替换的次数。这种情况下，如果是替换数据量大的字符串就会有内存问题。同样，这种方式也不利于扩展。 这篇文章通过使用自己实现的 TokenReplacingReader 来解决这个问题。 TokenReplacingReader 从标准的 Java.io.Reader 中读取字符数据（继承），所有可以使用 Reader的地方，就都可以使用 TokenReplacingReader 。 接着我们需要一个Token解析接口，用来解析替换字符：ITokenResolver ，整体关系图如下： TokenReplacingReader 字符串替换用法： public static void main(String[] args) throws IOException &#123; Map&lt;String, String&gt; tokens = new HashMap&lt;String, String&gt;(); tokens.put(&quot;token1&quot;, &quot;value1&quot;); tokens.put(&quot;token2&quot;, &quot;JJ ROCKS!!!&quot;); MapTokenResolver resolver = new MapTokenResolver(tokens); Reader source = new StringReader(&quot;1234567890$&#123;token1&#125;abcdefg$&#123;token2&#125;XYZ$000&quot;); Reader reader = new TokenReplacingReader(source, resolver); int data = reader.read(); while(data != -1)&#123; System.out.print((char) data); data = reader.read(); &#125; &#125; TokenReplacingReader 流替换，文件替换，字符数组替换以及字符串替换，就可以改成如下的方式： ITokenResolver resolver = ... ; // get ITokenResolver instance. Reader reader = new TokenReplacingReader( new InputStreamReader(inputStream), resolver); Reader reader = new TokenReplacingReader( new FileReader(new File(&quot;c:\\\\file.txt&quot;), resolver); Reader reader = new TokenReplacingReader( new CharArrayReader(charArray), resolver); Reader reader = new TokenReplacingReader( new StringReader(&quot;biiig string....&quot;), resolver); ITokenResolver 实现： public interface ITokenResolver &#123; public String resolveToken(String tokenName); &#125; ITokenResolver 实现： import java.util.HashMap; import java.util.Map; public class MapTokenResolver implements ITokenResolver &#123; protected Map&lt;String, String&gt; tokenMap = new HashMap&lt;String, String&gt;(); public MapTokenResolver(Map&lt;String, String&gt; tokenMap) &#123; this.tokenMap = tokenMap; &#125; public String resolveToken(String tokenName) &#123; return this.tokenMap.get(tokenName); &#125; &#125; TokenReplacingReader实现： import java.io.IOException; import java.io.PushbackReader; import java.io.Reader; import java.nio.CharBuffer; public class TokenReplacingReader extends Reader &#123; protected PushbackReader pushbackReader = null; protected ITokenResolver tokenResolver = null; protected StringBuilder tokenNameBuffer = new StringBuilder(); protected String tokenValue = null; protected int tokenValueIndex = 0; public TokenReplacingReader(Reader source, ITokenResolver resolver) &#123; this.pushbackReader = new PushbackReader(source, 2); this.tokenResolver = resolver; &#125; public int read(CharBuffer target) throws IOException &#123; throw new RuntimeException(&quot;Operation Not Supported&quot;); &#125; public int read() throws IOException &#123; if (this.tokenValue != null) &#123; if (this.tokenValueIndex &lt; this.tokenValue.length()) &#123; return this.tokenValue.charAt(this.tokenValueIndex++); &#125; if (this.tokenValueIndex == this.tokenValue.length()) &#123; this.tokenValue = null; this.tokenValueIndex = 0; &#125; &#125; int data = this.pushbackReader.read(); if (data != &#39;$&#39;) return data; data = this.pushbackReader.read(); if (data != &#39;&#123;&#39;) &#123; this.pushbackReader.unread(data); return &#39;$&#39;; &#125; this.tokenNameBuffer.delete(0, this.tokenNameBuffer.length()); data = this.pushbackReader.read(); while (data != &#39;&#125;&#39;) &#123; this.tokenNameBuffer.append((char) data); data = this.pushbackReader.read(); &#125; this.tokenValue = this.tokenResolver .resolveToken(this.tokenNameBuffer.toString()); if (this.tokenValue == null) &#123; this.tokenValue = &quot;$&#123;&quot; + this.tokenNameBuffer.toString() + &quot;&#125;&quot;; &#125; if (this.tokenValue.length() == 0) &#123; return read(); &#125; return this.tokenValue.charAt(this.tokenValueIndex++); &#125; public int read(char cbuf[]) throws IOException &#123; return read(cbuf, 0, cbuf.length); &#125; public int read(char cbuf[], int off, int len) throws IOException &#123; int charsRead = 0; for (int i = 0; i &lt; len; i++) &#123; int nextChar = read(); if (nextChar == -1) &#123; if (charsRead == 0) &#123; charsRead = -1; &#125; break; &#125; charsRead = i + 1; cbuf[off + i] = (char) nextChar; &#125; return charsRead; &#125; public void close() throws IOException &#123; this.pushbackReader.close(); &#125; public long skip(long n) throws IOException &#123; throw new RuntimeException(&quot;Operation Not Supported&quot;); &#125; public boolean ready() throws IOException &#123; return this.pushbackReader.ready(); &#125; public boolean markSupported() &#123; return false; &#125; public void mark(int readAheadLimit) throws IOException &#123; throw new RuntimeException(&quot;Operation Not Supported&quot;); &#125; public void reset() throws IOException &#123; throw new RuntimeException(&quot;Operation Not Supported&quot;); &#125; &#125; 原文：http://tutorials.jenkov.com/java-howto/replace-strings-in-streams-arrays-files.html","categories":[{"name":"Java IO","slug":"Java-IO","permalink":"https://blog.fenxiangz.com/categories/Java-IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"}]},{"title":"Java IO(3)  System.in , System.out,  and System.error","slug":"java/io/2017-04-17_java_io_3_system_in","date":"2017-04-17T00:00:00.000Z","updated":"2020-12-20T16:47:02.966Z","comments":true,"path":"post/java/io/2017-04-17_java_io_3_system_in.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/io/2017-04-17_java_io_3_system_in.html","excerpt":"","text":"System.out, System.in 和 System.err 在源码里的定义是： public final static InputStream in = null; public final static PrintStream out = null; public final static PrintStream err = null; System. in 用于控制台标准输入；System.out 用于控制台标准输出；System.error 用于控制台标准错误输出（有些 IDE 执行程序时，会显示红色字体，比如 Eclipse）。 System.out + System.err 使用例子: try &#123; InputStream input = new FileInputStream(&quot;c:\\\\data\\\\...&quot;); System.out.println(&quot;File opened...&quot;); &#125; catch (IOException e)&#123; System.err.println(&quot;File opening failed:&quot;); e.printStackTrace(); &#125; System. in , System.out, and System.error 可以使用 System.setIn(), System.setOut() 或 System.setErr() 改变标准输出，比如： OutputStream output = new FileOutputStream(&quot;c:\\\\data\\\\system.out.txt&quot;); PrintStream printOut = new PrintStream(output); System.setOut(printOut); 这样，System.out 将输出到指定的文件。 注意：in， out 和 error 使用了 final 修饰，所以像 “System.out = myOut” 这样直接赋值是编译不通过的，但是可以通过 setXXX 重新赋值是因为 System 里面的 setXXX 是通过 native 方法实现的，具体可以参考源码以及链接：http://stackoverflow.com/questions/5951464/java-final-system-out-system-in-and-system-err 。","categories":[{"name":"Java IO","slug":"Java-IO","permalink":"https://blog.fenxiangz.com/categories/Java-IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"}]},{"title":"Java IO(2) 管道流","slug":"java/io/2017-04-16_java_io_2_pipeline","date":"2017-04-16T00:00:00.000Z","updated":"2020-12-20T16:47:02.965Z","comments":true,"path":"post/java/io/2017-04-16_java_io_2_pipeline.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/io/2017-04-16_java_io_2_pipeline.html","excerpt":"","text":"Demo: import java.io.IOException; import java.io.PipedInputStream; import java.io.PipedOutputStream; public class PipeExample &#123; public static void main(String[] args) throws IOException &#123; final PipedOutputStream output = new PipedOutputStream(); final PipedInputStream input = new PipedInputStream(output); Thread thread1 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; output.write(&quot;Hello world, pipe!&quot;.getBytes()); &#125; catch (IOException e) &#123; &#125; &#125; &#125;); Thread thread2 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; int data = input.read(); while(data != -1)&#123; System.out.print((char) data); data = input.read(); &#125; &#125; catch (IOException e) &#123; &#125; &#125; &#125;); thread1.start(); thread2.start(); &#125; &#125; PipedOutputStream 和 PipedInputStream ，用于两个线程间通信。可以将管道输出流连接到管道输入流来创建通信管道。管道输出流是管道的发送端。通常，数据由某个线程写入 PipedOutputStream 对象，并由其他线程从连接的 PipedInputStream 读取。不建议对这两个对象尝试使用单个线程，因为这样可能会造成该线程死锁。如果某个线程正从连接的管道输入流中读取数据字节，但该线程不再处于活动状态，则该管道被视为处于 毁坏 状态。 PipedInputStream 和 PipedOutputStream 都有一个方法 connect()，用于连接另一个输入或输出管道，如果连接一个已连接（connected）的管道流，connect() 将抛出异常：java.io.IOException: Already connected。 PipedInputStream 中实际是用了一个1024字节固定大小的循环缓冲区。写入PipedOutputStream 的数据实际上保存到对应的 PipedInputStream 的内部缓冲区。从 PipedInputStream 执行读操作时，读取的数据实际上来自这个内部缓冲区。如果对应的 PipedInputStream 输入缓冲区已满，任何企图写入 PipedOutputStream 的线程都将被阻塞。而且这个写操作线程将一直阻塞，直至出现读取 PipedInputStream 的操作从缓冲区删除数据。这也就是说往 PipedOutputStream 写数据的线程Send若是和从 PipedInputStream 读数据的线程 Receive 是同一个线程的话，那么一旦Send线程发送数据过多（大于 1024 字节），它就会被阻塞，这就直接导致接受数据的线程阻塞而无法工作（因为是同一个线程嘛），那么这就是一个典型的死锁现象，这也就是为什么 javadoc 中关于这两个类的使用时告诉大家要在不同线程环境下使用的原因了。 同一个 JVM 中，除了管道，还有很多其他的方式用于线程之间数据交换，通常会使用一个对象来进行数据交换。但是，如果需要使用字节数据在线程之间进行交换，使用管道的方式提供了这种可能。","categories":[{"name":"Java IO","slug":"Java-IO","permalink":"https://blog.fenxiangz.com/categories/Java-IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"}]},{"title":"Java IO(1) Overview","slug":"java/io/2017-04-15_java_io_1_overview","date":"2017-04-15T00:00:00.000Z","updated":"2020-12-20T16:47:02.965Z","comments":true,"path":"post/java/io/2017-04-15_java_io_1_overview.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/io/2017-04-15_java_io_1_overview.html","excerpt":"","text":"Java IO 支持特性 文件访问网络访问内存访问线程之间的访问 (Pipes 管道)缓冲过滤解析读写文本 (Readers / Writers)读写原始数据类型 (long, int 等)读写对象 Java IO 类一览 From ：http://tutorials.jenkov.com/java-io/overview.html","categories":[{"name":"Java IO","slug":"Java-IO","permalink":"https://blog.fenxiangz.com/categories/Java-IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"}]},{"title":"【战略收藏】Java知识体系","slug":"java/advance/2017-03-28_java_acknowledge","date":"2017-03-28T00:00:00.000Z","updated":"2020-12-20T16:47:02.957Z","comments":true,"path":"post/java/advance/2017-03-28_java_acknowledge.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2017-03-28_java_acknowledge.html","excerpt":"","text":"先看看这些程序员技能树，你掌握或了解哪些？OMG竟然有么多～～～震精！震精！！！ 这是从450家企业的招聘信息中统计而来，相对来说还是比较真实的，虽然有些公司的招聘要求万年不变，但还是可以大致反应企业的招聘要求的。 尽管Struts2漏洞频出，但是由于政府、银行以及传统企业遗留项目大部分还是采用Struts2的，所以还是占有一定市场，但绝壁不会增长。新兴互联网公司，一般来说主要是Spring家族居多，spring、spring Mvc以及Spring Boot 出现的频率较多。 从图中可以看出，分布式服务框架应用也是大部分企业招聘的必要条件了，阿里系的Dubbo名列前茅。相应的分布式应用程序协调服Zookeeper也出现在其中。 时下很流行的RESTful架构，准确的来说它是一种标准。也是很多企业考察的对象。 以下是出现次数超过100的一些技能，大家可以做一个参考。 Spring 299MySQL 290JavaScript 216Linux 165J2EE 151设计模式 148Struts2 138Hibernate 132Mybatis 130jQuery 128HTML 127TOMCAT 117iBatis 103CSS 103redis 102多线程 102dubbo 47 （绝壁不是打酱油的） 相信每个程序员、或者说每个工作者都应该有自己的职业规划，问一下自己对编程到底持有的是一种什么样的态度，是够用就好呢还是不断研究？ Spring 框架 JAVA核心技术总结 J2EE技术总结 工作和学习总结 大数据相关技术总结 社区昵称 happycc 的精彩回答：正在使用的Spring框架Spring框架是一个分层架构,有7个定义良好的模块组成spring模块构建在核心容器智之上, 核心容器定义了创建、 配置、和管理bean的方式组成spring框架的每个模块(或组件)都可以单独存在, 或者与其他一个或多个模块联合实现 模块如下: 1–核心容器核心容器提供spring框架的基本功能,核心容器的主要组件是BeanFactory, 他是工厂模式的实现.BeanFactory使用控制反转(IOC)模式将应用程序的配置和依赖性与实际的应用程序代码分开2–Spring上下文是一个配置文件,该配置文件向spring框架提供上下文信息3–Spring AOP通过配置管理特性,Spring AOP 模块直接将面向切面(方面)编程功能集成到spring框架中4–spring DAOJDBC DAO抽象层提供了有意义的已成层次结构, 可用该结构管理异常处理和不同数据库抛出的错误信息,极大的降低了异常代码数量5–Spring ORMspring框架插入了若干个ORM框架, 从而提供了ORM的对象工具,其中包括了Hibernate, Mybatis6–Spring Webweb上下文模块建立在应用程序上下文模块之上,为基于web的应用程序提供上下文7–Spring MVC该框架是一个全功能的构建web应用程序的MVC实现. 通过策略接口,MVC框架变成高度可配置的. MVC容纳了大量视图技术. 其中包括JSP、Velocity和POI Spring 框架的好处 spring是最大的工厂spring负责业务逻辑组件的框架和生成, 并管理业务逻辑组件的生命周期spring可以生产所有实例, 从控制器、 业务逻辑组件、 持久层组件Spring特点 1–降低了组件之间的耦合性, 实现了软件各个层之间的解耦2–可以使用spring容器提供的服务, 如: 事务管理, 消息服务3–容器提供单例模式支持4–容器提供AOP技术, 利用它很容易实现权限拦截, 运行期监控5–容器提供了众多的辅助类, 能加快应用的开发(org.springframework.jdbc.core.JDBCTemplate 等)6–spring对主流的应用框架提供了集成支持, 例如: hibernate,JPA, Struts, Mybatis(IBatis)7–Spring属于低侵入式设计, 代码污染度极低8–独立于各种应用服务器9–spring的DI机制降低了业务对象替换的复杂性10–spring的高度开发性, 并不强制应用完全依赖于spring, 开发者可以自由选择spring的部分或者全部 社区昵称 小崽崽 的精彩回答： PHP才是世界上最好的语言，看我大PHP技能树 https://zhuanlan.zhihu.com/p/26035486?from=timeline","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"知识体系","slug":"知识体系","permalink":"https://blog.fenxiangz.com/tags/%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"}]},{"title":"小白科普：Netty有什么用？","slug":"java/netty/2019-01-02_小白科普Netty有什么用","date":"2017-01-02T00:00:00.000Z","updated":"2020-12-20T16:47:02.966Z","comments":true,"path":"post/java/netty/2019-01-02_小白科普Netty有什么用.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/netty/2019-01-02_%E5%B0%8F%E7%99%BD%E7%A7%91%E6%99%AENetty%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8.html","excerpt":"","text":"随着移动互联网的爆发性增长，小明公司的电子商务系统访问量越来越大，由于现有系统是个单体的巨型应用，已经无法满足海量的并发请求，拆分势在必行。 在微服务的大潮之中， 架构师小明把系统拆分成了多个服务，根据需要部署在多个机器上，这些服务非常灵活，可以随着访问量弹性扩展。 世界上没有免费的午餐， 拆分成多个“微服务”以后虽然增加了弹性，但也带来了一个巨大的挑战：服务之间互相调用的开销。 比如说：原来用户下一个订单需要登录，浏览产品详情，加入购物车，支付，扣库存等一系列操作，在单体应用的时候它们都在一台机器的同一个进程中，说白了就是模块之间的函数调用，效率超级高。 现在好了，服务被安置到了不同的服务器上，一个订单流程，几乎每个操作都要越网络，都是远程过程调用(RPC)， 那执行时间、执行效率可远远比不上以前了。 远程过程调用的第一版实现使用了HTTP协议，也就是说各个服务对外提供HTTP接口。 小明发现，HTTP协议虽然简单明了，但是废话太多，仅仅是给服务器发个简单的消息都会附带一大堆无用信息： GET /orders/1 HTTP/1.1 Host: order.myshop.com User-Agent: Mozilla/5.0 (Windows NT 6.1; ) Accept: text/html; Accept-Language: en-US,en; Accept-Encoding: gzip Connection: keep-alive …… 看看那User-Agent，Accept-Language ，这个协议明显是为浏览器而生的！但是我这里是程序之间的调用，用这个HTTP有点亏。 能不能自定义一个精简的协议？ 在这个协议中我只需要把要调用方法名和参数发给服务器即可，根本不用这么多乱七八糟的额外信息。 但是自定义协议客户端和服务器端就得直接使用“低级”的Socket了，尤其是服务器端，得能够处理高并发的访问请求才行。 小明复习了一下服务器端的socket编程，最早的Java是所谓的阻塞IO(Blocking IO)， 想处理多个socket的连接的话需要创建多个线程， 一个线程对应一个。 这种方式写起来倒是挺简单的，但是连接（socket）多了就受不了了，如果真的有成千上万个线程同时处理成千上万个socket，占用大量的空间不说，光是线程之间的切换就是一个巨大的开销。 更重要的是，虽然有大量的socket，但是真正需要处理的（可以读写数据的socket）却不多，大量的线程处于等待数据状态（这也是为什么叫做阻塞的原因），资源浪费得让人心疼。 后来Java为了解决这个问题，又搞了一个非阻塞IO(NIO：Non-Blocking IO，有人也叫做New IO)， 改变了一下思路：通过多路复用的方式让一个线程去处理多个Socket。 这样一来，只需要使用少量的线程就可以搞定多个socket了，线程只需要通过Selector去查一下它所管理的socket集合，哪个Socket的数据准备好了，就去处理哪个Socket，一点儿都不浪费。 好了，就是Java NIO了！ 小明先定义了一套精简的RPC的协议，里边规定了如何去调用一个服务，方法名和参数该如何传递，返回值用什么格式……等等。然后雄心勃勃地要把这个协议用Java NIO给实现了。 可是美好的理想很快被无情的现实给击碎， 小明努力了一周就意识到自己陷入了一个大坑之中，Java NIO虽然看起来简单，但是API还是太“低级”了，有太多的复杂性，没有强悍的、一流的编程能力根本无法驾驭，根本做不到高并发情况下的可靠和高效。 小明不死心，继续向领导要人要资源，一定要把这个坑给填上，挣扎了6个月以后，终于实现了一个自己的NIO框架，可以执行高并发的RPC调用了。 然后又是长达6个月的修修补补，小明经常半夜被叫醒：生产环境的RPC调用无法返回了！ 这样的Bug不知道改了多少个。 在那些不眠之夜中，小明经常仰天长叹：我用NIO做个高并发的RPC框架怎么这么难呐！ 一年之后，自研的框架终于稳定，可是小明也从张大胖那里听到了一个让他崩溃的消息： 小明你知道吗?有个叫Netty的开源框架，可以快速地开发高性能的面向协议的服务器和客户端。 易用、健壮、安全、高效，你可以在Netty上轻松实现各种自定义的协议！咱们也试试？ 小明赶紧研究，看完后不由得“泪流满面”：这东西怎么不早点出来啊！ 好了，这个故事我快编不下去了，要烂尾了。 说说Netty到底是何方神圣， 要解决什么问题吧。 像上面小明的例子，想使用Java NIO来实现一个高性能的RPC框架，调用协议，数据的格式和次序都是自己定义的，现有的HTTP根本玩不转，那使用Netty就是绝佳的选择。 其实游戏领域是个更好的例子，长连接，自定义协议，高并发，Netty就是绝配。 因为Netty本身就是一个基于NIO的网络框架， 封装了Java NIO那些复杂的底层细节，给你提供简单好用的抽象概念来编程。 注意几个关键词，首先它是个框架，是个“半成品”，不能开箱即用，你必须得拿过来做点定制，利用它开发出自己的应用程序，然后才能运行（就像使用Spring那样）。 一个更加知名的例子就是阿里巴巴的Dubbo了，这个RPC框架的底层用的就是Netty。 另外一个关键词是高性能，如果你的应用根本没有高并发的压力，那就不一定要用Netty了。 原文：http://zhuanlan.51cto.com/art/201711/558661.htm","categories":[{"name":"Netty","slug":"Netty","permalink":"https://blog.fenxiangz.com/categories/Netty/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"Netty","slug":"Netty","permalink":"https://blog.fenxiangz.com/tags/Netty/"}]},{"title":"Java书籍推荐","slug":"java/advance/2016-12-09_java_books","date":"2016-12-09T00:00:00.000Z","updated":"2020-12-20T16:47:02.956Z","comments":true,"path":"post/java/advance/2016-12-09_java_books.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2016-12-09_java_books.html","excerpt":"","text":"《深入理解 Java 虚拟机：JVM 高级特性与最佳实践》《HotSpot 实战》《Java 并发编程实战》《java 多线程编程核心技术》《Effective Java 中文版》《深入分析 Java Web 技术内幕》《大型网站技术架构 核心原理与案例分析》《大型网站系统与 Java 中间件实践》《从 Paxos 到 ZooKeeper 分布式一致性原理与实践》《MySQL5.6 从零开始学》《Spring 源码深度解析》 http://developer.51cto.com/art/201512/503095.htm","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"书籍推荐","slug":"书籍推荐","permalink":"https://blog.fenxiangz.com/tags/%E4%B9%A6%E7%B1%8D%E6%8E%A8%E8%8D%90/"}]},{"title":"Java7 里 try-with-resources 分析","slug":"java/basic/2016-11-18_try_with_resources","date":"2016-11-28T07:59:24.000Z","updated":"2020-12-20T16:47:02.963Z","comments":true,"path":"post/java/basic/2016-11-18_try_with_resources.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/basic/2016-11-18_try_with_resources.html","excerpt":"","text":"这个所谓的 try-with-resources，是个语法糖。实际上就是自动调用资源的 close() 函数。 例如： 12345static String readFirstLineFromFile(String path) throws IOException &#123; try (BufferedReader br &#x3D; new BufferedReader(new FileReader(path))) &#123; return br.readLine(); &#125; &#125; 可以看到 try 语句多了个括号，而在括号里初始化了一个 BufferedReader。这种在 try 后面加个括号，再初始化对象的语法就叫 try-with-resources。实际上，相当于下面的代码（其实略有不同，下面会说明）： 12345678static String readFirstLineFromFileWithFinallyBlock(String path) throws IOException &#123; BufferedReader br &#x3D; new BufferedReader(new FileReader(path)); try &#123; return br.readLine(); &#125; finally &#123; if (br !&#x3D; null) br.close(); &#125; &#125; 很容易可以猜想到，这是编绎器自动在 try-with-resources 后面增加了判断对象是否为 null，如果不为 null，则调用 close() 函数的的字节码。 只有实现了 java.lang.AutoCloseable 接口，或者 java.io.Closable（实际上继随自 java.lang.AutoCloseable）接口的对象，才会自动调用其 close() 函数。有点不同的是 Java.io.Closable 要求一实现者保证 close 函数可以被重复调用。而 AutoCloseable 的 close() 函数则不要求是幂等的。具体可以参考 Javadoc。 下面从编绎器生成的字节码来分析下，try-with-resources 到底是怎样工作的： 12345678910public class TryStudy implements AutoCloseable&#123; static void test() throws Exception &#123; try(TryStudy tryStudy &#x3D; new TryStudy())&#123; System.out.println(tryStudy); &#125; &#125; @Override public void close() throws Exception &#123; &#125; &#125; TryStudy 实现了 AutoCloseable 接口，下面来看下 test 函数的字节码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364static test()V throws java&#x2F;lang&#x2F;Exception TRYCATCHBLOCK L0 L1 L2 TRYCATCHBLOCK L3 L4 L4 L5 LINENUMBER 21 L5 ACONST_NULL ASTORE 0 ACONST_NULL ASTORE 1 L3 NEW TryStudy DUP INVOKESPECIAL TryStudy.&lt;init&gt; ()V ASTORE 2 L0 LINENUMBER 22 L0 GETSTATIC java&#x2F;lang&#x2F;System.out : Ljava&#x2F;io&#x2F;PrintStream; ALOAD 2 INVOKEVIRTUAL java&#x2F;io&#x2F;PrintStream.println (Ljava&#x2F;lang&#x2F;Object;)V L1 LINENUMBER 23 L1 ALOAD 2 IFNULL L6 ALOAD 2 INVOKEVIRTUAL TryStudy.close ()V GOTO L6 L2 FRAME FULL [java&#x2F;lang&#x2F;Throwable java&#x2F;lang&#x2F;Throwable TryStudy] [java&#x2F;lang&#x2F;Throwable] ASTORE 0 ALOAD 2 IFNULL L7 ALOAD 2 INVOKEVIRTUAL TryStudy.close ()V L7 FRAME CHOP 1 ALOAD 0 ATHROW L4 FRAME SAME1 java&#x2F;lang&#x2F;Throwable ASTORE 1 ALOAD 0 IFNONNULL L8 ALOAD 1 ASTORE 0 GOTO L9 L8 FRAME SAME ALOAD 0 ALOAD 1 IF_ACMPEQ L9 ALOAD 0 ALOAD 1 INVOKEVIRTUAL java&#x2F;lang&#x2F;Throwable.addSuppressed (Ljava&#x2F;lang&#x2F;Throwable;)V L9 FRAME SAME ALOAD 0 ATHROW L6 LINENUMBER 24 L6 FRAME CHOP 2 RETURN LOCALVARIABLE tryStudy LTryStudy; L0 L7 2 MAXSTACK &#x3D; 2 MAXLOCALS &#x3D; 3 从字节码里可以看出，的确是有判断 tryStudy 对象是否为 null，如果不是 null，则调用 close 函数进行资源回收。再仔细分析，可以发现有一个 Throwable.addSuppressed 的调用，那么这个调用是什么呢？其实，上面的字节码大概是这个样子的（当然，不完全是这样的，因为汇编的各种灵活的跳转用 Java 是表达不出来的）： 1234567891011121314151617static void test() throws Exception &#123; TryStudy tryStudy &#x3D; null; try&#123; tryStudy &#x3D; new TryStudy(); System.out.println(tryStudy); &#125;catch(Throwable suppressedException) &#123; if (tryStudy !&#x3D; null) &#123; try &#123; tryStudy.close(); &#125;catch(Throwable e) &#123; e.addSuppressed(suppressedException); throw e; &#125; &#125; throw suppressedException; &#125; &#125; 有点晕是吧，其实很简单。使用了 try-with-resources 语句之后，有可能会出现两个异常，一个是 try 块里的异常，一个是调用 close 函数里抛出的异常。当然，平时我们写代码时，没有关注到。一般都是再抛出 close 函数里的异常，前面的异常被丢弃了。如果在调用 close 函数时出现异常，那么前面的异常就被称为 Suppressed Exceptions，因此 Throwable 还有个 addSuppressed 函数可以把它们保存起来，当用户捕捉到 close 里抛出的异常时，就可以调用 Throwable.getSuppressed 函数来取出 close 之前的异常了。 总结：使用 try-with-resources 的语法可以实现资源的自动回收处理，大大提高了代码的便利性，和 mutil catch 一样，是个好东东。用编绎器生成的字节码的角度来看，try-with-resources 语法更加高效点。java.io.Closable 接口要求一实现者保证 close 函数可以被重复调用，而 AutoCloseable 的 close() 函数则不要求是幂等的。参考：http://docs.oracle.com/javase/tutorial/essential/exceptions/tryResourceClose.htmlhttp://docs.oracle.com/javase/7/docs/api/java/lang/AutoCloseable.htmlhttp://docs.oracle.com/javase/7/docs/api/java/io/Closeable.html 原文http://blog.csdn.net/hengyunabc/article/details/18459463","categories":[{"name":"Java 基础","slug":"Java-基础","permalink":"https://blog.fenxiangz.com/categories/Java-%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"异常","slug":"异常","permalink":"https://blog.fenxiangz.com/tags/%E5%BC%82%E5%B8%B8/"},{"name":"Java异常","slug":"Java异常","permalink":"https://blog.fenxiangz.com/tags/Java%E5%BC%82%E5%B8%B8/"}]},{"title":"序列化和反序列化浅析","slug":"java/advance/2016-11-24_java_serializable","date":"2016-11-24T00:00:00.000Z","updated":"2020-12-20T16:47:02.956Z","comments":true,"path":"post/java/advance/2016-11-24_java_serializable.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2016-11-24_java_serializable.html","excerpt":"","text":"0.简介序列化和反序列化对于现代的程序员来说是一个既熟悉又陌生的概念。说熟悉是因为几乎每个程序员在工作中都直接或间接的使用过它，说陌生是因为大多数程序员对序列化和反序列化的认识仅仅停留在比较一下各种不同实现的序列化的性能上面，而很少有程序员对序列化和反序列化的设计和实现有深入的研究。 本文将从序列化和反序列化的设计和实现的入手，来简单讲解一下序列化和反序列化。其中包括以下几个方面： 1.序列化和反序列化的作用2.什么样的数据是可序列化的3.序列化和反序列化的分类3.序列化和反序列化的类型映射 本文不会涉及到某几种语言的某几种序列化实现的性能对比之类的内容。 1.序列化和反序列化的作用我们在编写程序代码时，通常会定义一些常量和变量，然后再写一堆操作它们的指令。不管是变量还是常量，它们表示的都是数据。所以简单的说，一个程序就是一堆指令操作一堆数据。 但是为了更有效的管理这堆数据，现代的程序设计语言都会引入一个类型系统来对这些数据进行分类管理，而不是让程序员把所有数据都一股脑的当做二进制串来进行操作。 比如一个常量可能是一个数字，一个布尔值，一个字符串，或者是一个由它们构成的数组。而变量通常具有更丰富的类型可以使用。甚至你还可以自定义类型。对于面向对象的语言来说，一个类型表示的不仅仅是数据本身，还包括了对这种类型数据的一组操作。 一个程序可以以源码或者可执行的二进制形式保存在磁盘（或者其它存储介质）上。当你需要执行它时，它会以某种形式被载入内存，然后执行。 一个程序在执行过程中通常会生成新的数据，这些新的数据一部分是临时的，在内存中，它们转瞬即逝。还有一部分数据可能需要被保存下来，或者被传递到其它的地方去。在这种情况下，可能就会涉及到数据的形式转换的问题。这个把程序运行时的内存数据转换为一种可保存或可传递的数据的过程，我们就称它为序列化。 这些保存和传递的数据，可能会在某个时间被重新载入内存，但可能会是不同的进程，或者不同的程序，甚至不同的机器上被载入，还原为内存中的具体类型的数据变量，这个从保存的数据还原为具体语言具体类型的数据变量的过程，我们称它为反序列化。 2.什么样的数据是可序列化的什么样的数据可序列化这是一个相对的问题，而不是一个绝对的问题。因为它会受到各种不同因素的影响。 数据的可还原性被序列化的数据应该是可还原的。可还原的意思是，一个被序列化的数据在被反序列化后仍然是有意义的。注意，这里说的是有意义，而不是说被反序列化的数据应该跟序列化之前的源数据相同。为了便于理解，我们来举例说明一下。 指针数据是否可序列化 首先我们来讨论一下指针类型的数据是否可序列化。 通常我们认为指针数据是不可序列化的，因为它表示的是一个内存地址，而如果我们把这个内存地址保存下来，下一次我们将这个内存地址还原到一个指针变量中时，这个内存地址所指向的位置的数据可能早就不是我们所需要的数据了，甚至指向的是一个完全没有意义的数据。所以，在这种情况下，虽然前后两个指针变量的值相同，但是还原之后的指针变量指向的数据已经没有意义了，我们就称它不具有可还原性。 那么指针数据真的不可序列化吗？如果我们从需要反序列化的数据有意义这个角度考虑，那么我们也可以做到对指针数据的序列化。 指针通常分为指向具体类型数据的指针（例如：int *, string *）和指向不明类型数据的指针（例如 void *）。 对于前者，如果我们希望序列化的数据包含指向的具体类型的数据，并且在反序列化之后，能够还原为一个指向该具体类型数据的指针，且指向的数据值跟源值相同的话，那么我们其实是可以做到的。虽然，还原之后的指针所指向的内存地址，跟源指针指向的内存地址可能完全不一样，但是它指向的数据是有意义的，且是我们期望的，那么这种情况下，我们也可以称这个指针数据是可还原的。 对于后者，如果我们没有所指向数据的具体信息，那就没有办法对指向的数据进行保存。所以这种类型的指针也就没办法进行序列化了。 另外，还有一种特殊的指针类型，它保存的并不是一个具体的内存地址，而是一个相对的偏移量，比如 uintptr_t 类型就常作此用，这种时候，对它的值序列化和反序列化之后，得到的值仍然是同样的相对偏移量值，在这种情况下，反序列化后的数据就是有意义的，所以，这种指针数据也具有可还原性。 从上面的分析，我们可以看出，指针类型是否可序列化，取决于我们想要什么意义的反序列化数据。 资源类型是否可序列化 对于资源类型，有些语言有明确的定义，比如 PHP，而有些语言则没有明确的定义。但大致上我们可以认为一个打开的文件对象，一个打开的数据库连接，一个打开的网络套接字，以及诸如此类跟外部资源相关的数据类型，都可以被称作资源类型。 对于资源类型我们通常认为它们都是不可序列化的，哪怕表示该类型的结构体中的所有字段都是可序列化的基本类型数据。原因是这些资源类型中保存的数据是跟当前打开的资源相关的，这些数据如果复制到其它的进程，或者其它的机器中去之后，这些资源类型中保存的数据就失去了意义。 对于资源类型的一部分属性数据，比如文件名，数据库地址，网络套接字地址，它们可以在不同的进程、不同的机器之间传递之后，仍然表示原有的意义。 但是通常的序列化程序是不会对资源类型做这样的序列化操作的，因为序列化程序对资源类型序列化时，并不能假定用户需要的仅仅是这些信息，而且如果用户需要的真的就仅仅是这些信息的话，那用户完全可以明确的只序列化这些数据，而不是对整个资源类型做序列化操作。 但是有些特殊的资源，比如内存流，文件流等。不同的序列化实现可能对待它们的方式也不同。有些序列化实现认为这些资源类型同样不可序列化。而有些序列化实现则认为可以将资源本身一起序列化，比如内存流中的数据会被作为序列化数据的主体进行序列化，在反序列化时，被反序列化为另外一个内存流对象，虽然是两个不同的资源，但是资源中的数据是相同的。 序列化格式的限制一个数据能否被序列化，还要看所使用的序列化格式是否支持。 对于基本类型的数据来说，几乎所有的序列化格式都支持。但是对于有些采用代码生成器方式实现的序列化来说，它们可能只支持通过 IDL 生成的代码中所定义的类型的序列化，而不支持对语言内置的单个原生类型数据变量的序列化，也不支持通过普通方式定义的自定义类型数据的序列化。比如 Protocol Buffers 就是这样。 对于复杂类型，比如 map 这种类型，有些序列化格式只支持 Key 为字符串类型的 map 数据的序列化。而不支持其它 Key 类型的 map 数据的序列化。比如 JSON 就是这样。 还有一种复杂类型数据是带有循环引用结构的数据，比如下面这个 JavaScript 代码中定义的这个数组 a： var a = [];a[0] = a;它的第一个元素引用了自己，这就产生了循环引用。对于这种类型的数据，很多的序列化格式也是不支持的，比如 JSON，Msgpack 都不支持这种类型数据的序列化。 但是上面所说的情况，并不是所有的序列化格式都不支持，比如 Hprose 对上面所说的所有类型都支持。 以上这些限制都是序列化格式本身造成的。 序列化实现的限制对于同一种序列化格式，即便是在同一种语言中，也可能存在着多种不同的实现，比如对于 JSON 序列化来说，它的 Java 版本的实现甚至有上百种。这些不同的实现各有特色，也各有各的限制，甚至互不兼容。有些实现可能仅仅支持几种特别定义的类型。有些则对语言内置的类型提供了很好的支持。 还有一些序列化格式跟特定语言有紧密的绑定关系，因此无法做到跨语言的序列化和反序列化，比如 Java 序列化，.NET 的 Binary 序列化，Go 语言的 Gob 序列化格式就只能支持特定的语言。 而且即便是这种针对特定语言的序列化也不是支持该语言的所有类型。比如：Java 序列化对于 class 类型只支持实现了 java.io.Serializable 接口的类型；.NET Binary 序列化则只支持标记了 System.SerializableAttribute 属性的类型。 所以，我们不能想当然的认为，一个数据支持某一种序列化，就一定支持其它类型的序列化。这种假设是不成立的。 3.序列化和反序列化的分类序列化和反序列化的格式多种多样，它们之间的主要区别可以大致分这样几类： 按照可读性分类首先从可读性角度，大致可分为文本序列化和二进制序列化两种，但是也有一些序列化格式介于两者之间，我们将它们暂称为半文本序列化。 文本序列化 XML 和 JSON 是大家最常见的两种文本序列化格式。 文本序列化的数据都是使用人类可读的字符表示的，就像大部分编程语言一样。而且允许包含多余的空白，以增加可读性。当然也可以表示为紧凑编码形式，以利于减少存储空间和传输流量。 文本序列化除了可读性还具有可编辑性，因此，文本序列化格式也经常被用于作为配置文件的存储格式。这样，使用普通的文本编辑器就可以方便的编辑这种配置文件。 文本序列化在表示数字时，通常采用人类可读的十进制数（包括小数和科学计数法）的字符串形式，这除了具有可读性以外，还有另外一个好处，就是可以方便的表示大整数或者高精度小数。 二进制序列化 二进制序列化的的数据不具有可读性，但是通常比文本序列化格式更加紧凑，而且在解析速度上也更有优势，当然实际的解析速度还跟具体实现有很大的关系，所以这也不是绝对的。 因为它们本身不具有可读性，所以在实际使用时，如果要想查看这些数据，就需要借助一些工具将它们解析为可读信息之后才能使用。在这方面，它们相对于文本序列化具有明显的劣势。 二进制序列化表示数字时，通常会使用定长或者变长的二进制编码方式，这虽然有利于更快的编码和解析编程语言中的基本数字类型，但是却不能表示大整数和高精度小数。 Protocol Buffers，Msgpack，BSON，Hessian 等格式是二进制序列化格式的代表。 半文本序列化 半文本序列化格式通常兼具文本序列化的可读性和二进制序列化的性能。 半文本序列化的数据也使用人类可读的字符表示，具有一定的可读性，但是半文本序列化是空白敏感的，因此它们不能像文本序列化那样在序列化数据中添加空白。 半文本序列化格式采用紧凑编码形式，而且通常会采用跟二进制编码类似的 TLV（Type-Length-Value）编码方式，因此具有比文本序列化更高效的解析速度，当然实际解析效率也跟具体实现有关。 半文本序列化格式中对原本的二进制字符串数据仍然按照二进制字符串的格式保存，而不会像文本序列化格式一样，需要将它们转换为 Base64 格式的文本。对于二进制字符串来说，不管是转为 Base64 格式的文本还是原本的样子，都不具有可读性，因此，直接以原格式保存，并不损失可读性，但是却可以增加解析效率。 半文本序列化格式在表示字符串时不会像文本序列化那样在字符串中间增加转义字符，或者将原本的字符用转义符号表示，因此，半本文序列化格式中的字符串反而比文本序列化的字符串具有更好的可读性。 半文本序列化格式在数字编码上具有跟文本序列化格式一样的特点。 Hprose，PHP 序列化格式是半文本序列化的代表。 按照自描述性分类自描述序列化 如果序列化数据中包含有数据类型的元信息，或者数据的表示形式同时可以反映出它的类型，那么这种序列化格式就是自描述的。自描述的序列化格式，可以在不借助外部描述的情况下，进行解析。 文本序列化和半文本序列化基本上都是自描述的。二进制序列化格式中，大部分也是自描述的。 自描述序列化格式不依赖外描述文件是它的优势，在一些应用场景下，这具有不可替代的优越性。但也因为包含了元信息，导致它的数据大小通常要比非自描述序列化的数据大一些。 像 XML，JSON，Hprose，Hessian，Msgpack 都是自描述类型的序列化格式。 非自描述序列化 非自描述序列化的数据在体积上更小，但是因为舍弃了自描述性，使得这种序列化数据在离开外部描述之后，就无法再被使用。 Protocol Buffers 是典型的非自描述类型的序列化格式的代表。 按照实现方式分类序列化和反序列化的很大一部分特征是由它们的实现决定的。关于序列化通常是使用代码生成或者反射的方式来实现，而对于反序列化除了这两种方式之外，还有将序列化数据解析为语法树的方式，这种方式实际上并不算反序列化，但通常可以更快的查找和获取文本序列化数据中某个节点的值。 基于代码生成器实现的序列化 采用代码生成方式实现序列化的好处是可以不依赖编程语言本身运行时中的元数据信息，这样即使某个语言（比如 C/C++）的运行时中本身没有包含足够的元数据时，也可以方便的进行序列化和反序列化。 采用代码生成方式实现序列化的另一个好处是，因为不使用反射，序列化和反序列化的速度通常会比基于反射实现的序列化反序列化更快一些。 但是采用代码生成方式实现的序列化的缺点也很明显，比如对支持的数据类型限制比较严格，使用起来比较麻烦，需要编写 IDL 文件，在类型映射上比较死板，通常只能实现 1-1 的映射（这个我们后面再谈），类型升级时，会产生兼容性问题等等。 基于反射实现的序列化 基于动态反射来实现序列化和反序列化可以做到更好的类型支持，比如语言的内置类型和普通方式编写的自定义类型的数据都可以被序列化和反序列化，而且无需编写 IDL 文件就可以实现动态序列化，类型映射也更加灵活，可以实现 n-m 的映射，类型升级时，可以避免产生兼容性问题。 但通常基于反射实现的序列化和反序列化的速度要比采用代码生成方式的序列化和反序列化要慢一些，但是这也不是绝对的，因为在实现中，可通过一些其它的手段来提升性能。 例如采用缓存的方式，对于那些需要反射才能获得的元信息进行缓存，这样在获取元信息时可以避免反射而直接使用缓存的元信息来加快序列化速度。还可以使用动态的字节码生成方式，比如在 Java 中使用 ASM 技术来动态生成序列化和反序列化的代码，在 .NET 中使用 Emit 技术也可以实现同样的功能。而对于 C、C++、Rust 等语言可以采用宏和模板的方式在编译期生成具体类型的序列化和反序列化的代码，对于 D、Nim 等语言则可以采用编译期反射和编译期代码执行功能在编译期动态生成具体类型的序列化和反序列化代码，通过这些手段，既可以获得传统的代码生成器方式的序列化和反序列化的性能，又可以避免代码生成器的缺陷。 例如 Hprose for .NET 就采用上面提到的元数据缓存 + Emit 动态代码生成的优化手段，使得它的序列化和反序列化速度远远超过 Protocol Buffers 的速度。 按照跨语言能力分类并不是所有的序列化格式都是跨语言的。即使是跨语言的序列化格式，在跨语言的能力上也有所不同。 特定语言专有的序列化 大部分语言内置的序列化格式都属于特定语言专有的序列化。例如 Java 的序列化，.NET 的 Binary 序列化，Go 的 Gob 序列化都属于这一种。 但也有特例，比如 PHP 序列化，原本是 PHP 语言专有的序列化格式，但因为它的格式比较简单，因此也有一些其它语言上的 PHP 序列化的第三方实现。但终究 PHP 序列化格式跟 PHP 语言的关系更加紧密，所以在其他语言中使用 PHP 序列化时相对于其它跨语言的序列化格式或多或少的会有一些不方便的地方。 跨语言的序列化 文本序列化格式往往具有更好的跨语言特征。比如 XML，JSON 等序列化格式，对于不同的语言都有很多的实现来支持。 还有一些半文本或二进制序列化格式也是为跨语言而设计的，比如 Hprose，Protocol Buffers，MsgPack 等，它们也具有很好的跨语言能力。 但多数二进制序列化格式在跨语言方面有很多限制。 4.序列化和反序列化的类型映射如果编程语言中的数据类型跟序列化格式中的数据类型有且只有唯一的映射关系，我们就把这种类型映射关系称为 1-1 映射。 如果在序列化时，编程语言中的多种数据类型被映射为一种序列化格式的类型，并且在反序列化时，一种序列化类型可以被反序列化为编程语言中的多种类型，那么这种类型映射关系称为 n-m 映射。 当然还存在其它的情况，比如多种序列化类型被反序列化为编程语言中的同一种类型，再比如编程语言中的所有类型跟序列化类型中的某个类型都不存在映射关系，等等。这些其它情况，我们也把它们归到 1-1 映射中。 1-1 映射还是 n-m 映射，除了跟序列化格式有关以外，还跟具体的语言实现有很大的关系。 1-1 映射语言内置的序列化和反序列化实现一般都是 1-1 映射。这可以保证序列化之前的数据跟反序列化之后的数据在类型上的完全一致性。但也由于语言内置类型的丰富性和 1-1 映射的一致性，导致这些语言内置的序列化格式几乎无法做到跨语言实现。 我们前面也谈到过一个特例，那就是 PHP 序列化，PHP 序列化之所以能够做到跨语言实现，是因为它本身的内置类型非常有限，以至于即使在 PHP 中是 1-1 映射的数据类型还不如其它一些跨语言的序列化支持的数据类型更丰富。 而 JSON 格式，如果把它放到 JavaScript 中，它也是 1-1 映射的。而 JSON 序列化在其它语言中的实现则是多种多样，有的仅支持 1-1 映射，有的则支持 n-m 映射，即便是同一种语言的不同实现也是如此。 1-1 映射最麻烦的问题是，要么支持的类型不够丰富，要么跨语言方面难以实现。 第一个问题对于本来类型就不是很多的脚本语言来说通常不是问题，但对于 Java，C# 之类的语言来说，这就是个问题了。 n-m 映射n-m 映射可以很好的解决这个问题。 比如序列化格式中不需要为 Array，List，Tuple，Set 定义不同的类型，而只需要一种通用的列表类型，之后就可以将某种具体语言的 Array，List，Tuple，Set 等具有列表特征的数据都映射为这一种列表类型，在反序列化的时候，则直接反序列化为某种指定的类型。 这样做还有一个额外的好处：当你希望类型一致的时候，你就可以实现类型一致，而当你不希望使用一致的类型时，可以直接在序列化和反序列化的过程中进行类型的转换。而不需要得到了一致的类型之后，再去自己手动转换为另一种类型。 通过反射方式来实现的序列化和反序列化可以更方便的实现 n-m 映射。而通过代码生成器方式实现的序列化和反序列化则通常只能实现 1-1 映射。因此，通过反射方式来实现的序列化和反序列化具有更好的灵活性。 原文：https://hacpai.com/article/1478633580985","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"序列化","slug":"序列化","permalink":"https://blog.fenxiangz.com/tags/%E5%BA%8F%E5%88%97%E5%8C%96/"}]},{"title":"Java 内存分配和泄露","slug":"java/advance/2016-11-03_java_memory_malloc","date":"2016-11-03T13:29:24.000Z","updated":"2020-12-20T16:47:02.956Z","comments":true,"path":"post/java/advance/2016-11-03_java_memory_malloc.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2016-11-03_java_memory_malloc.html","excerpt":"","text":"Java 内存分配策略Java 程序运行时的内存分配策略有三种, 分别是静态分配, 栈式分配, 和堆式分配，对应的，三种存储策略使用的内存空间主要分别是静态存储区（也称方法区）、栈区和堆区。 静态存储区（方法区）：主要存放静态数据、全局 static 数据和常量。这块内存在程序编译时就已经分配好，并且在程序整个运行期间都存在。 栈区 ：当方法被执行时，方法体内的局部变量（其中包括基础数据类型、对象的引用）都在栈上创建，并在方法执行结束时这些局部变量所持有的内存将会自动被释放。因为栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存容量有限。 堆区 ： 又称动态内存分配，通常就是指在程序运行时直接 new 出来的内存，也就是对象的实例。这部分内存在不使用时将会由 Java 垃圾回收器来负责回收。 栈与堆的区别在方法体内定义的（局部变量）一些基本类型的变量和对象的引用变量都是在方法的栈内存中分配的。当在一段方法块中定义一个变量时，Java 就会在栈中为该变量分配内存空间，当超过该变量的作用域后，该变量也就无效了，分配给它的内存空间也将被释放掉，该内存空间可以被重新使用。堆内存用来存放所有由 new 创建的对象（包括该对象其中的所有成员变量）和数组。在堆中分配的内存，将由 Java 垃圾回收器来自动管理。在堆中产生了一个数组或者对象后，还可以在栈中定义一个特殊的变量，这个变量的取值等于数组或者对象在堆内存中的首地址，这个特殊的变量就是我们上面说的引用变量。我们可以通过这个引用变量来访问堆中的对象或者数组。 举个例子: 12345678910public class Sample &#123; int s1 &#x3D; 0; Sample mSample1 &#x3D; new Sample(); public void method() &#123; int s2 &#x3D; 1; Sample mSample2 &#x3D; new Sample(); &#125; &#125; Sample mSample3 &#x3D; new Sample(); Sample 类的局部变量 s2 和引用变量 mSample2 都是存在于栈中，但 mSample2 指向的对象是存在于堆上的。mSample3 指向的对象实体存放在堆上，包括这个对象的所有成员变量 s1 和 mSample1，而它自己存在于栈中。 局部变量的基本数据类型和引用存储于栈中，引用的对象实体存储于堆中。—— 因为它们属于方法中的变量，生命周期随方法而结束。 成员变量全部存储于堆中（包括基本数据类型，引用和引用的对象实体）—— 因为它们属于类，类对象终究是要被new出来使用的。 了解了 Java 的内存分配之后，我们再来看看 Java 是怎么管理内存的。 Java是如何管理内存Java的内存管理就是对象的分配和释放问题。在 Java 中，程序员需要通过关键字 new 为每个对象申请内存空间 (基本类型除外)，所有的对象都在堆 (Heap)中分配空间。另外，对象的释放是由 GC 决定和执行的。在 Java 中，内存的分配是由程序完成的，而内存的释放是由 GC 完成的，这种收支两条线的方法确实简化了程序员的工作。但同时，它也加重了JVM的工作。这也是 Java 程序运行速度较慢的原因之一。因为，GC 为了能够正确释放对象，GC 必须监控每一个对象的运行状态，包括对象的申请、引用、被引用、赋值等，GC 都需要进行监控。监视对象状态是为了更加准确地、及时地释放对象，而释放对象的根本原则就是该对象不再被引用。 为了更好理解 GC 的工作原理，我们可以将对象考虑为有向图的顶点，将引用关系考虑为图的有向边，有向边从引用者指向被引对象。另外，每个线程对象可以作为一个图的起始顶点，例如大多程序从 main 进程开始执行，那么该图就是以 main 进程顶点开始的一棵根树。在这个有向图中，根顶点可达的对象都是有效对象，GC将不回收这些对象。如果某个对象 (连通子图)与这个根顶点不可达(注意，该图为有向图)，那么我们认为这个(这些)对象不再被引用，可以被 GC 回收。以下，我们举一个例子说明如何用有向图表示内存管理。对于程序的每一个时刻，我们都有一个有向图表示JVM的内存分配情况。以下右图，就是左边程序运行到第6行的示意图。 Java使用有向图的方式进行内存管理，可以消除引用循环的问题，例如有三个对象，相互引用，只要它们和根进程不可达的，那么GC也是可以回收它们的。这种方式的优点是管理内存的精度很高，但是效率较低。另外一种常用的内存管理技术是使用计数器，例如COM模型采用计数器方式管理构件，它与有向图相比，精度行低(很难处理循环引用的问题)，但执行效率很高。 Java 内存回收机制不论哪种语言的内存分配方式，都需要返回所分配内存的真实地址，也就是返回一个指针到内存块的首地址。Java 中对象是采用 new 或者反射的方法创建的，这些对象的创建都是在堆（Heap）中分配的，所有对象的回收都是由 Java 虚拟机通过垃圾回收机制完成的。GC 为了能够正确释放对象，会监控每个对象的运行状况，对他们的申请、引用、被引用、赋值等状况进行监控，Java 会使用有向图的方法进行管理内存，实时监控对象是否可以达到，如果不可到达，则就将其回收，这样也可以消除引用循环的问题。在 Java 语言中，判断一个内存空间是否符合垃圾收集标准有两个：一个是给对象赋予了空值 null，以下再没有调用过，另一个是给对象赋予了新值，这样重新分配了内存空间。 什么是Java中的内存泄露在Java中，内存泄漏就是存在一些被分配的对象，这些对象有下面两个特点，首先，这些对象是可达的，即在有向图中，存在通路可以与其相连；其次，这些对象是无用的，即程序以后不会再使用这些对象。如果对象满足这两个条件，这些对象就可以判定为Java中的内存泄漏，这些对象不会被GC所回收，然而它却占用内存。 在C++中，内存泄漏的范围更大一些。有些对象被分配了内存空间，然后却不可达，由于C++中没有GC，这些内存将永远收不回来。在Java中，这些不可达的对象都由GC负责回收，因此程序员不需要考虑这部分的内存泄露。 通过分析，我们得知，对于C++，程序员需要自己管理边和顶点，而对于Java程序员只需要管理边就可以了(不需要管理顶点的释放)。通过这种方式，Java提高了编程的效率。 因此，通过以上分析，我们知道在Java中也有内存泄漏，但范围比C++要小一些。因为Java从语言上保证，任何对象都是可达的，所有的不可达对象都由GC管理。 对于程序员来说，GC基本是透明的，不可见的。虽然，我们只有几个函数可以访问GC，例如运行GC的函数System.gc()，但是根据Java语言规范定义， 该函数不保证JVM的垃圾收集器一定会执行。因为，不同的JVM实现者可能使用不同的算法管理GC。通常，GC的线程的优先级别较低。JVM调用GC的策略也有很多种，有的是内存使用到达一定程度时，GC才开始工作，也有定时执行的，有的是平缓执行GC，有的是中断式执行GC。但通常来说，我们不需要关心这些。除非在一些特定的场合，GC的执行影响应用程序的性能，例如对于基于Web的实时系统，如网络游戏等，用户不希望GC突然中断应用程序执行而进行垃圾回收，那么我们需要调整GC的参数，让GC能够通过平缓的方式释放内存，例如将垃圾回收分解为一系列的小步骤执行，Sun提供的HotSpot JVM就支持这一特性。 Java 内存泄露引起原因那么，Java 内存泄露根本原因是什么呢？长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是 java 中内存泄露的发生场景。具体主要有如下几大类： 1、静态集合类引起内存泄露： 像 HashMap、Vector 等的使用最容易出现内存泄露，这些静态变量的生命周期和应用程序一致，他们所引用的所有的对象 Object 也不能被释放，因为他们也将一直被 Vector 等引用着。例: 123456static Vector v &#x3D; new Vector(10); for (int i &#x3D; 1; i&lt;100; i++) &#123; Object o &#x3D; new Object(); v.add(o); o &#x3D; null; &#125; 在这个例子中，循环申请 Object 对象，并将所申请的对象放入一个 Vector 中，如果仅仅释放引用本身（o = null），那么 Vector 仍然引用该对象，所以这个对象对 GC 来说是不可回收的。因此，如果对象加入到 Vector 后，还必须从 Vector 中删除，最简单的方法就是将 Vector 对象设置为 null。 2、当集合里面的对象属性被修改后，再调用 remove（）方法时不起作用： 例： 123456789101112131415161718public static void main(String[] args) &#123; Set&lt;Person&gt; set &#x3D; new HashSet&lt;Person&gt;(); Person p1 &#x3D; new Person(&quot;唐僧&quot;,&quot;pwd1&quot;,25); Person p2 &#x3D; new Person(&quot;孙悟空&quot;,&quot;pwd2&quot;,26); Person p3 &#x3D; new Person(&quot;猪八戒&quot;,&quot;pwd3&quot;,27); set.add(p1); set.add(p2); set.add(p3); System.out.println(&quot;总共有:&quot;+set.size()+&quot;个元素!&quot;); &#x2F;&#x2F; 结果：总共有: 3 个元素! p3.setAge(2); &#x2F;&#x2F; 修改 p3 的年龄, 此时 p3 元素对应的 hashcode 值发生改变 set.remove(p3); &#x2F;&#x2F; 此时 remove 不掉，造成内存泄漏 set.add(p3); &#x2F;&#x2F; 重新添加，居然添加成功 System.out.println(&quot;总共有:&quot;+set.size()+&quot;个元素!&quot;); &#x2F;&#x2F; 结果：总共有: 4 个元素! for (Person person : set) &#123; System.out.println(person); &#125; &#125; 3、监听器 在 java 编程中，我们都需要和监听器打交道，通常一个应用当中会用到很多监听器，我们会调用一个控件的诸如 addXXXListener() 等方法来增加监听器，但往往在释放对象的时候却没有记住去删除这些监听器，从而增加了内存泄漏的机会。 4、各种连接 比如数据库连接（dataSourse.getConnection()），网络连接 (socket) 和 io 连接，除非其显式的调用了其 close（）方法将其连接关闭，否则是不会自动被 GC 回收的。对于 Resultset 和 Statement 对象可以不进行显式回收，但 Connection 一定要显式回收，因为 Connection 在任何时候都无法自动回收，而 Connection 一旦回收，Resultset 和 Statement 对象就会立即为 NULL。但是如果使用连接池，情况就不一样了，除了要显式地关闭连接，还必须显式地关闭 Resultset Statement 对象（关闭其中一个，另外一个也会关闭），否则就会造成大量的 Statement 对象无法释放，从而引起内存泄漏。这种情况下一般都会在 try 里面去的连接，在 finally 里面释放连接。 5、内部类和外部模块等的引用 内部类的引用是比较容易遗忘的一种，而且一旦没释放可能导致一系列的后继类对象没有释放。此外程序员还要小心外部模块不经意的引用，例如程序员 A 负责 A 模块，调用了 B 模块的一个方法如： 1public void registerMsg(Object b); 这种调用就要非常小心了，传入了一个对象，很可能模块 B 就保持了对该对象的引用，这时候就需要注意模块 B 是否提供相应的操作去除引用。 6、单例模式 不正确使用单例模式是引起内存泄露的一个常见问题，单例对象在被初始化后将在 JVM 的整个生命周期中存在（以静态变量的方式），如果单例对象持有外部对象的引用，那么这个外部对象将不能被 jvm 正常回收，导致内存泄露，考虑下面的例子： 1234567891011121314151617181920class A&#123; public A()&#123; B.getInstance().setA(this); &#125; .... &#125; &#x2F;&#x2F;B 类采用单例模式 class B&#123; private A a; private static B instance&#x3D;new B(); public B()&#123;&#125; public static B getInstance()&#123; return instance; &#125; public void setA(A a)&#123; this.a&#x3D;a; &#125; &#x2F;&#x2F;getter... &#125; 显然 B 采用 singleton 模式，它持有一个 A 对象的引用，而这个 A 类的对象将不能被回收。想象下如果 A 是个比较复杂的对象或者集合类型会发生什么情况。","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"内存分配","slug":"内存分配","permalink":"https://blog.fenxiangz.com/tags/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"},{"name":"内存泄露","slug":"内存泄露","permalink":"https://blog.fenxiangz.com/tags/%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2/"}]},{"title":"Spring 组合注解&注解继承","slug":"java/spring/2016-10-30_Spring 组合注解&注解继承","date":"2016-10-30T00:00:00.000Z","updated":"2020-12-20T16:47:02.970Z","comments":true,"path":"post/java/spring/2016-10-30_Spring 组合注解&注解继承.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/spring/2016-10-30_Spring%20%E7%BB%84%E5%90%88%E6%B3%A8%E8%A7%A3&%E6%B3%A8%E8%A7%A3%E7%BB%A7%E6%89%BF.html","excerpt":"","text":"组合注解被注解的注解称为组合注解。 1.好处 简单化注解配置，用很少的注解来标注特定含义的多个元注解 提供了很好的扩展性，可以根据实际需要灵活的自定义注解。 2 如何使用 （1）自定义一个组合注解 12345678@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Configuration &#x2F;&#x2F;实际上就是一个bean注解@ComponentScan&#x2F;&#x2F;自动扫描对应value（package路径）值下面的所有beanpublic @interface WiselyConfiguration &#123; String[] value() default&#123;&#125;;&#x2F;&#x2F;可以不写，实际在@ComponentScan注解中已经定义过，所以可以拿过来直接用&#125; （2）使用自定义注解 12345&#x2F;&#x2F;@Configuration &#x2F;&#x2F;属性注解&#x2F;&#x2F;@ComponentScan(&quot;com.gdb.spingboot.service&quot;) &#x2F;&#x2F; 要扫描的bean路径@WiselyConfiguration(value &#x3D; &quot;com.gdb.spingboot.service&quot;) &#x2F;&#x2F;自定义注解，扫描的所有的bean来源于value值所对应的包路径下public class ElConfig&#123;&#125; 组合注解，和上面两个注解实现的功能完全一致，所以如果在繁琐的注解被多次使用的情况下，可以考虑自定义组合注解。 注解继承@Inhberited注解可以让指定的注解在某个类上使用后，这个类的子类也将自动被该注解标记。 123456789101112131415@Retention(RetentionPolicy.RUNTIME)@Inherited public @interface Hello &#123;&#125;public class AnnotationTest3&#123; &#x2F;&#x2F;在基类上使用被@Inherited标记了的注解@Hello@HelloclassBase&#123;&#125; &#x2F;&#x2F;派生类没有直接注解@HelloclassDerivedextendsBase&#123;&#125; public static void main(String[] args)&#123; &#x2F;&#x2F;派生类也会自动被注解@Hello标记。if( Derived.class.isAnnotationPresent(Hello.class))&#123; Hello hello &#x3D; (Hello)Derived.class.getAnnotation(Hello.class); System.out.println(&quot;Hello&quot;); &#125; &#125;&#125; 注解的其它事项（1）当注解中含有数组属性时，使用{}赋值，各个元素使用逗号分隔。 （2）注解的属性可以是另外一个注解。 （3）注解的属性可以是另外一个注解的数组。 （4）注解的默认属性是value，只有一个value属性时可以不写value=xxx，直接写值即可。 （5）注解的属性的默认值使用default来定义。 123456&#x2F;&#x2F;定义注解@Retention(RetentionPolicy.RUNTIME)public @interface SomeFuture &#123; String value();&#x2F;&#x2F;默认的属性int[] arrayValue(); &#x2F;&#x2F;数组Hello helloValue();&#x2F;&#x2F;是另外一个注解@Hello Hello[] helloArrayValue() default &#123;@Hello,@Hello&#125;; &#x2F;&#x2F;注解的数组，而且提供默认值。&#125; 123456789&#x2F;&#x2F;使用注解@SomeFuture( value&#x3D;&quot;hello&quot;, &#x2F;&#x2F;默认属性 arrayValue&#x3D;&#123;1,2,3&#125;,&#x2F;&#x2F;数组属性 helloValue&#x3D;@Hello,&#x2F;&#x2F;属性是另外一个注解 helloArrayValue&#x3D;&#123;@Hello,@Hello,@Hello&#125;&#x2F;&#x2F;属性是另外一个注解的数组。)public class AnnotationTest4&#123;&#125;","categories":[{"name":"Spring Core","slug":"Spring-Core","permalink":"https://blog.fenxiangz.com/categories/Spring-Core/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"注解","slug":"注解","permalink":"https://blog.fenxiangz.com/tags/%E6%B3%A8%E8%A7%A3/"},{"name":"Spring","slug":"Spring","permalink":"https://blog.fenxiangz.com/tags/Spring/"}]},{"title":"为什么 8 位有符号整数的范围为 “-128 ~ +127”","slug":"java/advance/2016-10-25_number","date":"2016-10-26T11:29:24.000Z","updated":"2020-12-20T16:47:02.955Z","comments":true,"path":"post/java/advance/2016-10-25_number.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/advance/2016-10-25_number.html","excerpt":"","text":"这是一个困惑了我几年的问题，它让我对现在的教科书和老师极其不满，从我 N 年前开始摸电脑时，就几乎在每一本 C++ 教科书上都说，8 位有符号的取值范围是 - 128 到 +127，为什么不是 - 127 到 +127 呢，后来的 java，int 的取值范围，再 32 位计算，-2^31 到 +2^31-1，可是，却从来没有任何一本教科书或一个老师比我解释过这个问题。 原因没有在工作上或者是什么地方直接遇到它，所以我也一直忽略它，但心里总是有一根刺。 直到刚才!!!! 就是刚才，无聊之极，在看汇编的书时，又遇到它了，但一如以往，书上直接地，有心地，明显地绕过了这个问题，真是可恶啊。 几经周折，终于把它搞清楚了:其实，它是计算机底层为了实现数值运算而决定的，涉及非常非常基础的原码，反码，补码知识，一般 (99.9999%) 都不会用得上。 那 0.0001%，估计也就是计算机考试了。 话说: 用 2^8 来表示无符号整数的话，全世界的理解都是 0 到 255 了，那么，有符号呢? 用最高位表示符号，0 为 +，1 为 -，那么，正常的理解就是 -127 至 +127 了。 这就是原码了，值得一提的是，原码的弱点，有 2 个 0，即 + 0 和 - 0，还有就是，进行异号相加或同号相减时，比较笨蛋，先要判断 2 个数的绝对值大小，然后进行加减操作，最后运算结果的符号还要与大的符号相同。 于是乎，反码产生了，原因。。。。略，反正，没过多久，反码就成为了过滤产物，也就是，后来补码出现了。 补码的知识不说了，只说有关 + 127 和 - 128 的。官方的定义 [-2^(n-1)，2(n-1)-1]，补码的 0 没有正负之分。原因呢? 没有一本书上有说，这也是我这么火的原因，但通过思考，google，再思考，很快找到答案: 首先，难不免干点白痴般地事情，穷举一下。。。 正数，原码跟补码一样 123456789101112+127， 0111 1111 +126， 0111 1110 +125， 0111 1101 +124， 0111 1100 +123， 0111 1011 +122， 0111 1010 ……+4， 0000 0100 +3， 0000 0011 +2， 0000 0010 +1， 0000 0001 0， 0000 0000 (无正负之分) 下面是负数了 123456789101112131415161718192021222324252627值， 原码， 符号位不变其它取反， +1 -1， 1000 0001， 1111 1110， 1111 1111-2， 1000 0010， 1111 1101， 1111 1110 -3， 1000 0011， 1111 1100， 1111 1101 -4， 1000 0100， 1111 1011， 1111 1100 -5， 1000 0101， 1111 1010， 1111 1011 -6， 1000 0110， 1111 1001， 1111 1010 -7， 1000 0111， 1111 1000， 1111 1001 -8， 1000 1000， 1111 0111， 1111 1000 -9， 1000 1001， 1111 0110， 1111 0111 -10， 1000 1010， 1111 0101， 1111 0110 -11， 1000 1011， 1111 0100， 1111 0101 -12， 1000 1100， 1111 0011， 1111 0100 -13， 1000 1101， 1111 0010， 1111 0011 -14， 1000 1110， 1111 0001， 1111 0010 -15， 1000 1111， 1111 0000， 1111 0001 -16， 1001 0000， 1110 1111， 1111 0000 -17， 1001 0001， 1110 1110， 1110 1111 …… -24， 1001 1000， 1110 0111， 1110 1000 …… -99， 1110 0011， 1001 1100， 1110 0100 …… -124， 1111 1100， 1000 0011， 1000 0100 -125， 1111 1101， 1000 0010， 1000 0011 -126， 1111 1110， 1000 0001， 1000 0010 -127， 1111 1111， 1000 0000， 1000 0001 看出点什么了没有?如果没有，那么，给个提示， 再继续下去，下一个补码是什么呢? 当然是 12值， 原码， 符号位不变其它取反， +1 -128， 先略过， 再略过， 1000 0000 1000 0000 那么，它的原码是什么呢? 从补码求原码的方法跟原码求补码是一样的。 先保留符号位其它求反: 1111 1111， 再加 1:11000 0000， 超过了 8 位了 对，用 8 位数的原码在这里已经无法表示了。 关键就在这里，补码 1000 0000 为 -128 是不用怀疑的 (上面的穷举)， 那么，回到原码处， 它的原码也是 1000 0000(超出的自动丢失)， 1000 0000 在原码表示什么呢? -0， 但补码却规定 0 没有正负之分 转换一下思路，看看计算机里，是怎么运算的: 对于负数，先取绝对值，然后求反，加一 -128 -&gt; 128 -&gt; 1000 0000 -&gt; 0111 1111 -&gt; 1000 0000 现在明确了吧。 所以， 8 位有符号的整数取值范围的补码表示 1000 0000 到 0000 0000， 再到 0111 1111 即 -128 到 0， 再到 127 最终 -128 ~ +127。","categories":[{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"二进制","slug":"二进制","permalink":"https://blog.fenxiangz.com/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6/"}]},{"title":"ThreadLocal 笔记","slug":"java/basic/2016-10-25_threadlocal","date":"2016-10-25T11:29:24.000Z","updated":"2020-12-20T16:47:02.963Z","comments":true,"path":"post/java/basic/2016-10-25_threadlocal.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/basic/2016-10-25_threadlocal.html","excerpt":"","text":"作用 ThreadLocal 不是用来解决共享对象访问的多线程访问问题，而是用于解决不同线程保持各自独立的一个对象。典型的问题就是：当一个单例 A 持有某个属性对象 a.b 时，如果 a.b 在多个方法里面使用，就有可能造成线程不安全，如果把 b 定义成 ThreadLocal b 就可以避免以上问题。 实现 一、ThreadLocal 实例方法： 123456789101112131415161718192021222324public void set(T value) &#123; Thread t &#x3D; Thread.currentThread(); ThreadLocalMap map &#x3D; getMap(t); if (map !&#x3D; null) map.set(this, value); else createMap(t, value);&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125;void createMap(Thread t, T firstValue) &#123; t.threadLocals &#x3D; new ThreadLocalMap(this, firstValue);&#125;public T get() &#123; Thread t &#x3D; Thread.currentThread(); ThreadLocalMap map &#x3D; getMap(t); if (map !&#x3D; null) &#123; ThreadLocalMap.Entry e &#x3D; map.getEntry(this); if (e !&#x3D; null) return (T)e.value; &#125; return setInitialValue();&#125; set() 方法逻辑：获取当前线程对应的 ThreadLocalMap map;如果存在直接 map.set;否则 new ThreadLocalMap 给线程使用。 由此可知，每个线程都有一个自己的 ThreadLocal.ThreadLocalMap 对象。 get() 方法逻辑：获取当前线程对应的 ThreadLocalMap map;如果存在，且当前 ThreadLocal 实例对应的值不为空，返回 map 拿到的值；否则，设置前 ThreadLocal 实例默认值并返回。 二、ThreadLocalMap ThreadLocalMap 构造函数： 12345678private static final int INITIAL_CAPACITY &#x3D; 16;ThreadLocalMap(ThreadLocal firstKey, Object firstValue) &#123; table &#x3D; new Entry[INITIAL_CAPACITY]; int i &#x3D; firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] &#x3D; new Entry(firstKey, firstValue); size &#x3D; 1; setThreshold(INITIAL_CAPACITY);&#125; 第一点：INITIAL_CAPACITY 必须是 2 的 N 次幂，默认值为 16。 为什么是 2 的 N 次幂值？ ThreadLocalMap 类保存着一个 table 每一个 ThreadLocal 有自己的 threadLocalHashCode 值 从 ThreadLocalMap table 里存 / 取值的时候会通过 threadLocalHashCode 值计算出一个 i，再通过 table[i] 得到 ThreadLocal 的值。如构造函数代码所示： 1int i &#x3D; firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); 这是计算方法，这个过程实际上是一个取模的过程。 举个例子 十进制取模：26 % 16 = 10 二进制取模： 1234 00011010 &amp; 00001111&#x3D; 00001010&#x3D; 10 所以，十进制的取模对于二进制，只需要使用公式 M &amp; (C-1) 即可，这种与操作对于 CPU 运算效率很高。当然，一个大前提就是 C-1 的值转换为二进制时，低位部分要求全是 1 才行。所以要求 C 必须是 2 的 N 次幂。 第二点：threadLocalHashCode 看一下 ThreadLocal 这部分的代码： 12345678910111213141516171819202122232425262728&#x2F;** * ThreadLocals rely on per-thread linear-probe hash maps attached * to each thread (Thread.threadLocals and * inheritableThreadLocals). The ThreadLocal objects act as keys, * searched via threadLocalHashCode. This is a custom hash code * (useful only within ThreadLocalMaps) that eliminates collisions * in the common case where consecutively constructed ThreadLocals * are used by the same threads, while remaining well-behaved in * less common cases. *&#x2F;private final int threadLocalHashCode &#x3D; nextHashCode();&#x2F;** * The next hash code to be given out. Updated atomically. Starts at * zero. *&#x2F;private static AtomicInteger nextHashCode &#x3D; new AtomicInteger();&#x2F;** * The difference between successively generated hash codes - turns * implicit sequential thread-local IDs into near-optimally spread * multiplicative hash values for power-of-two-sized tables. *&#x2F;private static final int HASH_INCREMENT &#x3D; 0x61c88647;&#x2F;** * Returns the next hash code. *&#x2F;private static int nextHashCode() &#123; return nextHashCode.getAndAdd(HASH_INCREMENT);&#125; 可以看出来，ThreadLocal 第一次 set 值的时候，threadLocalHashCode 得到的是 0，之后每次得到的数都是加了 0x61c88647。这算一个 16 进制表示的数，转换成十进制是：1640531527。 为什么是这个数？ 本屌暂时还没搞懂~ 简单的总结一下 ThreadLocal： 1、每一个 ThreadLocal 实例有一个自己的 threadLocalHashCode；2、每一个 Thread 有一个自己的 ThreadLocalMap threadLocals， threadLocals 的 key 是 ThreadLocal 实例，value 是 ThreadLocal 真是的实际保存的对象实例。3、ThreadLocalMap 使用 table 数组保存每一个 Entry（key-value）。4、ThreadLocalMap 计算 ThreadLocal 对应 table[i] 的 i 使用 threadLocalHashCode 取模获得。","categories":[{"name":"Java 基础","slug":"Java-基础","permalink":"https://blog.fenxiangz.com/categories/Java-%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"ThreadLocal","slug":"ThreadLocal","permalink":"https://blog.fenxiangz.com/tags/ThreadLocal/"},{"name":"Java线程","slug":"Java线程","permalink":"https://blog.fenxiangz.com/tags/Java%E7%BA%BF%E7%A8%8B/"}]},{"title":"关于 HashMap （JDK 1.7）","slug":"java/basic/2016-10-25_hashmap_jdk7","date":"2016-10-25T00:00:00.000Z","updated":"2020-12-20T16:47:02.963Z","comments":true,"path":"post/java/basic/2016-10-25_hashmap_jdk7.html","link":"","permalink":"https://blog.fenxiangz.com/post/java/basic/2016-10-25_hashmap_jdk7.html","excerpt":"","text":"JDK 1.8 对 HashMap 改进很多，1.8 中已经移除了 Entry 的这种实现方式了，改用了 Node，所以存储结构也发生了很大的变化，代码也从 1k 行膨胀到了 2k 行，这次梳理的是 1.7 的 HashMap 实现。1.8 下次再详细看看。 HashMap 继承 AbstractMap，实现了 Map 接口。 属性123456789101112131415161718192021static final int DEFAULT_INITIAL_CAPACITY &#x3D; 1 &lt;&lt; 4; &#x2F;&#x2F; 默认初始化容量值，16，要求 2 的 N 次幂（计算 table index 要求），可以在构造方法指定，如果在构造函数指定 50，那么 Map 会自动选择扩容到 50 以后的 2 次幂，即：64static final int MAXIMUM_CAPACITY &#x3D; 1 &lt;&lt; 30;&#x2F;&#x2F; 最大容量static final float DEFAULT_LOAD_FACTOR &#x3D; 0.75f;&#x2F;&#x2F; 默认负载因子，可以在构造方法指定实际值transient Entry[] table &#x3D; (Entry[]) EMPTY_TABLE;&#x2F;&#x2F; 所有 HashMap 实例初始化时，共享该值，表示空集合transient int size;&#x2F;&#x2F; Map 中 k-v 个数int threshold;&#x2F;&#x2F; 临界值，size &gt;&#x3D; 该值的时候，进行 resizefinal float loadFactor;&#x2F;&#x2F; 负载因子，用于计算 threshold，threshold &#x3D; (int)Math.min(capacity* loadFactor, MAXIMUM_CAPACITY + 1);transient int modCount;&#x2F;&#x2F; 记录 Map 结构被修改的次数，这里的修改可能是 Map 的 size 改变，比如 put，remove，clear 等；rehash；当 clone 的时候，返回的克隆 Map 对象的 modCout 被为 0static final int ALTERNATIVE_HASHING_THRESHOLD_DEFAULT &#x3D; Integer.MAX_VALUE;&#x2F;&#x2F; 默认替换散列阈值private static class Holder.ALTERNATIVE_HASHING_THRESHOLD &#x2F;&#x2F; 替换散列阈值，该值在 VM 启动以后初始化，如果系统属性（system property）数定义了 jdk.map.althashing.threshold &gt;&#x3D; 0 时，则使用定义的值作为该值；否则使用 Integer.MAX_VALUE。当调用 resize 或者 inflateTable 的时候，如果满足条件 capacity &gt;&#x3D; 该值，会进行 rehash，rehash 的目的是为了避免 hash 值的碰撞。transient int hashSeed &#x3D; 0;&#x2F;&#x2F; hashSeed 用于计算 key 的 hash 值，它与 key 的 hashCode 进行按位异或运算。这个 hashSeed 是一个与实例相关的随机值，主要用于解决 hash 冲突。上面说到 rehash 其实就是事先改变 hashSeed 的值，以至于计算 key 的新 hash 值与原 hash 值不同 构造方法1234567public HashMap(int initialCapacity, float loadFactor)public HashMap(int initialCapacity)public HashMap()public HashMap(Map&lt;? extends K, ? extends V&gt; m) 常用方法1234567public V put(K key, V value)public V get(Object key)public V remove(Object key)public boolean containsKey(Object key) 内部存储结构（JDK 1.7） （图片来源：http://javaconceptoftheday.com/how-hashmap-works-internally-in-java/） Entry 结构123456static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; int hash;&#125; Entry 自身持有另外一个 Entry 对象 next，构成了链表的数据结构。 put 方法算哈希值 –&gt; 算 table index –&gt; 存。 1、检查 table ，如果 == EMPTY_TABLE，先按照指定的容量扩容 table ，即：table = new Entry[capacity]，如果 capacity 的值不是 2 的 N 次幂，则 capacity 值会被修改为比 capacity 大的最近一个 2 的 N 次幂数。 2、检查 key == null ，如果是，则认为 key 的 hashValue 对应的 table index 为 0，否则需要计算 key 的 hashValue ，计算方法： 123456789101112final int hash(Object k) &#123; int h &#x3D; hashSeed; if (0 !&#x3D; h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^&#x3D; k.hashCode(); &#x2F;&#x2F; This function ensures that hashCodes that differ only by &#x2F;&#x2F; constant multiples at each bit position have a bounded &#x2F;&#x2F; number of collisions (approximately 8 at default load factor). h ^&#x3D; (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; 然后，计算 hashValue 对应的 table index ，方法：index = hashValue &amp; (table.length-1); 这个运算相当于 hashValue 对 table.length 的取模运算。因为 table.length 是优化过的，是 2 的 N 次幂，所以可以这样计算。（数学原理参考另外篇文章：http://blog.fenxiangz.com/java-basic/thread-local/ ，ThreadLocalMap 部分） 最后，就算出了这个 key-value 对于的 table[index] 了。 3、put，如果 table[index] == null , 则直接 new Entry，存就好了； 否则，先计算 table[index] 链表上的所有 Entry e，确认是否有 e.hash == hashValue &amp;&amp; e.key.equals(key) 的 e， 如果有，则用新的 value 替换旧的 e.value 并返回旧的 e.value； 否则，e1 = new Entry， table[index] = e1 ，并把原有的 table[index] 作为 e1.next。 存的过程，如果 (size&gt;= threshold) &amp;&amp; (null != table[index])，会导致 resize 扩容，并且如果满足条件 capacity &gt;= Holder.ALTERNATIVE_HASHING_THRESHOLD 的时候会进行 rehash，rehash 的目的是为了避免 hash 值的碰撞。 rehash 的成本是很高的，所以，如果使用 HashMap 的时候，指定了 capacity ，capacity 应该尽可能的大于业务预期的大小。防止过程中 rehash。","categories":[{"name":"Java 基础","slug":"Java-基础","permalink":"https://blog.fenxiangz.com/categories/Java-%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"HashMap","slug":"HashMap","permalink":"https://blog.fenxiangz.com/tags/HashMap/"},{"name":"Java集合","slug":"Java集合","permalink":"https://blog.fenxiangz.com/tags/Java%E9%9B%86%E5%90%88/"}]},{"title":"关于我","slug":"about","date":"2012-11-03T02:00:00.000Z","updated":"2020-12-20T16:47:02.944Z","comments":true,"path":"post/about.html","link":"","permalink":"https://blog.fenxiangz.com/post/about.html","excerpt":"","text":"","categories":[],"tags":[{"name":"关于我","slug":"关于我","permalink":"https://blog.fenxiangz.com/tags/%E5%85%B3%E4%BA%8E%E6%88%91/"}]},{"title":"友情链接","slug":"link","date":"2012-11-03T01:00:00.000Z","updated":"2020-12-20T16:47:02.974Z","comments":true,"path":"post/link.html","link":"","permalink":"https://blog.fenxiangz.com/post/link.html","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2012-11-03T00:00:00.000Z","updated":"2020-12-20T16:47:02.954Z","comments":true,"path":"post/hello-world.html","link":"","permalink":"https://blog.fenxiangz.com/post/hello-world.html","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[{"name":"Hello World","slug":"Hello-World","permalink":"https://blog.fenxiangz.com/tags/Hello-World/"}]}],"categories":[{"name":"IDE","slug":"IDE","permalink":"https://blog.fenxiangz.com/categories/IDE/"},{"name":"武夷岩茶","slug":"武夷岩茶","permalink":"https://blog.fenxiangz.com/categories/%E6%AD%A6%E5%A4%B7%E5%B2%A9%E8%8C%B6/"},{"name":"RPC","slug":"RPC","permalink":"https://blog.fenxiangz.com/categories/RPC/"},{"name":"Java 基础","slug":"Java-基础","permalink":"https://blog.fenxiangz.com/categories/Java-%E5%9F%BA%E7%A1%80/"},{"name":"资源导航","slug":"资源导航","permalink":"https://blog.fenxiangz.com/categories/%E8%B5%84%E6%BA%90%E5%AF%BC%E8%88%AA/"},{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/categories/IO/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.fenxiangz.com/categories/Redis/"},{"name":"其他/英语","slug":"其他-英语","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96-%E8%8B%B1%E8%AF%AD/"},{"name":"其他/管理","slug":"其他-管理","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96-%E7%AE%A1%E7%90%86/"},{"name":"其他/Jira","slug":"其他-Jira","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96-Jira/"},{"name":"其他/V2ray","slug":"其他-V2ray","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96-V2ray/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/categories/MySQL/"},{"name":"MSSQL","slug":"MSSQL","permalink":"https://blog.fenxiangz.com/categories/MSSQL/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://blog.fenxiangz.com/categories/Spring-Cloud/"},{"name":"Java 进阶","slug":"Java-进阶","permalink":"https://blog.fenxiangz.com/categories/Java-%E8%BF%9B%E9%98%B6/"},{"name":"Netty","slug":"Netty","permalink":"https://blog.fenxiangz.com/categories/Netty/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://blog.fenxiangz.com/categories/Spring-Boot/"},{"name":"Spring Core","slug":"Spring-Core","permalink":"https://blog.fenxiangz.com/categories/Spring-Core/"},{"name":"其他/Tampermonkey","slug":"其他-Tampermonkey","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96-Tampermonkey/"},{"name":"算法","slug":"算法","permalink":"https://blog.fenxiangz.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"Android","slug":"Android","permalink":"https://blog.fenxiangz.com/categories/Android/"},{"name":"跨平台","slug":"跨平台","permalink":"https://blog.fenxiangz.com/categories/%E8%B7%A8%E5%B9%B3%E5%8F%B0/"},{"name":"VUE","slug":"VUE","permalink":"https://blog.fenxiangz.com/categories/VUE/"},{"name":"jQuery","slug":"jQuery","permalink":"https://blog.fenxiangz.com/categories/jQuery/"},{"name":"概念","slug":"概念","permalink":"https://blog.fenxiangz.com/categories/%E6%A6%82%E5%BF%B5/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://blog.fenxiangz.com/categories/zookeeper/"},{"name":"Java NIO","slug":"Java-NIO","permalink":"https://blog.fenxiangz.com/categories/Java-NIO/"},{"name":"设计模式","slug":"设计模式","permalink":"https://blog.fenxiangz.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"网络/术语","slug":"网络-术语","permalink":"https://blog.fenxiangz.com/categories/%E7%BD%91%E7%BB%9C-%E6%9C%AF%E8%AF%AD/"},{"name":"其他","slug":"其他","permalink":"https://blog.fenxiangz.com/categories/%E5%85%B6%E4%BB%96/"},{"name":"Java 面试","slug":"Java-面试","permalink":"https://blog.fenxiangz.com/categories/Java-%E9%9D%A2%E8%AF%95/"},{"name":"TCP/IP","slug":"TCP-IP","permalink":"https://blog.fenxiangz.com/categories/TCP-IP/"},{"name":"SVN","slug":"SVN","permalink":"https://blog.fenxiangz.com/categories/SVN/"},{"name":"监控","slug":"监控","permalink":"https://blog.fenxiangz.com/categories/%E7%9B%91%E6%8E%A7/"},{"name":"Docker","slug":"Docker","permalink":"https://blog.fenxiangz.com/categories/Docker/"},{"name":"网络/其他","slug":"网络-其他","permalink":"https://blog.fenxiangz.com/categories/%E7%BD%91%E7%BB%9C-%E5%85%B6%E4%BB%96/"},{"name":"Gradle","slug":"Gradle","permalink":"https://blog.fenxiangz.com/categories/Gradle/"},{"name":"Java IO","slug":"Java-IO","permalink":"https://blog.fenxiangz.com/categories/Java-IO/"}],"tags":[{"name":"Idea","slug":"Idea","permalink":"https://blog.fenxiangz.com/tags/Idea/"},{"name":"IDE","slug":"IDE","permalink":"https://blog.fenxiangz.com/tags/IDE/"},{"name":"武夷岩茶","slug":"武夷岩茶","permalink":"https://blog.fenxiangz.com/tags/%E6%AD%A6%E5%A4%B7%E5%B2%A9%E8%8C%B6/"},{"name":"茶知识","slug":"茶知识","permalink":"https://blog.fenxiangz.com/tags/%E8%8C%B6%E7%9F%A5%E8%AF%86/"},{"name":"Java","slug":"Java","permalink":"https://blog.fenxiangz.com/tags/Java/"},{"name":"HttpClient","slug":"HttpClient","permalink":"https://blog.fenxiangz.com/tags/HttpClient/"},{"name":"同步","slug":"同步","permalink":"https://blog.fenxiangz.com/tags/%E5%90%8C%E6%AD%A5/"},{"name":"文件名批量修改","slug":"文件名批量修改","permalink":"https://blog.fenxiangz.com/tags/%E6%96%87%E4%BB%B6%E5%90%8D%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9/"},{"name":"IO","slug":"IO","permalink":"https://blog.fenxiangz.com/tags/IO/"},{"name":"零拷贝","slug":"零拷贝","permalink":"https://blog.fenxiangz.com/tags/%E9%9B%B6%E6%8B%B7%E8%B4%9D/"},{"name":"异步IO","slug":"异步IO","permalink":"https://blog.fenxiangz.com/tags/%E5%BC%82%E6%AD%A5IO/"},{"name":"直接IO","slug":"直接IO","permalink":"https://blog.fenxiangz.com/tags/%E7%9B%B4%E6%8E%A5IO/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.fenxiangz.com/tags/Redis/"},{"name":"优化","slug":"优化","permalink":"https://blog.fenxiangz.com/tags/%E4%BC%98%E5%8C%96/"},{"name":"英语","slug":"英语","permalink":"https://blog.fenxiangz.com/tags/%E8%8B%B1%E8%AF%AD/"},{"name":"语法","slug":"语法","permalink":"https://blog.fenxiangz.com/tags/%E8%AF%AD%E6%B3%95/"},{"name":"管理","slug":"管理","permalink":"https://blog.fenxiangz.com/tags/%E7%AE%A1%E7%90%86/"},{"name":"认知","slug":"认知","permalink":"https://blog.fenxiangz.com/tags/%E8%AE%A4%E7%9F%A5/"},{"name":"Jira","slug":"Jira","permalink":"https://blog.fenxiangz.com/tags/Jira/"},{"name":"工具","slug":"工具","permalink":"https://blog.fenxiangz.com/tags/%E5%B7%A5%E5%85%B7/"},{"name":"Wiki","slug":"Wiki","permalink":"https://blog.fenxiangz.com/tags/Wiki/"},{"name":"整理","slug":"整理","permalink":"https://blog.fenxiangz.com/tags/%E6%95%B4%E7%90%86/"},{"name":"V2ray","slug":"V2ray","permalink":"https://blog.fenxiangz.com/tags/V2ray/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.fenxiangz.com/tags/MySQL/"},{"name":"临时表","slug":"临时表","permalink":"https://blog.fenxiangz.com/tags/%E4%B8%B4%E6%97%B6%E8%A1%A8/"},{"name":"字符集","slug":"字符集","permalink":"https://blog.fenxiangz.com/tags/%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"name":"Collation","slug":"Collation","permalink":"https://blog.fenxiangz.com/tags/Collation/"},{"name":"性能优化","slug":"性能优化","permalink":"https://blog.fenxiangz.com/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"阅读笔记","slug":"阅读笔记","permalink":"https://blog.fenxiangz.com/tags/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"name":"慢查","slug":"慢查","permalink":"https://blog.fenxiangz.com/tags/%E6%85%A2%E6%9F%A5/"},{"name":"备份","slug":"备份","permalink":"https://blog.fenxiangz.com/tags/%E5%A4%87%E4%BB%BD/"},{"name":"MSSQL","slug":"MSSQL","permalink":"https://blog.fenxiangz.com/tags/MSSQL/"},{"name":"Spring","slug":"Spring","permalink":"https://blog.fenxiangz.com/tags/Spring/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://blog.fenxiangz.com/tags/Spring-Cloud/"},{"name":"jvm","slug":"jvm","permalink":"https://blog.fenxiangz.com/tags/jvm/"},{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://blog.fenxiangz.com/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"name":"思维导图","slug":"思维导图","permalink":"https://blog.fenxiangz.com/tags/%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/"},{"name":"Netty","slug":"Netty","permalink":"https://blog.fenxiangz.com/tags/Netty/"},{"name":"笔记","slug":"笔记","permalink":"https://blog.fenxiangz.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"单例","slug":"单例","permalink":"https://blog.fenxiangz.com/tags/%E5%8D%95%E4%BE%8B/"},{"name":"DCL","slug":"DCL","permalink":"https://blog.fenxiangz.com/tags/DCL/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://blog.fenxiangz.com/tags/Spring-Boot/"},{"name":"Spring Core","slug":"Spring-Core","permalink":"https://blog.fenxiangz.com/tags/Spring-Core/"},{"name":"源码分析","slug":"源码分析","permalink":"https://blog.fenxiangz.com/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"并发编程","slug":"并发编程","permalink":"https://blog.fenxiangz.com/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"JUC","slug":"JUC","permalink":"https://blog.fenxiangz.com/tags/JUC/"},{"name":"Tampermonkey","slug":"Tampermonkey","permalink":"https://blog.fenxiangz.com/tags/Tampermonkey/"},{"name":"弹琴吧","slug":"弹琴吧","permalink":"https://blog.fenxiangz.com/tags/%E5%BC%B9%E7%90%B4%E5%90%A7/"},{"name":"吉他","slug":"吉他","permalink":"https://blog.fenxiangz.com/tags/%E5%90%89%E4%BB%96/"},{"name":"文件加载","slug":"文件加载","permalink":"https://blog.fenxiangz.com/tags/%E6%96%87%E4%BB%B6%E5%8A%A0%E8%BD%BD/"},{"name":"八皇后","slug":"八皇后","permalink":"https://blog.fenxiangz.com/tags/%E5%85%AB%E7%9A%87%E5%90%8E/"},{"name":"算法","slug":"算法","permalink":"https://blog.fenxiangz.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"Android","slug":"Android","permalink":"https://blog.fenxiangz.com/tags/Android/"},{"name":"apk","slug":"apk","permalink":"https://blog.fenxiangz.com/tags/apk/"},{"name":"反编译","slug":"反编译","permalink":"https://blog.fenxiangz.com/tags/%E5%8F%8D%E7%BC%96%E8%AF%91/"},{"name":"JavaScript","slug":"JavaScript","permalink":"https://blog.fenxiangz.com/tags/JavaScript/"},{"name":"chameleon","slug":"chameleon","permalink":"https://blog.fenxiangz.com/tags/chameleon/"},{"name":"跨平台","slug":"跨平台","permalink":"https://blog.fenxiangz.com/tags/%E8%B7%A8%E5%B9%B3%E5%8F%B0/"},{"name":"小程序","slug":"小程序","permalink":"https://blog.fenxiangz.com/tags/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"Vue","slug":"Vue","permalink":"https://blog.fenxiangz.com/tags/Vue/"},{"name":"分布式","slug":"分布式","permalink":"https://blog.fenxiangz.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"术语","slug":"术语","permalink":"https://blog.fenxiangz.com/tags/%E6%9C%AF%E8%AF%AD/"},{"name":"理论","slug":"理论","permalink":"https://blog.fenxiangz.com/tags/%E7%90%86%E8%AE%BA/"},{"name":"CAP","slug":"CAP","permalink":"https://blog.fenxiangz.com/tags/CAP/"},{"name":"概念","slug":"概念","permalink":"https://blog.fenxiangz.com/tags/%E6%A6%82%E5%BF%B5/"},{"name":"脑裂","slug":"脑裂","permalink":"https://blog.fenxiangz.com/tags/%E8%84%91%E8%A3%82/"},{"name":"内存模型","slug":"内存模型","permalink":"https://blog.fenxiangz.com/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"},{"name":"NIO","slug":"NIO","permalink":"https://blog.fenxiangz.com/tags/NIO/"},{"name":"直接内存","slug":"直接内存","permalink":"https://blog.fenxiangz.com/tags/%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98/"},{"name":"非直接内存","slug":"非直接内存","permalink":"https://blog.fenxiangz.com/tags/%E9%9D%9E%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98/"},{"name":"堆内存","slug":"堆内存","permalink":"https://blog.fenxiangz.com/tags/%E5%A0%86%E5%86%85%E5%AD%98/"},{"name":"函数式编程","slug":"函数式编程","permalink":"https://blog.fenxiangz.com/tags/%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B/"},{"name":"lambda","slug":"lambda","permalink":"https://blog.fenxiangz.com/tags/lambda/"},{"name":"问题","slug":"问题","permalink":"https://blog.fenxiangz.com/tags/%E9%97%AE%E9%A2%98/"},{"name":"RPC","slug":"RPC","permalink":"https://blog.fenxiangz.com/tags/RPC/"},{"name":"操作系统","slug":"操作系统","permalink":"https://blog.fenxiangz.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Reactor","slug":"Reactor","permalink":"https://blog.fenxiangz.com/tags/Reactor/"},{"name":"NIO模型","slug":"NIO模型","permalink":"https://blog.fenxiangz.com/tags/NIO%E6%A8%A1%E5%9E%8B/"},{"name":"Java线程","slug":"Java线程","permalink":"https://blog.fenxiangz.com/tags/Java%E7%BA%BF%E7%A8%8B/"},{"name":"源码","slug":"源码","permalink":"https://blog.fenxiangz.com/tags/%E6%BA%90%E7%A0%81/"},{"name":"设计模式","slug":"设计模式","permalink":"https://blog.fenxiangz.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"行为型模式","slug":"行为型模式","permalink":"https://blog.fenxiangz.com/tags/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"结构型模式","slug":"结构型模式","permalink":"https://blog.fenxiangz.com/tags/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"创建型模式","slug":"创建型模式","permalink":"https://blog.fenxiangz.com/tags/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"ACID","slug":"ACID","permalink":"https://blog.fenxiangz.com/tags/ACID/"},{"name":"幻读","slug":"幻读","permalink":"https://blog.fenxiangz.com/tags/%E5%B9%BB%E8%AF%BB/"},{"name":"SQL优化","slug":"SQL优化","permalink":"https://blog.fenxiangz.com/tags/SQL%E4%BC%98%E5%8C%96/"},{"name":"网络术语","slug":"网络术语","permalink":"https://blog.fenxiangz.com/tags/%E7%BD%91%E7%BB%9C%E6%9C%AF%E8%AF%AD/"},{"name":"URL","slug":"URL","permalink":"https://blog.fenxiangz.com/tags/URL/"},{"name":"URI","slug":"URI","permalink":"https://blog.fenxiangz.com/tags/URI/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://blog.fenxiangz.com/tags/Ubuntu/"},{"name":"包管理","slug":"包管理","permalink":"https://blog.fenxiangz.com/tags/%E5%8C%85%E7%AE%A1%E7%90%86/"},{"name":"微服务","slug":"微服务","permalink":"https://blog.fenxiangz.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"Vert.x","slug":"Vert-x","permalink":"https://blog.fenxiangz.com/tags/Vert-x/"},{"name":"面试","slug":"面试","permalink":"https://blog.fenxiangz.com/tags/%E9%9D%A2%E8%AF%95/"},{"name":"数据结构","slug":"数据结构","permalink":"https://blog.fenxiangz.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"命令","slug":"命令","permalink":"https://blog.fenxiangz.com/tags/%E5%91%BD%E4%BB%A4/"},{"name":"集合运算","slug":"集合运算","permalink":"https://blog.fenxiangz.com/tags/%E9%9B%86%E5%90%88%E8%BF%90%E7%AE%97/"},{"name":"协议","slug":"协议","permalink":"https://blog.fenxiangz.com/tags/%E5%8D%8F%E8%AE%AE/"},{"name":"TCP","slug":"TCP","permalink":"https://blog.fenxiangz.com/tags/TCP/"},{"name":"输入法","slug":"输入法","permalink":"https://blog.fenxiangz.com/tags/%E8%BE%93%E5%85%A5%E6%B3%95/"},{"name":"云原生","slug":"云原生","permalink":"https://blog.fenxiangz.com/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"Cloud","slug":"Cloud","permalink":"https://blog.fenxiangz.com/tags/Cloud/"},{"name":"内存","slug":"内存","permalink":"https://blog.fenxiangz.com/tags/%E5%86%85%E5%AD%98/"},{"name":"SVN","slug":"SVN","permalink":"https://blog.fenxiangz.com/tags/SVN/"},{"name":"MMAP","slug":"MMAP","permalink":"https://blog.fenxiangz.com/tags/MMAP/"},{"name":"内存映射","slug":"内存映射","permalink":"https://blog.fenxiangz.com/tags/%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%84/"},{"name":"监控","slug":"监控","permalink":"https://blog.fenxiangz.com/tags/%E7%9B%91%E6%8E%A7/"},{"name":"报警","slug":"报警","permalink":"https://blog.fenxiangz.com/tags/%E6%8A%A5%E8%AD%A6/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://blog.fenxiangz.com/tags/Prometheus/"},{"name":"网络","slug":"网络","permalink":"https://blog.fenxiangz.com/tags/%E7%BD%91%E7%BB%9C/"},{"name":"模型","slug":"模型","permalink":"https://blog.fenxiangz.com/tags/%E6%A8%A1%E5%9E%8B/"},{"name":"磁盘管理","slug":"磁盘管理","permalink":"https://blog.fenxiangz.com/tags/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/"},{"name":"挂载","slug":"挂载","permalink":"https://blog.fenxiangz.com/tags/%E6%8C%82%E8%BD%BD/"},{"name":"UDP","slug":"UDP","permalink":"https://blog.fenxiangz.com/tags/UDP/"},{"name":"XXNET","slug":"XXNET","permalink":"https://blog.fenxiangz.com/tags/XXNET/"},{"name":"bbr","slug":"bbr","permalink":"https://blog.fenxiangz.com/tags/bbr/"},{"name":"快捷键","slug":"快捷键","permalink":"https://blog.fenxiangz.com/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/"},{"name":"deb","slug":"deb","permalink":"https://blog.fenxiangz.com/tags/deb/"},{"name":"chrome","slug":"chrome","permalink":"https://blog.fenxiangz.com/tags/chrome/"},{"name":"目录","slug":"目录","permalink":"https://blog.fenxiangz.com/tags/%E7%9B%AE%E5%BD%95/"},{"name":"Docker","slug":"Docker","permalink":"https://blog.fenxiangz.com/tags/Docker/"},{"name":"入门","slug":"入门","permalink":"https://blog.fenxiangz.com/tags/%E5%85%A5%E9%97%A8/"},{"name":"学习笔记","slug":"学习笔记","permalink":"https://blog.fenxiangz.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"异步","slug":"异步","permalink":"https://blog.fenxiangz.com/tags/%E5%BC%82%E6%AD%A5/"},{"name":"阻塞","slug":"阻塞","permalink":"https://blog.fenxiangz.com/tags/%E9%98%BB%E5%A1%9E/"},{"name":"非阻塞","slug":"非阻塞","permalink":"https://blog.fenxiangz.com/tags/%E9%9D%9E%E9%98%BB%E5%A1%9E/"},{"name":"网络协议","slug":"网络协议","permalink":"https://blog.fenxiangz.com/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"CDN","slug":"CDN","permalink":"https://blog.fenxiangz.com/tags/CDN/"},{"name":"Socket","slug":"Socket","permalink":"https://blog.fenxiangz.com/tags/Socket/"},{"name":"vpn","slug":"vpn","permalink":"https://blog.fenxiangz.com/tags/vpn/"},{"name":"ssh","slug":"ssh","permalink":"https://blog.fenxiangz.com/tags/ssh/"},{"name":"Heidi","slug":"Heidi","permalink":"https://blog.fenxiangz.com/tags/Heidi/"},{"name":"符号链接","slug":"符号链接","permalink":"https://blog.fenxiangz.com/tags/%E7%AC%A6%E5%8F%B7%E9%93%BE%E6%8E%A5/"},{"name":"lambda表达式","slug":"lambda表达式","permalink":"https://blog.fenxiangz.com/tags/lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"Gradle","slug":"Gradle","permalink":"https://blog.fenxiangz.com/tags/Gradle/"},{"name":"Java引用","slug":"Java引用","permalink":"https://blog.fenxiangz.com/tags/Java%E5%BC%95%E7%94%A8/"},{"name":"Selector","slug":"Selector","permalink":"https://blog.fenxiangz.com/tags/Selector/"},{"name":"Buffer","slug":"Buffer","permalink":"https://blog.fenxiangz.com/tags/Buffer/"},{"name":"Stream","slug":"Stream","permalink":"https://blog.fenxiangz.com/tags/Stream/"},{"name":"Exception","slug":"Exception","permalink":"https://blog.fenxiangz.com/tags/Exception/"},{"name":"二进制","slug":"二进制","permalink":"https://blog.fenxiangz.com/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6/"},{"name":"反码","slug":"反码","permalink":"https://blog.fenxiangz.com/tags/%E5%8F%8D%E7%A0%81/"},{"name":"补码","slug":"补码","permalink":"https://blog.fenxiangz.com/tags/%E8%A1%A5%E7%A0%81/"},{"name":"知识体系","slug":"知识体系","permalink":"https://blog.fenxiangz.com/tags/%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"name":"书籍推荐","slug":"书籍推荐","permalink":"https://blog.fenxiangz.com/tags/%E4%B9%A6%E7%B1%8D%E6%8E%A8%E8%8D%90/"},{"name":"异常","slug":"异常","permalink":"https://blog.fenxiangz.com/tags/%E5%BC%82%E5%B8%B8/"},{"name":"Java异常","slug":"Java异常","permalink":"https://blog.fenxiangz.com/tags/Java%E5%BC%82%E5%B8%B8/"},{"name":"序列化","slug":"序列化","permalink":"https://blog.fenxiangz.com/tags/%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"内存分配","slug":"内存分配","permalink":"https://blog.fenxiangz.com/tags/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"},{"name":"内存泄露","slug":"内存泄露","permalink":"https://blog.fenxiangz.com/tags/%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2/"},{"name":"注解","slug":"注解","permalink":"https://blog.fenxiangz.com/tags/%E6%B3%A8%E8%A7%A3/"},{"name":"ThreadLocal","slug":"ThreadLocal","permalink":"https://blog.fenxiangz.com/tags/ThreadLocal/"},{"name":"HashMap","slug":"HashMap","permalink":"https://blog.fenxiangz.com/tags/HashMap/"},{"name":"Java集合","slug":"Java集合","permalink":"https://blog.fenxiangz.com/tags/Java%E9%9B%86%E5%90%88/"},{"name":"关于我","slug":"关于我","permalink":"https://blog.fenxiangz.com/tags/%E5%85%B3%E4%BA%8E%E6%88%91/"},{"name":"Hello World","slug":"Hello-World","permalink":"https://blog.fenxiangz.com/tags/Hello-World/"}]}